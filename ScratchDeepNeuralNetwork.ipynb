{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN969heUThZ0rD+QnRhHzHo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyMariel/Lucy/blob/master/ScratchDeepNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will extend the implementation of the neural network created from scratch.\n",
        "Last time, we created a three-layer neural network, but this time we will rewrite it into one that can easily be expanded to an arbitrary number of layers. After that, we will be able to deal with advanced functions, activation functions, initial values, and optimization methods.\n",
        "\n",
        "By doing this from scratch, we aim to give you an idea of the inner workings of the various frameworks that we will be using.\n",
        "\n",
        "The name should be changed from Scratch Deep Neural Network Classifier class.\n",
        "\n",
        "Classifying layers, etc.\n",
        "By putting them in a class, we will make the implementation easy to change the configuration.\n",
        "\n",
        "Places to modify\n",
        "\n",
        "Number of layers\n",
        "Layer type (other types of layers such as convolutional layers will appear in the future)\n",
        "Types of activation functions\n",
        "Weight and bias initialization method\n",
        "Optimization method\n",
        "To do this, we create classes for all the coupling layers, for the various activation functions, for the initialisation of weights and biases, and for each of the optimisation methods.\n",
        "\n",
        "You can implement it freely, but here is a simple example. Create an instance of the fully connected layer and activation function as in sample code 1, and use it as in sample code 2 and 3. Each class will be explained later."
      ],
      "metadata": {
        "id": "Oder7GeiqYUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<<Sample Code 1>>\n",
        "\n",
        "In the fit method of ScratchDeepNeuralNetrowkClassifier"
      ],
      "metadata": {
        "id": "_VY_5T1Is0gG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2y_K--Ln1cy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# self.sigma: Standard deviation of Gaussian distribution\n",
        "# self.lr : Learning rate\n",
        "# self.n_nodes1: Number of nodes in the first layer\n",
        "# self.n_nodes2: Number of nodes in the second layer\n",
        "# self.n_output: Number of nodes in the output layer\n",
        "\n",
        "optimizer = SGD(self.lr)\n",
        "self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation1 = Tanh()\n",
        "self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation2 = Tanh()\n",
        "self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation3 = Softmax()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "《Sample code 2》\n",
        "\n",
        "Forward for each iteration"
      ],
      "metadata": {
        "id": "D5O9j4j0tGGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A1 = self.FC1.forward(X)\n",
        "Z1 = self.activation1.forward(A1)\n",
        "A2 = self.FC2.forward(Z1)\n",
        "Z2 = self.activation2.forward(A2)\n",
        "A3 = self.FC3.forward(Z2)\n",
        "Z3 = self.activation3.forward(A3)"
      ],
      "metadata": {
        "id": "CAOcwDKDtHMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "《Sample code 3》\n",
        "\n",
        "Backward for each iteration"
      ],
      "metadata": {
        "id": "kswVqYNntMgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスTo合わせている\n",
        "dZ2 = self.FC3.backward(dA3)\n",
        "dA2 = self.activation2.backward(dZ2)\n",
        "dZ1 = self.FC2.backward(dA2)\n",
        "dA1 = self.activation1.backward(dZ1)\n",
        "dZ0 = self.FC1.backward(dA1) # dZ0 is not used"
      ],
      "metadata": {
        "id": "5gAcrveatSUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 1] Classifying fully connected layers\n",
        "Please classify the fully connected layer.\n",
        "\n",
        "Below is a template. Initialize weights and bias in the constructor, and prepare forward and backward methods. By holding the weight W, the bias B, and the forward input X as instance variables, complicated input/output becomes unnecessary.\n",
        "\n",
        "You can also pass an instance as an argument. Therefore, if you receive the instance initializer of the initialization method in the constructor, it will be initialized. You can change the initialization method by changing the instance to be passed.\n",
        "\n",
        "You can also pass your self as an argument. You can use this to update the layer weights like self.optimizer.update(self) There are multiple values required for the update, but all can be instance variables of the fully connected layer.\n",
        "\n",
        "The initialization method and the class of optimization methods are described later.\n",
        "\n",
        "Model"
      ],
      "metadata": {
        "id": "OBQr34fktrwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This scratch model represents a fully connected (FC) layer, which is a crucial component of a neural network. The purpose of this class is to define the structure and functionality of an FC layer, including its initialization, forward propagation, and backward propagation steps. Here’s a breakdown of its components and their purposes:"
      ],
      "metadata": {
        "id": "n4MuAfgmu5G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        # Initialize\n",
        "        # To initialize the FC layer with the given number of nodes, initializer, and optimizer.\n",
        "        pass\n",
        "    def forward(self, X): #To compute the output of the layer by applying the weights and biases to the input data.\n",
        "        \"\"\"\n",
        "        forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "        Returns\n",
        "        ----------\n",
        "        A : of the following form. ndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return A\n",
        "    def backward(self, dA): #To compute the gradient of the loss with respect to the input of the layer (dZ), which will be propagated back to the previous layer.\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : of the following form. ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient flowing from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : of the following form. ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to flow forward\n",
        "        \"\"\"\n",
        "        pass\n",
        "        # update\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ\n"
      ],
      "metadata": {
        "id": "yPiPad5uts_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 2] Classifying the initialization method\n",
        "Classify the initialization code.\n",
        "\n",
        "As mentioned above, we will be able to pass an instance of the initialization method to the constructor of the fully connected layer. Please add the necessary code to the following template. By receiving the standard deviation value (sigma) in the constructor, it is not necessary to pass this value (sigma) in the class of the fully connected layer.\n",
        "\n",
        "The initialization method we have been dealing with so far will be named the SimpleInitializer class.\n",
        "\n",
        "《Model 1 FC Layer》"
      ],
      "metadata": {
        "id": "DJgOWpxbEPXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W :\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B :\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return B"
      ],
      "metadata": {
        "id": "s6B6UO5VEQqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 3] Classifying optimization methods\n",
        "Please classify the optimization method.\n",
        "\n",
        "With respect to the optimization method, it is passed as an instance to the fully connected layer as well as the initialization method. When backward, we can update it as self.optimizer.update(self). Please add the necessary code to the following template.\n",
        "\n",
        "The optimization methods we have dealt with so far are created as SGD class (Stochastic Gradient Descent).\n",
        "\n",
        " Prototype"
      ],
      "metadata": {
        "id": "CrB6VFvGE4VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "r73MmcIZE5a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 4] Classifying activation functions\n",
        "Please classify the activation function.\n",
        "\n",
        "The backpropagation of the softmax function is simplified by implementing it including the calculation of the cross entropy error."
      ],
      "metadata": {
        "id": "W77nRU6oFItH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evolutionary element\n",
        "We will implement other than the activation functions, initial values of weights, and optimization methods that we have not seen so far.\n",
        "\n",
        "[Problem 5] ReLU class creation\n",
        "Please implement ReLU (Rectified Linear Unit) which is a commonly used activation function as ReLU class.\n",
        "\n",
        "ReLU is the following formula."
      ],
      "metadata": {
        "id": "vLzODGxGFOd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 6] Initial value of weight\n",
        "So far, the initial values of weights and bias have been simply Gaussian distributions, and standard deviation has been treated as a hyperparameter. However, it is known what value it should be. For sigmoidal and hyperbolic tangent functions, the initial value of Xavier (or the initial value of Glorot) is used, and for ReLU the initial value of He.\n",
        "\n",
        "Create XavierI nitializer class and HeIn itializer class.\n",
        "\n",
        "Initial value of Xavier\n",
        "The standard deviation $\\sigma$ at the initial value of Xavier can be obtained by the following equation\n",
        "\n",
        "σ\n",
        "​ ​\n",
        "=\n",
        "​ ​\n",
        "1\n",
        "​ ​\n",
        "√\n",
        "​ ​\n",
        "n\n",
        "$n$ : number of nodes in the previous layer\n",
        "\n",
        "\"paper\"\n",
        "\n",
        "Glorot, X., & Bengio, Y. (nd). Understanding the difficulty of training deep feedforward neural networks.\n",
        "\n",
        "Initial value of He\n",
        "The standard deviation $\\sigma$ at He's initial value can be obtained by the following equation\n",
        "\n",
        "σ\n",
        "​ ​\n",
        "=\n",
        "​ ​\n",
        "√\n",
        "​ ​\n",
        "2\n",
        "​ ​\n",
        "n\n",
        "$n$ : number of nodes in the previous layer\n",
        "\n",
        "\"paper\"\n",
        "\n",
        "He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.\n",
        "\n",
        "[Problem 7] Optimization method\n",
        "The most common method is to vary the learning rate in the learning process. Please create a class of AdaGrad which is the basic method.\n",
        "\n",
        "First, check the SGD you have been using so far.\n",
        "\n",
        "W\n",
        "′\n",
        "i\n",
        "=\n",
        "W\n",
        "i\n",
        "−\n",
        "α\n",
        "∂\n",
        "l\n",
        "∂\n",
        "W\n",
        "i\n",
        "B\n",
        "′\n",
        "i\n",
        "=\n",
        "B\n",
        "i\n",
        "−\n",
        "α\n",
        "∂\n",
        "l\n",
        "∂\n",
        "B\n",
        "i\n",
        "$\\alpha$ : learning rate (can be changed from layer to layer, but basically assumed to be the same for all)\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_i}$ : slope of loss $L$ with respect to $W_i$.\n",
        "\n",
        "$\\frac{\\partial L}{\\partial B_i}$ : slope of loss $L$ with respect to $B_i$.\n",
        "\n",
        "Next is AdaGrad. The bias formula is omitted, but it does something similar to weighting.\n",
        "\n",
        "Gradually reduce the learning rate for that weight as it is updated. Save the sum of the squares of the gradients $H$ for each iteration, and reduce the learning rate by that amount.\n",
        "\n",
        "The learning rate will be different for each weight.\n",
        "\n",
        "H\n",
        "′\n",
        "i\n",
        "=\n",
        "H\n",
        "i\n",
        "+\n",
        "∂\n",
        "l\n",
        "∂\n",
        "W\n",
        "i\n",
        "⊙\n",
        "∂\n",
        "l\n",
        "∂\n",
        "W\n",
        "i\n",
        "W\n",
        "′\n",
        "i\n",
        "=\n",
        "W\n",
        "i\n",
        "−\n",
        "α\n",
        "(\n",
        "1\n",
        "√\n",
        "H\n",
        "′\n",
        "i\n",
        "⊙\n",
        "∂\n",
        "l\n",
        "∂\n",
        "W\n",
        "i\n",
        ")\n",
        "$H_i$ : sum of squares of gradients up to the previous iteration for the i-th layer (initial value is 0)\n",
        "\n",
        "$H_i^{\\prime}$ : updated $H_i$.\n",
        "\n",
        "\"paper\"\n",
        "\n",
        "Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).\n",
        "\n",
        "[Problem 8] Class completion\n",
        "Complete the Scratch Deep Neural Netrowk Classifier class that can be trained and estimated with any configuration."
      ],
      "metadata": {
        "id": "eDYBvT6cFVGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 9] Learning and estimation\n",
        "Create several networks with varying numbers of layers and activation functions. Then, train and estimate the MNIST data and calculate the Accuracy.\n",
        "\n",
        "Model 2 layer with Random data"
      ],
      "metadata": {
        "id": "RJuzTouTFlim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleInitializer:\n",
        "    def initialize(self, shape):\n",
        "        return np.random.randn(*shape) * 0.01\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, W, B, dW, dB):\n",
        "        W -= self.learning_rate * dW\n",
        "        B -= self.learning_rate * dB\n",
        "        return W, B"
      ],
      "metadata": {
        "id": "5PMztvJoFUjW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the FC Layer:\n",
        "Use the code from the previous example for the FC layer.\n",
        "\n",
        "Build a Simple Neural Network:\n",
        "Create a simple neural network that includes the FC layer."
      ],
      "metadata": {
        "id": "p59PpkOUHxc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        initializer = SimpleInitializer()\n",
        "        optimizer = SGD(learning_rate=0.01)\n",
        "        self.fc1 = FC(input_size, hidden_size, initializer, optimizer)\n",
        "        self.fc2 = FC(hidden_size, output_size, initializer, optimizer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        A1 = self.fc1.forward(X)\n",
        "        A2 = self.fc2.forward(A1)\n",
        "        return A2\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dA1 = self.fc2.backward(dA)\n",
        "        dZ = self.fc1.backward(dA1)\n",
        "        return dZ"
      ],
      "metadata": {
        "id": "iE6lRVmBFJ-W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the Data:\n",
        "Assume we have some training data X_train and corresponding labels y_train."
      ],
      "metadata": {
        "id": "XYtXm8OoH8mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data\n",
        "X_train = np.random.randn(10, 4)  # 10 samples, 4 features\n",
        "y_train = np.random.randn(10, 3)  # 10 samples, 3 output classes"
      ],
      "metadata": {
        "id": "hhw-FgfIIA3L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model:\n",
        "Perform forward and backward passes to train the model."
      ],
      "metadata": {
        "id": "GshzS_I1ILYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "input_size = 4\n",
        "hidden_size = 5\n",
        "output_size = 3\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    output = model.forward(X_train)\n",
        "\n",
        "    # Compute loss (simple mean squared error)\n",
        "    loss = np.mean((output - y_train) ** 2)\n",
        "\n",
        "    # Compute gradient of the loss w.r.t. output\n",
        "    dA = 2 * (output - y_train) / y_train.shape[0]\n",
        "\n",
        "    # Backward pass\n",
        "    model.backward(dA)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")"
      ],
      "metadata": {
        "id": "dicPsmZnIQSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simple implementation demonstrates how to integrate the FC layer into a neural network and feed it data. It includes:\n",
        "\n",
        "An initializer for weights and biases.\n",
        "An optimizer for updating weights and biases.\n",
        "A simple neural network with one hidden layer.\n",
        "Training loop with forward and backward passes.\n",
        "This model and training loop can be extended with more complex architectures, loss functions, and training procedures as needed."
      ],
      "metadata": {
        "id": "F8nZ7GXFIZhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 layers"
      ],
      "metadata": {
        "id": "u5oQdfrpKNRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleInitializer:\n",
        "    def initialize(self, shape):\n",
        "        return np.random.randn(*shape) * 0.01\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, W, B, dW, dB):\n",
        "        W -= self.learning_rate * dW\n",
        "        B -= self.learning_rate * dB\n",
        "        return W, B\n",
        "\n",
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.W = initializer.initialize((n_nodes1, n_nodes2))\n",
        "        self.B = initializer.initialize((1, n_nodes2))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X @ self.W + self.B\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dW = self.X.T @ dA\n",
        "        dB = np.sum(dA, axis=0, keepdims=True)\n",
        "        dZ = dA @ self.W.T\n",
        "\n",
        "        self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        initializer = SimpleInitializer()\n",
        "        optimizer = SGD(learning_rate=0.01)\n",
        "        self.fc1 = FC(input_size, hidden_size, initializer, optimizer)\n",
        "        self.fc2 = FC(hidden_size, output_size, initializer, optimizer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        A1 = self.fc1.forward(X)\n",
        "        A2 = self.fc2.forward(A1)\n",
        "        return A2\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dA1 = self.fc2.backward(dA)\n",
        "        dZ = self.fc1.backward(dA1)\n",
        "        return dZ\n",
        "\n",
        "# Example data\n",
        "X_train = np.random.randn(10, 4)  # 10 samples, 4 features\n",
        "y_train = np.random.randn(10, 3)  # 10 samples, 3 output classes\n",
        "\n",
        "# Initialize the model\n",
        "input_size = 4\n",
        "hidden_size = 5\n",
        "output_size = 3\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    output = model.forward(X_train)\n",
        "\n",
        "    # Compute loss (simple mean squared error)\n",
        "    loss = np.mean((output - y_train) ** 2)\n",
        "\n",
        "    # Compute gradient of the loss w.r.t. output\n",
        "    dA = 2 * (output - y_train) / y_train.shape[0]\n",
        "\n",
        "    # Backward pass\n",
        "    model.backward(dA)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEjjGzjyKPEv",
        "outputId": "76637042-2d3b-4882-c483-ed85a1d2a6e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.374524418408315\n",
            "Epoch 100, Loss: 1.285417688962274\n",
            "Epoch 200, Loss: 1.172939970615394\n",
            "Epoch 300, Loss: 0.7052619064188007\n",
            "Epoch 400, Loss: 0.6693141496497926\n",
            "Epoch 500, Loss: 0.6532647447823573\n",
            "Epoch 600, Loss: 0.6228268621464818\n",
            "Epoch 700, Loss: 0.5947804694322335\n",
            "Epoch 800, Loss: 0.5834218987744719\n",
            "Epoch 900, Loss: 0.5790782155746281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Predictions\n",
        "predictions = model.forward(X_train)\n",
        "\n",
        "# Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "\n",
        "# Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_train, predictions))\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "\n",
        "# R-squared (R²)\n",
        "r2 = r2_score(y_train, predictions)\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_M1czKvN8pe",
        "outputId": "f982dc81-775d-48aa-a84c-93169f5bc7bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 0.5686681653243103\n",
            "Root Mean Squared Error: 0.7595997706195532\n",
            "R-squared: 0.5732025125234386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 layer"
      ],
      "metadata": {
        "id": "9dmgShoHKmVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleInitializer:\n",
        "    def initialize(self, shape):\n",
        "        return np.random.randn(*shape) * 0.01\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, W, B, dW, dB):\n",
        "        W -= self.learning_rate * dW\n",
        "        B -= self.learning_rate * dB\n",
        "        return W, B\n",
        "\n",
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.W = initializer.initialize((n_nodes1, n_nodes2))\n",
        "        self.B = initializer.initialize((1, n_nodes2))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X @ self.W + self.B\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dW = self.X.T @ dA\n",
        "        dB = np.sum(dA, axis=0, keepdims=True)\n",
        "        dZ = dA @ self.W.T\n",
        "\n",
        "        self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB)\n",
        "\n",
        "        return dZ\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        initializer = SimpleInitializer()\n",
        "        optimizer = SGD(learning_rate=0.01)\n",
        "        self.fc = FC(input_size, output_size, initializer, optimizer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        A = self.fc.forward(X)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dZ = self.fc.backward(dA)\n",
        "        return dZ\n",
        "\n",
        "# Example data\n",
        "X_train = np.random.randn(10, 4)  # 10 samples, 4 features\n",
        "y_train = np.random.randn(10, 3)  # 10 samples, 3 output classes\n",
        "\n",
        "# Initialize the model\n",
        "input_size = 4\n",
        "output_size = 3\n",
        "model = SimpleNN(input_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    output = model.forward(X_train)\n",
        "\n",
        "    # Compute loss (simple mean squared error)\n",
        "    loss = np.mean((output - y_train) ** 2)\n",
        "\n",
        "    # Compute gradient of the loss w.r.t. output\n",
        "    dA = 2 * (output - y_train) / y_train.shape[0]\n",
        "\n",
        "    # Backward pass\n",
        "    model.backward(dA)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7d5qcUcKo3u",
        "outputId": "8e44e6b1-c7e1-438f-ac30-6a57627378f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.8917309973762257\n",
            "Epoch 100, Loss: 0.5127562498993703\n",
            "Epoch 200, Loss: 0.4876356393312072\n",
            "Epoch 300, Loss: 0.48018429432849047\n",
            "Epoch 400, Loss: 0.47739942243023137\n",
            "Epoch 500, Loss: 0.4762835816401125\n",
            "Epoch 600, Loss: 0.47582704205677956\n",
            "Epoch 700, Loss: 0.47563909398821774\n",
            "Epoch 800, Loss: 0.47556157928122433\n",
            "Epoch 900, Loss: 0.47552959324784255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Capacity:\n",
        "Two-layer Model: A model with two fully connected layers has more capacity to learn complex patterns in the data. It can capture non-linear relationships better than a single-layer model. This is because each layer can learn a different representation of the data, which, when combined, can represent more complex functions.\n",
        "One-layer Model: A single fully connected layer has limited capacity. It can only capture linear relationships between the input features and the output."
      ],
      "metadata": {
        "id": "F1SiN1ztLerQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Dynamics:\n",
        "Two-layer Model: The loss reduction in the two-layer model shows a more gradual improvement, which is typical for models with more parameters and complexity. The initial higher loss indicates that the model starts off less optimized, but it gradually learns the patterns in the data.\n",
        "One-layer Model: The one-layer model shows a rapid decrease in loss initially, which indicates it is easier to optimize but reaches its limit sooner. This is typical for simpler models, as they converge faster but might not achieve as low a loss as more complex models."
      ],
      "metadata": {
        "id": "wshQmlbjL1Nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting vs. Underfitting:\n",
        "Two-layer Model: With more layers, there's a risk of overfitting if the model becomes too complex relative to the amount of training data. However, with proper regularization and enough data, it can generalize well.\n",
        "One-layer Model: A single-layer model might underfit the data, meaning it might not capture all the relevant patterns. This can be seen if the training loss plateaus at a higher value compared to the more complex model."
      ],
      "metadata": {
        "id": "R6mdCbrdNH_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Predictions\n",
        "predictions = model.forward(X_train)\n",
        "\n",
        "# Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "\n",
        "# Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_train, predictions))\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "\n",
        "# R-squared (R²)\n",
        "r2 = r2_score(y_train, predictions)\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPvm1lCDNzt-",
        "outputId": "8b21fe82-ecf3-4ec6-d562-c2144a3c002a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 0.5509931714687599\n",
            "Root Mean Squared Error: 0.6895769662181007\n",
            "R-squared: 0.42419082441458206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the predictions of the SimpleNN model and its current weights (W) and biases (B), you can add some code after your training loop to make predictions and display the model parameters. Here's how you can modify your code:"
      ],
      "metadata": {
        "id": "tgK1DfKCPC3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleInitializer:\n",
        "    def initialize(self, shape):\n",
        "        return np.random.randn(*shape) * 0.01\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, W, B, dW, dB):\n",
        "        W -= self.learning_rate * dW\n",
        "        B -= self.learning_rate * dB\n",
        "        return W, B\n",
        "\n",
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.W = initializer.initialize((n_nodes1, n_nodes2))\n",
        "        self.B = initializer.initialize((1, n_nodes2))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X @ self.W + self.B\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dW = self.X.T @ dA\n",
        "        dB = np.sum(dA, axis=0, keepdims=True)\n",
        "        dZ = dA @ self.W.T\n",
        "\n",
        "        self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB)\n",
        "        return dZ\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        initializer = SimpleInitializer()\n",
        "        optimizer = SGD(learning_rate=0.01)\n",
        "        self.fc = FC(input_size, output_size, initializer, optimizer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        A = self.fc.forward(X)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dZ = self.fc.backward(dA)\n",
        "        return dZ\n",
        "\n",
        "# Example data\n",
        "X_train = np.random.randn(10, 4)  # 10 samples, 4 features\n",
        "y_train = np.random.randn(10, 3)  # 10 samples, 3 output classes\n",
        "\n",
        "# Initialize the model\n",
        "input_size = 4\n",
        "output_size = 3\n",
        "model = SimpleNN(input_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    output = model.forward(X_train)\n",
        "\n",
        "    # Compute loss (simple mean squared error)\n",
        "    loss = np.mean((output - y_train) ** 2)\n",
        "\n",
        "    # Compute gradient of the loss w.r.t. output\n",
        "    dA = 2 * (output - y_train) / y_train.shape[0]\n",
        "\n",
        "    # Backward pass\n",
        "    model.backward(dA)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# After training\n",
        "# Forward pass to get predictions\n",
        "predictions = model.forward(X_train)\n",
        "print(\"\\nExample predictions:\")\n",
        "print(predictions)\n",
        "\n",
        "# Display model parameters\n",
        "print(\"\\nModel parameters:\")\n",
        "print(\"Weight matrix (W):\")\n",
        "print(model.fc.W)\n",
        "print(\"\\nBias matrix (B):\")\n",
        "print(model.fc.B)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPbtAfYePjOf",
        "outputId": "b80275fb-0322-444c-9744-4244e2139731"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.1788306692217547\n",
            "Epoch 100, Loss: 0.7723893737307727\n",
            "Epoch 200, Loss: 0.7426058965066623\n",
            "Epoch 300, Loss: 0.7369098394697801\n",
            "Epoch 400, Loss: 0.7351943797278327\n",
            "Epoch 500, Loss: 0.7345603094634776\n",
            "Epoch 600, Loss: 0.7343067125676473\n",
            "Epoch 700, Loss: 0.734202105652827\n",
            "Epoch 800, Loss: 0.7341584137971006\n",
            "Epoch 900, Loss: 0.7341400709452637\n",
            "\n",
            "Example predictions:\n",
            "[[-0.89296643 -0.1649297  -0.01049177]\n",
            " [-0.39552486 -1.15703503 -0.66937901]\n",
            " [-1.20981986  0.37867071  0.88885449]\n",
            " [-0.89978599  0.30695112  0.6673922 ]\n",
            " [-1.14924691 -0.21980929  0.34633305]\n",
            " [ 0.26059571 -0.11019482 -1.0431927 ]\n",
            " [-0.64469452 -0.25163366 -0.3310182 ]\n",
            " [-0.60337224 -0.14781291  0.38773812]\n",
            " [-0.53201344  0.01978117 -0.0813861 ]\n",
            " [-1.32445332  0.80690994  0.33707705]]\n",
            "\n",
            "Model parameters:\n",
            "Weight matrix (W):\n",
            "[[ 0.38747398 -0.02134242 -0.52878613]\n",
            " [-0.46397796  0.08630813 -0.01203217]\n",
            " [ 0.00190301  0.08857254  0.06947919]\n",
            " [-0.1957224   0.59083288  0.43926975]]\n",
            "\n",
            "Bias matrix (B):\n",
            "[[-0.78623841 -0.3699668   0.00626722]]\n"
          ]
        }
      ]
    }
  ]
}