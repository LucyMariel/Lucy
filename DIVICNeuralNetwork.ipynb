{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+T7SqbKvQgC9GfXNJi4lB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyMariel/Lucy/blob/master/DIVICNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing a simple neural network in scratch, we will train and validate it.\n",
        "\n",
        "The MNIST dataset is used to validate the neural network scratch. It can be downloaded from various libraries and sites, but here we will use Keras, a deep learning framework. You can download the dataset and even extract it by executing the following code."
      ],
      "metadata": {
        "id": "gvRrvMv74_wg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nWwaCHD4e6s",
        "outputId": "b0f5a09d-f07c-44e2-c24e-4b533976f44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "《What is MNIST?》\n",
        "\n",
        "This is a standard dataset for image classification and handwritten digit recognition. The dataset contains 60,000 black-and-white images of 28 x 28 pixels for training and 10,000 for testing, along with labels indicating which digits they are from 0 to 9.\n",
        "\n",
        "《What is image data?》\n",
        "\n",
        "A digital image is a set of points, which are called pixels. Generally, for a black-and-white image, a pixel contains a value from 0 to 255. On the other hand, a color image contains 0 to 255 values corresponding to R (red), G (green), and B (blue), respectively. In machine learning, each of these 0-255 values is treated as a feature value, and since 0-255 can be represented as an unsigned 8-bit integer, NumPy can hold it as a variable of type \"uint8\"."
      ],
      "metadata": {
        "id": "53VYiJkP5y2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the dataset\n",
        "Let's see what the data looks like."
      ],
      "metadata": {
        "id": "xO_D8auV528v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape) # (60000, 28, 28)\n",
        "print(X_test.shape) # (10000, 28, 28)\n",
        "print(X_train[0].dtype) # uint8\n",
        "print(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZqTNMkb536a",
        "outputId": "b64ce1ae-461d-4d7a-920a-472c362857b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "uint8\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each data is a black and white image of 28 x 28 pixels."
      ],
      "metadata": {
        "id": "xijT2egx6DEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "smoothing\n",
        "Each image of (1, 28, 28) is converted to (1, 784). The machine learning methods we have learned so far and the neural networks we will be working with in this article, which only have an all-connected layer, will be handled in this form. The fact that all pixels are in a row is called \"flattened\".\n",
        "\n",
        "《sample code》"
      ],
      "metadata": {
        "id": "zY4Jvkbw6OAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)"
      ],
      "metadata": {
        "id": "GCKPLy3b6PH2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "《 Supplemental information 》\n",
        "\n",
        "As we have been learning about machine learning, we have referred to the number of features as \"dimensions\". From that perspective, MNIST is 784 dimensional data. On the other hand, the state where the NumPy shape is (784,) is also called a 1-dimensional array. If the state of (28, 28) has the information of height and width as an image, it is a 2-dimensional array. From this perspective, the data is two-dimensional. Furthermore, if it is a color image, then it is (28, 28, 3), which is a 3-dimensional array. In the previous viewpoint, the data is 3-dimensional. However, from the perspective that both black-and-white and color images are flat images and not three-dimensional data, they are two-dimensional data. Note that the word \"dimension\" is thus used with multiple connotations when dealing with image data."
      ],
      "metadata": {
        "id": "JzyyfEYj6b2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of image data\n",
        "Visualize image data. Pass toplt.imshow.\n",
        "\n",
        "《sample code》"
      ],
      "metadata": {
        "id": "04de2kaW6d0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "index = 0\n",
        "image = X_train[index].reshape(28,28)\n",
        "# X_train[index]: (784,)\n",
        "# image: (28, 28)\n",
        "plt.imshow(image, 'gray')\n",
        "plt.title('label : {}'.format(y_train[index]))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "_g-QrGv26i6u",
        "outputId": "fd9703c2-3b2d-4a40-e4ec-683f555626b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "link: numpy.reshape — NumPy v1.17 Manual\n",
        "\n",
        "link: matplotlib.pyplot.imshow — Matplotlib 3.1.1 documentation\n",
        "\n",
        "《Developmental Topic》\n",
        "\n",
        "Image data is typically held in unsigned 8-bit integer type uint8, but plt.imshowcan display a more flexible array as an image. For example, a floating point of type float64 with negative values as shown below will not cause an error and will be displayed in exactly the same way as before."
      ],
      "metadata": {
        "id": "gEKxFt7U646w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "index = 0\n",
        "image = X_train[index].reshape(28, 28)\n",
        "image = image.astype(float)  # Convert to float type\n",
        "image -= 105.35  # Intentionally try to create a negative decimal value\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title('label : {}'.format(y_train[index]))\n",
        "plt.show()\n",
        "\n",
        "print(image)  # Check the value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hs2MOMro7M3K",
        "outputId": "d5eff63e-468f-4bd8-86bb-4ea86bbd904d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgiklEQVR4nO3de3BU9fnH8c9yyXJLFsIlIVwDCKjcpggpIggSCdFSQLRotQPVQaHBKijYOApaL1FUVBSFOpaICgozAsp0sAoktAo43GTQkgKNBSQBAbOBAAGS7+8P6v5cCcIJG54kvF8z35nsOd9nz8PhkA9n9+xZn3POCQCAi6yGdQMAgEsTAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBhEtSZmamfD6fvvnmG8+1AwYMUJcuXSLaT9u2bTVmzJiIPidQ2RFAQDXVtm1b+Xy+M8a4ceOsWwMkSbWsGwBQcXr06KEHHnggbFnHjh2NugHCEUBANdaiRQvdcccd1m0AZeIlOOB/li5dqhtvvFEJCQny+/1q3769nnjiCZWUlJQ5f8OGDbr66qtVt25dJSYmavbs2WfMKS4u1rRp09ShQwf5/X61atVKU6ZMUXFxcbl6zMvL07Zt23Ty5Mnzrjlx4oSKiorKtT2gIhFAwP9kZmaqQYMGmjRpkl5++WX17NlTU6dO1Z/+9Kcz5n7//fe64YYb1LNnT02fPl0tW7bU+PHj9de//jU0p7S0VL/+9a/1/PPPa+jQoXrllVc0fPhwvfjiixo1alS5ekxPT9fll1+ub7/99rzmr1y5UvXq1VODBg3Utm1bvfzyy+XaLlAhHHAJmjt3rpPkcnNzQ8uOHj16xrx77rnH1atXzx0/fjy07Nprr3WS3AsvvBBaVlxc7Hr06OGaNWvmTpw44Zxz7u2333Y1atRw//jHP8Kec/bs2U6S++yzz0LL2rRp40aPHn3OvkePHn1G32czdOhQ9+yzz7olS5a4N9980/Xr189JclOmTDlnLXAxcAYE/E/dunVDPx8+fFgHDhxQv379dPToUW3bti1sbq1atXTPPfeEHkdFRemee+7R/v37tWHDBknSokWLdPnll6tz5846cOBAaFx33XWSpFWrVnnuMTMzU845tW3b9pxzP/zwQ02ZMkXDhg3TnXfeqezsbKWkpGjGjBnas2eP520DkUYAAf/z1VdfacSIEQoEAoqJiVHTpk1Db+AHg8GwuQkJCapfv37Ysh+uLvvhs0Xbt2/XV199paZNm4aNH+bt37+/gv9E4Xw+nyZOnKhTp04pKyvrom4bKAtXwQGSCgoKdO211yomJkZ//vOf1b59e9WpU0cbN27UQw89pNLSUs/PWVpaqq5du2rGjBllrm/VqtWFtu3ZD9s8dOjQRd828FMEECApKytLBw8e1AcffKD+/fuHlufm5pY5f+/evSoqKgo7C/r3v/8tSaGXx9q3b68vv/xSgwYNks/nq7jmPfjPf/4jSWratKlxJwAvwQGSpJo1a0qSnHOhZSdOnNBrr71W5vxTp05pzpw5YXPnzJmjpk2bqmfPnpKk3/zmN/r222/1xhtvnFF/7Nixcl0afb6XYR86dOiMy8dPnjypZ555RlFRURo4cKDnbQORxhkQIOnqq69Wo0aNNHr0aP3xj3+Uz+fT22+/HRZIP5aQkKBnn31W33zzjTp27Kj3339fmzdv1l/+8hfVrl1bkvS73/1OCxcu1Lhx47Rq1Sr17dtXJSUl2rZtmxYuXKiPP/5YV111lac+09PT9dZbbyk3N/dnL0T48MMP9eSTT+rmm29WYmKiDh06pPnz52vr1q16+umnFR8f72m7QEUggABJjRs31rJly/TAAw/okUceUaNGjXTHHXdo0KBBSklJOWN+o0aN9NZbb+nee+/VG2+8obi4OL366qsaO3ZsaE6NGjW0ZMkSvfjii5o3b54WL16sevXqqV27drrvvvsq9JY4Xbt21RVXXKF33nlH3333naKiotSjRw8tXLhQt9xyS4VtF/DC5872XzwAACoQ7wEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABOV7nNApaWl2rt3r6KjoyvN7UsAAOfPOafDhw8rISFBNWqc/Tyn0gXQ3r17TW7SCACIrN27d6tly5ZnXV/pXoKLjo62bgEAEAHn+n1eYQE0a9YstW3bVnXq1FFSUpK++OKL86rjZTcAqB7O9fu8QgLo/fff16RJkzRt2jRt3LhR3bt3V0pKykX/Ai4AQCVWEd/z3bt3b5eWlhZ6XFJS4hISElxGRsY5a4PBoJPEYDAYjCo+gsHgz/6+j/gZ0IkTJ7RhwwYlJyeHltWoUUPJyclas2bNGfOLi4tVWFgYNgAA1V/EA+jAgQMqKSlRXFxc2PK4uDjl5+efMT8jI0OBQCA0uAIOAC4N5lfBpaenKxgMhsbu3butWwIAXAQR/xxQkyZNVLNmTe3bty9s+b59+8r8Fka/3y+/3x/pNgAAlVzEz4CioqLUs2dPrVixIrSstLRUK1asUJ8+fSK9OQBAFVUhd0KYNGmSRo8erauuukq9e/fWSy+9pKKiIv3+97+viM0BAKqgCgmgUaNG6bvvvtPUqVOVn5+vHj16aPny5WdcmAAAuHT5nHPOuokfKywsVCAQsG4DAHCBgsGgYmJizrre/Co4AMCliQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJWtYNAJVJzZo1PdcEAoEK6CQyJkyYUK66evXqea7p1KmT55q0tDTPNc8//7znmttuu81zjSQdP37cc80zzzzjuebxxx/3XFMdcAYEADBBAAEATEQ8gB577DH5fL6w0blz50hvBgBQxVXIe0BXXnmlPv300//fSC3eagIAhKuQZKhVq5bi4+Mr4qkBANVEhbwHtH37diUkJKhdu3a6/fbbtWvXrrPOLS4uVmFhYdgAAFR/EQ+gpKQkZWZmavny5Xr99deVm5urfv366fDhw2XOz8jIUCAQCI1WrVpFuiUAQCUU8QBKTU3VLbfcom7duiklJUV/+9vfVFBQoIULF5Y5Pz09XcFgMDR2794d6ZYAAJVQhV8d0LBhQ3Xs2FE7duwoc73f75ff76/oNgAAlUyFfw7oyJEj2rlzp5o3b17RmwIAVCERD6AHH3xQ2dnZ+uabb/T5559rxIgRqlmzZrlvhQEAqJ4i/hLcnj17dNttt+ngwYNq2rSprrnmGq1du1ZNmzaN9KYAAFVYxAPovffei/RTopJq3bq155qoqCjPNVdffbXnmmuuucZzjXT6PUuvRo4cWa5tVTd79uzxXDNz5kzPNSNGjPBcc7arcM/lyy+/9FyTnZ1drm1dirgXHADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABM+55yzbuLHCgsLFQgErNu4pPTo0aNcdStXrvRcw99t1VBaWuq55s477/Rcc+TIEc815ZGXl1euuu+//95zTU5OTrm2VR0Fg0HFxMScdT1nQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE7WsG4C9Xbt2lavu4MGDnmu4G/Zp69at81xTUFDguWbgwIGeayTpxIkTnmvefvvtcm0Lly7OgAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjgZqTQoUOHylU3efJkzzW/+tWvPNds2rTJc83MmTM915TX5s2bPddcf/31nmuKioo811x55ZWeayTpvvvuK1cd4AVnQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEz4nHPOuokfKywsVCAQsG4DFSQmJsZzzeHDhz3XzJkzx3ONJN11112ea+644w7PNQsWLPBcA1Q1wWDwZ//NcwYEADBBAAEATHgOoNWrV2vo0KFKSEiQz+fTkiVLwtY75zR16lQ1b95cdevWVXJysrZv3x6pfgEA1YTnACoqKlL37t01a9asMtdPnz5dM2fO1OzZs7Vu3TrVr19fKSkpOn78+AU3CwCoPjx/I2pqaqpSU1PLXOec00svvaRHHnlEw4YNkyTNmzdPcXFxWrJkiW699dYL6xYAUG1E9D2g3Nxc5efnKzk5ObQsEAgoKSlJa9asKbOmuLhYhYWFYQMAUP1FNIDy8/MlSXFxcWHL4+LiQut+KiMjQ4FAIDRatWoVyZYAAJWU+VVw6enpCgaDobF7927rlgAAF0FEAyg+Pl6StG/fvrDl+/btC637Kb/fr5iYmLABAKj+IhpAiYmJio+P14oVK0LLCgsLtW7dOvXp0yeSmwIAVHGer4I7cuSIduzYEXqcm5urzZs3KzY2Vq1bt9b999+vJ598UpdddpkSExP16KOPKiEhQcOHD49k3wCAKs5zAK1fv14DBw4MPZ40aZIkafTo0crMzNSUKVNUVFSku+++WwUFBbrmmmu0fPly1alTJ3JdAwCqPG5GimrpueeeK1fdD/+h8iI7O9tzzY8/qnC+SktLPdcAlrgZKQCgUiKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmOBu2KiW6tevX666jz76yHPNtdde67kmNTXVc83f//53zzWAJe6GDQColAggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjgZqTAj7Rv395zzcaNGz3XFBQUeK5ZtWqV55r169d7rpGkWbNmea6pZL9KUAlwM1IAQKVEAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABDcjBS7QiBEjPNfMnTvXc010dLTnmvJ6+OGHPdfMmzfPc01eXp7nGlQd3IwUAFApEUAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMHNSAEDXbp08VwzY8YMzzWDBg3yXFNec+bM8Vzz1FNPea759ttvPdfABjcjBQBUSgQQAMCE5wBavXq1hg4dqoSEBPl8Pi1ZsiRs/ZgxY+Tz+cLGkCFDItUvAKCa8BxARUVF6t69u2bNmnXWOUOGDFFeXl5oLFiw4IKaBABUP7W8FqSmpio1NfVn5/j9fsXHx5e7KQBA9Vch7wFlZWWpWbNm6tSpk8aPH6+DBw+edW5xcbEKCwvDBgCg+ot4AA0ZMkTz5s3TihUr9Oyzzyo7O1upqakqKSkpc35GRoYCgUBotGrVKtItAQAqIc8vwZ3LrbfeGvq5a9eu6tatm9q3b6+srKwyP5OQnp6uSZMmhR4XFhYSQgBwCajwy7DbtWunJk2aaMeOHWWu9/v9iomJCRsAgOqvwgNoz549OnjwoJo3b17RmwIAVCGeX4I7cuRI2NlMbm6uNm/erNjYWMXGxurxxx/XyJEjFR8fr507d2rKlCnq0KGDUlJSIto4AKBq8xxA69ev18CBA0OPf3j/ZvTo0Xr99de1ZcsWvfXWWyooKFBCQoIGDx6sJ554Qn6/P3JdAwCqPG5GClQRDRs29FwzdOjQcm1r7ty5nmt8Pp/nmpUrV3quuf766z3XwAY3IwUAVEoEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABPcDRvAGYqLiz3X1Krl+dtddOrUKc815flusaysLM81uHDcDRsAUCkRQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAw4f3ugQAuWLdu3TzX3HzzzZ5revXq5blGKt+NRcvj66+/9lyzevXqCugEFjgDAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIKbkQI/0qlTJ881EyZM8Fxz0003ea6Jj4/3XHMxlZSUeK7Jy8vzXFNaWuq5BpUTZ0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMcDNSVHrluQnnbbfdVq5tlefGom3bti3Xtiqz9evXe6556qmnPNd8+OGHnmtQfXAGBAAwQQABAEx4CqCMjAz16tVL0dHRatasmYYPH66cnJywOcePH1daWpoaN26sBg0aaOTIkdq3b19EmwYAVH2eAig7O1tpaWlau3atPvnkE508eVKDBw9WUVFRaM7EiRP10UcfadGiRcrOztbevXvL9eVbAIDqzdNFCMuXLw97nJmZqWbNmmnDhg3q37+/gsGg3nzzTc2fP1/XXXedJGnu3Lm6/PLLtXbtWv3yl7+MXOcAgCrtgt4DCgaDkqTY2FhJ0oYNG3Ty5EklJyeH5nTu3FmtW7fWmjVrynyO4uJiFRYWhg0AQPVX7gAqLS3V/fffr759+6pLly6SpPz8fEVFRalhw4Zhc+Pi4pSfn1/m82RkZCgQCIRGq1atytsSAKAKKXcApaWlaevWrXrvvfcuqIH09HQFg8HQ2L179wU9HwCgaijXB1EnTJigZcuWafXq1WrZsmVoeXx8vE6cOKGCgoKws6B9+/ad9cOEfr9ffr+/PG0AAKowT2dAzjlNmDBBixcv1sqVK5WYmBi2vmfPnqpdu7ZWrFgRWpaTk6Ndu3apT58+kekYAFAteDoDSktL0/z587V06VJFR0eH3tcJBAKqW7euAoGA7rrrLk2aNEmxsbGKiYnRvffeqz59+nAFHAAgjKcAev311yVJAwYMCFs+d+5cjRkzRpL04osvqkaNGho5cqSKi4uVkpKi1157LSLNAgCqD59zzlk38WOFhYUKBALWbeA8xMXFea654oorPNe8+uqrnms6d+7suaayW7duneea5557rlzbWrp0qeea0tLScm0L1VcwGFRMTMxZ13MvOACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAiXJ9Iyoqr9jYWM81c+bMKde2evTo4bmmXbt25dpWZfb55597rnnhhRc813z88ceea44dO+a5BrhYOAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABggpuRXiRJSUmeayZPnuy5pnfv3p5rWrRo4bmmsjt69Gi56mbOnOm55umnn/ZcU1RU5LkGqG44AwIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCm5FeJCNGjLgoNRfT119/7blm2bJlnmtOnTrlueaFF17wXCNJBQUF5aoD4B1nQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEz4nHPOuokfKywsVCAQsG4DAHCBgsGgYmJizrqeMyAAgAkCCABgwlMAZWRkqFevXoqOjlazZs00fPhw5eTkhM0ZMGCAfD5f2Bg3blxEmwYAVH2eAig7O1tpaWlau3atPvnkE508eVKDBw9WUVFR2LyxY8cqLy8vNKZPnx7RpgEAVZ+nb0Rdvnx52OPMzEw1a9ZMGzZsUP/+/UPL69Wrp/j4+Mh0CAColi7oPaBgMChJio2NDVv+7rvvqkmTJurSpYvS09N19OjRsz5HcXGxCgsLwwYA4BLgyqmkpMTdeOONrm/fvmHL58yZ45YvX+62bNni3nnnHdeiRQs3YsSIsz7PtGnTnCQGg8FgVLMRDAZ/NkfKHUDjxo1zbdq0cbt37/7ZeStWrHCS3I4dO8pcf/z4cRcMBkNj9+7d5juNwWAwGBc+zhVAnt4D+sGECRO0bNkyrV69Wi1btvzZuUlJSZKkHTt2qH379mes9/v98vv95WkDAFCFeQog55zuvfdeLV68WFlZWUpMTDxnzebNmyVJzZs3L1eDAIDqyVMApaWlaf78+Vq6dKmio6OVn58vSQoEAqpbt6527typ+fPn64YbblDjxo21ZcsWTZw4Uf3791e3bt0q5A8AAKiivLzvo7O8zjd37lznnHO7du1y/fv3d7Gxsc7v97sOHTq4yZMnn/N1wB8LBoPmr1syGAwG48LHuX73czNSAECF4GakAIBKiQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgotIFkHPOugUAQASc6/d5pQugw4cPW7cAAIiAc/0+97lKdspRWlqqvXv3Kjo6Wj6fL2xdYWGhWrVqpd27dysmJsaoQ3vsh9PYD6exH05jP5xWGfaDc06HDx9WQkKCatQ4+3lOrYvY03mpUaOGWrZs+bNzYmJiLukD7Afsh9PYD6exH05jP5xmvR8CgcA551S6l+AAAJcGAggAYKJKBZDf79e0adPk9/utWzHFfjiN/XAa++E09sNpVWk/VLqLEAAAl4YqdQYEAKg+CCAAgAkCCABgggACAJgggAAAJqpMAM2aNUtt27ZVnTp1lJSUpC+++MK6pYvusccek8/nCxudO3e2bqvCrV69WkOHDlVCQoJ8Pp+WLFkStt45p6lTp6p58+aqW7eukpOTtX37dptmK9C59sOYMWPOOD6GDBli02wFycjIUK9evRQdHa1mzZpp+PDhysnJCZtz/PhxpaWlqXHjxmrQoIFGjhypffv2GXVcMc5nPwwYMOCM42HcuHFGHZetSgTQ+++/r0mTJmnatGnauHGjunfvrpSUFO3fv9+6tYvuyiuvVF5eXmj885//tG6pwhUVFal79+6aNWtWmeunT5+umTNnavbs2Vq3bp3q16+vlJQUHT9+/CJ3WrHOtR8kaciQIWHHx4IFCy5ihxUvOztbaWlpWrt2rT755BOdPHlSgwcPVlFRUWjOxIkT9dFHH2nRokXKzs7W3r17ddNNNxl2HXnnsx8kaezYsWHHw/Tp0406PgtXBfTu3dulpaWFHpeUlLiEhASXkZFh2NXFN23aNNe9e3frNkxJcosXLw49Li0tdfHx8e65554LLSsoKHB+v98tWLDAoMOL46f7wTnnRo8e7YYNG2bSj5X9+/c7SS47O9s5d/rvvnbt2m7RokWhOf/617+cJLdmzRqrNivcT/eDc85de+217r777rNr6jxU+jOgEydOaMOGDUpOTg4tq1GjhpKTk7VmzRrDzmxs375dCQkJateunW6//Xbt2rXLuiVTubm5ys/PDzs+AoGAkpKSLsnjIysrS82aNVOnTp00fvx4HTx40LqlChUMBiVJsbGxkqQNGzbo5MmTYcdD586d1bp162p9PPx0P/zg3XffVZMmTdSlSxelp6fr6NGjFu2dVaW7G/ZPHThwQCUlJYqLiwtbHhcXp23bthl1ZSMpKUmZmZnq1KmT8vLy9Pjjj6tfv37aunWroqOjrdszkZ+fL0llHh8/rLtUDBkyRDfddJMSExO1c+dOPfzww0pNTdWaNWtUs2ZN6/YirrS0VPfff7/69u2rLl26SDp9PERFRalhw4Zhc6vz8VDWfpCk3/72t2rTpo0SEhK0ZcsWPfTQQ8rJydEHH3xg2G24Sh9A+H+pqamhn7t166akpCS1adNGCxcu1F133WXYGSqDW2+9NfRz165d1a1bN7Vv315ZWVkaNGiQYWcVIy0tTVu3br0k3gf9OWfbD3fffXfo565du6p58+YaNGiQdu7cqfbt21/sNstU6V+Ca9KkiWrWrHnGVSz79u1TfHy8UVeVQ8OGDdWxY0ft2LHDuhUzPxwDHB9nateunZo0aVItj48JEyZo2bJlWrVqVdj3h8XHx+vEiRMqKCgIm19dj4ez7YeyJCUlSVKlOh4qfQBFRUWpZ8+eWrFiRWhZaWmpVqxYoT59+hh2Zu/IkSPauXOnmjdvbt2KmcTERMXHx4cdH4WFhVq3bt0lf3zs2bNHBw8erFbHh3NOEyZM0OLFi7Vy5UolJiaGre/Zs6dq164ddjzk5ORo165d1ep4ONd+KMvmzZslqXIdD9ZXQZyP9957z/n9fpeZmem+/vprd/fdd7uGDRu6/Px869YuqgceeMBlZWW53Nxc99lnn7nk5GTXpEkTt3//fuvWKtThw4fdpk2b3KZNm5wkN2PGDLdp0yb33//+1znn3DPPPOMaNmzoli5d6rZs2eKGDRvmEhMT3bFjx4w7j6yf2w+HDx92Dz74oFuzZo3Lzc11n376qfvFL37hLrvsMnf8+HHr1iNm/PjxLhAIuKysLJeXlxcaR48eDc0ZN26ca926tVu5cqVbv36969Onj+vTp49h15F3rv2wY8cO9+c//9mtX7/e5ebmuqVLl7p27dq5/v37G3cerkoEkHPOvfLKK65169YuKirK9e7d261du9a6pYtu1KhRrnnz5i4qKsq1aNHCjRo1yu3YscO6rQq3atUqJ+mMMXr0aOfc6UuxH330URcXF+f8fr8bNGiQy8nJsW26Avzcfjh69KgbPHiwa9q0qatdu7Zr06aNGzt2bLX7T1pZf35Jbu7cuaE5x44dc3/4wx9co0aNXL169dyIESNcXl6eXdMV4Fz7YdeuXa5///4uNjbW+f1+16FDBzd58mQXDAZtG/8Jvg8IAGCi0r8HBACongggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABg4v8AdMucyrqBf0QAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -102.35  -87.35  -87.35  -87.35   20.65   30.65\n",
            "    69.65  -79.35   60.65  149.65  141.65   21.65 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -75.35\n",
            "   -69.35  -11.35   48.65   64.65  147.65  147.65  147.65  147.65  147.65\n",
            "   119.65   66.65  147.65  136.65   89.65  -41.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -56.35  132.65\n",
            "   147.65  147.65  147.65  147.65  147.65  147.65  147.65  147.65  145.65\n",
            "   -12.35  -23.35  -23.35  -49.35  -66.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35  113.65\n",
            "   147.65  147.65  147.65  147.65  147.65   92.65   76.65  141.65  135.65\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -25.35\n",
            "    50.65    1.65  147.65  147.65   99.65  -94.35 -105.35  -62.35   48.65\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "   -91.35 -104.35   48.65  147.65  -15.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35   33.65  147.65   84.65 -103.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35  -94.35   84.65  147.65  -35.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35  -70.35  135.65  119.65   54.65    2.65 -104.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35  -24.35  134.65  147.65  147.65   13.65\n",
            "   -80.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35  -60.35   80.65  147.65  147.65\n",
            "    44.65  -78.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -89.35  -12.35  146.65\n",
            "   147.65   81.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  143.65\n",
            "   147.65  143.65  -41.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35  -59.35   24.65   77.65  147.65\n",
            "   147.65  101.65 -103.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35  -66.35   42.65  123.65  147.65  147.65  147.65\n",
            "   144.65   76.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35  -81.35    8.65  115.65  147.65  147.65  147.65  147.65   95.65\n",
            "   -27.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -82.35\n",
            "   -39.35  107.65  147.65  147.65  147.65  147.65   92.65  -24.35 -103.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35   65.65  113.65\n",
            "   147.65  147.65  147.65  147.65   89.65  -25.35  -96.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35  -50.35   66.65  120.65  147.65  147.65\n",
            "   147.65  147.65  138.65   27.65  -94.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35   30.65  147.65  147.65  147.65  106.65\n",
            "    29.65   26.65  -89.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is because it is designed to automatically convert the value to an integer between 0 and 255 and process it; even if it is of type uint8, if the minimum value is not 0 and the maximum value is not 255, the tint will be wrong. To prevent this, include the following argument"
      ],
      "metadata": {
        "id": "KbkunPkh8fWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "index = 0\n",
        "image = X_train[index].reshape(28, 28)\n",
        "image = image.astype(float)  # Convert to float type\n",
        "image -= 105.35  # Intentionally try to create a negative decimal value\n",
        "\n",
        "plt.imshow(image, 'gray', vmin = 0, vmax = 255) #here made adjustments\n",
        "plt.title('label : {}'.format(y_train[index]))\n",
        "plt.show()\n",
        "\n",
        "print(image)  # Check the value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LLu5k5yx8gSa",
        "outputId": "c6c0a020-9a08-46e8-b8fb-dacbf0182186"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfo0lEQVR4nO3de3BU9fnH8c9yyQKSLIRAwsotgIDKbUSIiNwkQ0CHcqvFWye0DgoNqIBi4yh4a6O09VoUai3xBgodAWU6dDCQ0CqXAUEGLRRoKOESEJRNCCZg8v39Qd2fK+GyYZcnCe/XzHcme8732fNwepqPZ8/JWY9zzgkAgEusjnUDAIDLEwEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAYTLUnZ2tjwej/bs2RN27aBBg9S1a9eI9tOuXTuNHz8+ou8JVHcEEFBLtWvXTh6P54wxceJE69YASVI96wYARE/Pnj01ffr0kGWdOnUy6gYIRQABtdiVV16pu+++27oNoFJ8BAf8z7Jly3TrrbfK7/fL6/WqQ4cOevrpp1VeXl7p/E2bNunGG29Uw4YNlZycrLlz554xp6ysTLNmzVLHjh3l9XrVunVrzZgxQ2VlZVXq8eDBg9q+fbtOnTp1wTUnT55USUlJlbYHRBMBBPxPdna2GjdurGnTpumll15Sr169NHPmTP36178+Y+4333yjW265Rb169dLs2bPVqlUrTZo0SX/5y1+CcyoqKvSTn/xEv//97zVixAi98sorGjVqlF544QWNGzeuSj1mZmbq6quv1v79+y9o/qpVq9SoUSM1btxY7dq100svvVSl7QJR4YDL0Pz5850kl5+fH1x24sSJM+bdd999rlGjRq60tDS4bODAgU6S+8Mf/hBcVlZW5nr27OlatGjhTp486Zxz7u2333Z16tRx//jHP0Lec+7cuU6S++STT4LL2rZt69LT08/bd3p6+hl9n82IESPcc88955YuXereeOMN179/fyfJzZgx47y1wKXAGRDwPw0bNgz+XFxcrCNHjqh///46ceKEtm/fHjK3Xr16uu+++4KvY2JidN999+nw4cPatGmTJGnx4sW6+uqr1aVLFx05ciQ4br75ZknS6tWrw+4xOztbzjm1a9fuvHM//PBDzZgxQyNHjtQvf/lL5eXlKS0tTc8//7z27dsX9raBSCOAgP/54osvNHr0aPl8PsXFxal58+bBC/iBQCBkrt/v1xVXXBGy7Pu7y77/26KdO3fqiy++UPPmzUPG9/MOHz4c5X9RKI/Ho6lTp+q7775Tbm7uJd02UBnuggMkHTt2TAMHDlRcXJyeeuopdejQQQ0aNNBnn32mRx55RBUVFWG/Z0VFhbp166bnn3++0vWtW7e+2LbD9v02v/7660u+beDHCCBAUm5uro4ePaoPPvhAAwYMCC7Pz8+vdP6BAwdUUlISchb073//W5KCH4916NBBn3/+uYYMGSKPxxO95sPwn//8R5LUvHlz404APoIDJEl169aVJDnngstOnjypV199tdL53333nebNmxcyd968eWrevLl69eolSfrZz36m/fv36/XXXz+j/ttvv63SrdEXehv2119/fcbt46dOndKzzz6rmJgYDR48OOxtA5HGGRAg6cYbb1TTpk2Vnp6u+++/Xx6PR2+//XZIIP2Q3+/Xc889pz179qhTp056//33tWXLFv3pT39S/fr1JUk///nPtWjRIk2cOFGrV69Wv379VF5eru3bt2vRokX6+9//ruuvvz6sPjMzM/Xmm28qPz//nDcifPjhh3rmmWf005/+VMnJyfr666+1YMECbdu2Tb/97W+VlJQU1naBaCCAAEnNmjXT8uXLNX36dD322GNq2rSp7r77bg0ZMkRpaWlnzG/atKnefPNNTZkyRa+//roSExP1xz/+URMmTAjOqVOnjpYuXaoXXnhBb731lpYsWaJGjRqpffv2euCBB6L6SJxu3brpmmuu0TvvvKOvvvpKMTEx6tmzpxYtWqTbbrstatsFwuFxZ/tPPAAAoohrQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARLX7O6CKigodOHBAsbGx1ebxJQCAC+ecU3Fxsfx+v+rUOft5TrULoAMHDpg8pBEAEFkFBQVq1arVWddXu4/gYmNjrVsAAETA+X6fRy2A5syZo3bt2qlBgwZKSUnRhg0bLqiOj90AoHY43+/zqATQ+++/r2nTpmnWrFn67LPP1KNHD6WlpV3yL+ACAFRj0fie7z59+riMjIzg6/Lycuf3+11WVtZ5awOBgJPEYDAYjBo+AoHAOX/fR/wM6OTJk9q0aZNSU1ODy+rUqaPU1FStXbv2jPllZWUqKioKGQCA2i/iAXTkyBGVl5crMTExZHliYqIKCwvPmJ+VlSWfzxcc3AEHAJcH87vgMjMzFQgEgqOgoMC6JQDAJRDxvwNKSEhQ3bp1dejQoZDlhw4dqvRbGL1er7xeb6TbAABUcxE/A4qJiVGvXr2Uk5MTXFZRUaGcnBz17ds30psDANRQUXkSwrRp05Senq7rr79effr00YsvvqiSkhL94he/iMbmAAA1UFQCaNy4cfrqq680c+ZMFRYWqmfPnlqxYsUZNyYAAC5fHuecs27ih4qKiuTz+azbAABcpEAgoLi4uLOuN78LDgBweSKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgop51AwCiJyEhoUp1jRo1Crumc+fOYdesXLky7Jqbbrop7Jo777wz7BpJKi0tDbsmKysr7Jqvvvoq7JragDMgAIAJAggAYCLiAfTEE0/I4/GEjC5dukR6MwCAGi4q14CuvfZaffzxx/+/kXpcagIAhIpKMtSrV09JSUnReGsAQC0RlWtAO3fulN/vV/v27XXXXXdp7969Z51bVlamoqKikAEAqP0iHkApKSnKzs7WihUr9Nprryk/P1/9+/dXcXFxpfOzsrLk8/mCo3Xr1pFuCQBQDUU8gIYPH67bbrtN3bt3V1pamv72t7/p2LFjWrRoUaXzMzMzFQgEgqOgoCDSLQEAqqGo3x3QpEkTderUSbt27ap0vdfrldfrjXYbAIBqJup/B3T8+HHt3r1bLVu2jPamAAA1SMQD6KGHHlJeXp727NmjTz/9VKNHj1bdunV1xx13RHpTAIAaLOIfwe3bt0933HGHjh49qubNm+umm27SunXr1Lx580hvCgBQg0U8gN57771IvyUQtp49e1aprkmTJmHXjB07tkrbqm32798fds13330Xds2YMWPCrjnbXbjns2XLlrBrLtcHi1YFz4IDAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgwuOcc9ZN/FBRUZF8Pp91G7gAv/nNb8KuiYuLi0IniLSq/Fq4//77o9AJarJAIHDO/89zBgQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMFHPugHUXEeOHAm7hqdhn7Zhw4awa7755puwa26++eawayTp5MmTVaoDwsEZEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMe55yzbuKHioqK5PP5rNtAlFx33XVh12zevDnsmpdffjnsmqr6/PPPw67585//HIVOIqdnz55h12zZsiXifaBmCwQC53wAMWdAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATPAwUtRKVXmYpiTdc889YddMmTKlStsCajseRgoAqJYIIACAibADaM2aNRoxYoT8fr88Ho+WLl0ast45p5kzZ6ply5Zq2LChUlNTtXPnzkj1CwCoJcIOoJKSEvXo0UNz5sypdP3s2bP18ssva+7cuVq/fr2uuOIKpaWlqbS09KKbBQDUHvXCLRg+fLiGDx9e6TrnnF588UU99thjGjlypCTprbfeUmJiopYuXarbb7/94roFANQaEb0GlJ+fr8LCQqWmpgaX+Xw+paSkaO3atZXWlJWVqaioKGQAAGq/iAZQYWGhJCkxMTFkeWJiYnDdj2VlZcnn8wVH69atI9kSAKCaMr8LLjMzU4FAIDgKCgqsWwIAXAIRDaCkpCRJ0qFDh0KWHzp0KLjux7xer+Li4kIGAKD2i2gAJScnKykpSTk5OcFlRUVFWr9+vfr27RvJTQEAariw74I7fvy4du3aFXydn5+vLVu2KD4+Xm3atNGDDz6oZ555RldddZWSk5P1+OOPy+/3a9SoUZHsGwBQw4UdQBs3btTgwYODr6dNmyZJSk9PV3Z2tmbMmKGSkhLde++9OnbsmG666SatWLFCDRo0iFzXAIAaj4eRolZq165dleqmT58edk1eXl7YNX/961/DrgFqGh5GCgColgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJsL+OgagJtizZ0+V6qryZOuBAweGXcPTsAHOgAAARgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjwOOecdRM/VFRUJJ/PZ90GcMGeffbZsGuOHTsWds2qVavCrtm4cWPYNZJUUVFRpTrghwKBgOLi4s66njMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJngYKWAgKysr7JrGjRtHoZPKPfroo2HXFBcXR6ET1GQ8jBQAUC0RQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwcNIgRpi1KhRYdcMGTIk8o2cxbx588Ku2bZtWxQ6QXXBw0gBANUSAQQAMBF2AK1Zs0YjRoyQ3++Xx+PR0qVLQ9aPHz9eHo8nZAwbNixS/QIAaomwA6ikpEQ9evTQnDlzzjpn2LBhOnjwYHAsXLjwopoEANQ+9cItGD58uIYPH37OOV6vV0lJSVVuCgBQ+0XlGlBubq5atGihzp07a9KkSTp69OhZ55aVlamoqChkAABqv4gH0LBhw/TWW28pJydHzz33nPLy8jR8+HCVl5dXOj8rK0s+ny84WrduHemWAADVUNgfwZ3P7bffHvy5W7du6t69uzp06KDc3NxK/yYhMzNT06ZNC74uKioihADgMhD127Dbt2+vhIQE7dq1q9L1Xq9XcXFxIQMAUPtFPYD27duno0ePqmXLltHeFACgBgn7I7jjx4+HnM3k5+dry5Ytio+PV3x8vJ588kmNHTtWSUlJ2r17t2bMmKGOHTsqLS0too0DAGq2sANo48aNGjx4cPD199dv0tPT9dprr2nr1q168803dezYMfn9fg0dOlRPP/20vF5v5LoGANR4PIwUwBleeeWVS7KdVatWhV2zZMmSKHSCaOBhpACAaokAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYCLiX8kNoOYrLy8Pu6Zu3bph1wwcODDsGp6GXXtwBgQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEDyMFaoikpKSwa3r37l2lbVXlwaJV8eWXX16S7aB64gwIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACR5GClyka665JuyaMWPGhF2TmJgYds2lVFFREXbNgQMHotAJagrOgAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjgYaSolRo0aFClusmTJ4dd07Zt2yptqzrbtGlT2DXZ2dmRbwS1GmdAAAATBBAAwERYAZSVlaXevXsrNjZWLVq00KhRo7Rjx46QOaWlpcrIyFCzZs3UuHFjjR07VocOHYpo0wCAmi+sAMrLy1NGRobWrVunlStX6tSpUxo6dKhKSkqCc6ZOnaqPPvpIixcvVl5eng4cOFClL98CANRuYd2EsGLFipDX2dnZatGihTZt2qQBAwYoEAjojTfe0IIFC3TzzTdLkubPn6+rr75a69at0w033BC5zgEANdpFXQMKBAKSpPj4eEmn75w5deqUUlNTg3O6dOmiNm3aaO3atZW+R1lZmYqKikIGAKD2q3IAVVRU6MEHH1S/fv3UtWtXSVJhYaFiYmLUpEmTkLmJiYkqLCys9H2ysrLk8/mCo3Xr1lVtCQBQg1Q5gDIyMrRt2za99957F9VAZmamAoFAcBQUFFzU+wEAaoYq/SHq5MmTtXz5cq1Zs0atWrUKLk9KStLJkyd17NixkLOgQ4cOKSkpqdL38nq98nq9VWkDAFCDhXUG5JzT5MmTtWTJEq1atUrJyckh63v16qX69esrJycnuGzHjh3au3ev+vbtG5mOAQC1QlhnQBkZGVqwYIGWLVum2NjY4HUdn8+nhg0byufz6Z577tG0adMUHx+vuLg4TZkyRX379uUOOABAiLAC6LXXXpMkDRo0KGT5/PnzNX78eEnSCy+8oDp16mjs2LEqKytTWlqaXn311Yg0CwCoPTzOOWfdxA8VFRXJ5/NZt4FqpCoPCO3cuXMUOrG1YcOGsGvefvvtKHQCXJhAIKC4uLizrudZcAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE1X6RlTULoMHD65SXc+ePcOuad++fZW2VZ19+umnYdcsXLgwCp0ANQtnQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEzwMNJqrCoPCb3hhhvCrvH7/WHXVHelpaVVqnvxxRfDrtm/f3+VtgVc7jgDAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIKHkV4ibdq0CbtmzJgxUegkcrZv3x52zYcffhh2TXl5edg1Bw4cCLsGwKXFGRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATHuecs27ih4qKiuTz+azbAABcpEAgoLi4uLOu5wwIAGCCAAIAmAgrgLKystS7d2/FxsaqRYsWGjVqlHbs2BEyZ9CgQfJ4PCFj4sSJEW0aAFDzhRVAeXl5ysjI0Lp167Ry5UqdOnVKQ4cOVUlJSci8CRMm6ODBg8Exe/bsiDYNAKj5wvpG1BUrVoS8zs7OVosWLbRp0yYNGDAguLxRo0ZKSkqKTIcAgFrpoq4BBQIBSVJ8fHzI8nfffVcJCQnq2rWrMjMzdeLEibO+R1lZmYqKikIGAOAy4KqovLzc3Xrrra5fv34hy+fNm+dWrFjhtm7d6t555x135ZVXutGjR5/1fWbNmuUkMRgMBqOWjUAgcM4cqXIATZw40bVt29YVFBScc15OTo6T5Hbt2lXp+tLSUhcIBIKjoKDAfKcxGAwG4+LH+QIorGtA35s8ebKWL1+uNWvWqFWrVuecm5KSIknatWuXOnTocMZ6r9crr9dblTYAADVYWAHknNOUKVO0ZMkS5ebmKjk5+bw1W7ZskSS1bNmySg0CAGqnsAIoIyNDCxYs0LJlyxQbG6vCwkJJks/nU8OGDbV7924tWLBAt9xyi5o1a6atW7dq6tSpGjBggLp37x6VfwAAoIYK57qPzvI53/z5851zzu3du9cNGDDAxcfHO6/X6zp27Ogefvjh834O+EOBQMD8c0sGg8FgXPw43+9+HkYKAIgKHkYKAKiWCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmql0AOeesWwAARMD5fp9XuwAqLi62bgEAEAHn+33ucdXslKOiokIHDhxQbGysPB5PyLqioiK1bt1aBQUFiouLM+rQHvvhNPbDaeyH09gPp1WH/eCcU3Fxsfx+v+rUOft5Tr1L2NMFqVOnjlq1anXOOXFxcZf1AfY99sNp7IfT2A+nsR9Os94PPp/vvHOq3UdwAIDLAwEEADBRowLI6/Vq1qxZ8nq91q2YYj+cxn44jf1wGvvhtJq0H6rdTQgAgMtDjToDAgDUHgQQAMAEAQQAMEEAAQBMEEAAABM1JoDmzJmjdu3aqUGDBkpJSdGGDRusW7rknnjiCXk8npDRpUsX67aibs2aNRoxYoT8fr88Ho+WLl0ast45p5kzZ6ply5Zq2LChUlNTtXPnTptmo+h8+2H8+PFnHB/Dhg2zaTZKsrKy1Lt3b8XGxqpFixYaNWqUduzYETKntLRUGRkZatasmRo3bqyxY8fq0KFDRh1Hx4Xsh0GDBp1xPEycONGo48rViAB6//33NW3aNM2aNUufffaZevToobS0NB0+fNi6tUvu2muv1cGDB4Pjn//8p3VLUVdSUqIePXpozpw5la6fPXu2Xn75Zc2dO1fr16/XFVdcobS0NJWWll7iTqPrfPtBkoYNGxZyfCxcuPASdhh9eXl5ysjI0Lp167Ry5UqdOnVKQ4cOVUlJSXDO1KlT9dFHH2nx4sXKy8vTgQMHNGbMGMOuI+9C9oMkTZgwIeR4mD17tlHHZ+FqgD59+riMjIzg6/Lycuf3+11WVpZhV5ferFmzXI8ePazbMCXJLVmyJPi6oqLCJSUlud/97nfBZceOHXNer9ctXLjQoMNL48f7wTnn0tPT3ciRI036sXL48GEnyeXl5TnnTv9vX79+fbd48eLgnH/9619Oklu7dq1Vm1H34/3gnHMDBw50DzzwgF1TF6DanwGdPHlSmzZtUmpqanBZnTp1lJqaqrVr1xp2ZmPnzp3y+/1q37697rrrLu3du9e6JVP5+fkqLCwMOT58Pp9SUlIuy+MjNzdXLVq0UOfOnTVp0iQdPXrUuqWoCgQCkqT4+HhJ0qZNm3Tq1KmQ46FLly5q06ZNrT4efrwfvvfuu+8qISFBXbt2VWZmpk6cOGHR3llVu6dh/9iRI0dUXl6uxMTEkOWJiYnavn27UVc2UlJSlJ2drc6dO+vgwYN68skn1b9/f23btk2xsbHW7ZkoLCyUpEqPj+/XXS6GDRumMWPGKDk5Wbt379ajjz6q4cOHa+3atapbt651exFXUVGhBx98UP369VPXrl0lnT4eYmJi1KRJk5C5tfl4qGw/SNKdd96ptm3byu/3a+vWrXrkkUe0Y8cOffDBB4bdhqr2AYT/N3z48ODP3bt3V0pKitq2batFixbpnnvuMewM1cHtt98e/Llbt27q3r27OnTooNzcXA0ZMsSws+jIyMjQtm3bLovroOdytv1w7733Bn/u1q2bWrZsqSFDhmj37t3q0KHDpW6zUtX+I7iEhATVrVv3jLtYDh06pKSkJKOuqocmTZqoU6dO2rVrl3UrZr4/Bjg+ztS+fXslJCTUyuNj8uTJWr58uVavXh3y/WFJSUk6efKkjh07FjK/th4PZ9sPlUlJSZGkanU8VPsAiomJUa9evZSTkxNcVlFRoZycHPXt29ewM3vHjx/X7t271bJlS+tWzCQnJyspKSnk+CgqKtL69esv++Nj3759Onr0aK06Ppxzmjx5spYsWaJVq1YpOTk5ZH2vXr1Uv379kONhx44d2rt3b606Hs63HyqzZcsWSapex4P1XRAX4r333nNer9dlZ2e7L7/80t17772uSZMmrrCw0Lq1S2r69OkuNzfX5efnu08++cSlpqa6hIQEd/jwYevWoqq4uNht3rzZbd682Ulyzz//vNu8ebP773//65xz7tlnn3VNmjRxy5Ytc1u3bnUjR450ycnJ7ttvvzXuPLLOtR+Ki4vdQw895NauXevy8/Pdxx9/7K677jp31VVXudLSUuvWI2bSpEnO5/O53Nxcd/DgweA4ceJEcM7EiRNdmzZt3KpVq9zGjRtd3759Xd++fQ27jrzz7Yddu3a5p556ym3cuNHl5+e7ZcuWufbt27sBAwYYdx6qRgSQc8698sorrk2bNi4mJsb16dPHrVu3zrqlS27cuHGuZcuWLiYmxl155ZVu3LhxbteuXdZtRd3q1audpDNGenq6c+70rdiPP/64S0xMdF6v1w0ZMsTt2LHDtukoONd+OHHihBs6dKhr3ry5q1+/vmvbtq2bMGFCrfuPtMr+/ZLc/Pnzg3O+/fZb96tf/co1bdrUNWrUyI0ePdodPHjQrukoON9+2Lt3rxswYICLj493Xq/XdezY0T388MMuEAjYNv4jfB8QAMBEtb8GBAConQggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABg4v8AzqxN+8+VzrcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -102.35  -87.35  -87.35  -87.35   20.65   30.65\n",
            "    69.65  -79.35   60.65  149.65  141.65   21.65 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -75.35\n",
            "   -69.35  -11.35   48.65   64.65  147.65  147.65  147.65  147.65  147.65\n",
            "   119.65   66.65  147.65  136.65   89.65  -41.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -56.35  132.65\n",
            "   147.65  147.65  147.65  147.65  147.65  147.65  147.65  147.65  145.65\n",
            "   -12.35  -23.35  -23.35  -49.35  -66.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35  113.65\n",
            "   147.65  147.65  147.65  147.65  147.65   92.65   76.65  141.65  135.65\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -25.35\n",
            "    50.65    1.65  147.65  147.65   99.65  -94.35 -105.35  -62.35   48.65\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "   -91.35 -104.35   48.65  147.65  -15.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35   33.65  147.65   84.65 -103.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35  -94.35   84.65  147.65  -35.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35  -70.35  135.65  119.65   54.65    2.65 -104.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35  -24.35  134.65  147.65  147.65   13.65\n",
            "   -80.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35  -60.35   80.65  147.65  147.65\n",
            "    44.65  -78.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -89.35  -12.35  146.65\n",
            "   147.65   81.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  143.65\n",
            "   147.65  143.65  -41.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35  -59.35   24.65   77.65  147.65\n",
            "   147.65  101.65 -103.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35  -66.35   42.65  123.65  147.65  147.65  147.65\n",
            "   144.65   76.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35  -81.35    8.65  115.65  147.65  147.65  147.65  147.65   95.65\n",
            "   -27.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -82.35\n",
            "   -39.35  107.65  147.65  147.65  147.65  147.65   92.65  -24.35 -103.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35   65.65  113.65\n",
            "   147.65  147.65  147.65  147.65   89.65  -25.35  -96.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35  -50.35   66.65  120.65  147.65  147.65\n",
            "   147.65  147.65  138.65   27.65  -94.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35   30.65  147.65  147.65  147.65  106.65\n",
            "    29.65   26.65  -89.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This automatic scaling in image-related libraries can produce unexpected results, so check when using new methods."
      ],
      "metadata": {
        "id": "kWigU4Uv85SX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretreatment\n",
        "Images are represented as uint8 type from 0 to 255, but for machine learning, they are handled as float type from 0 to 1. The following code can be used for conversion.\n",
        "\n",
        "《sample code》"
      ],
      "metadata": {
        "id": "AVkbbgCS9L4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype(float)\n",
        "X_test = X_test.astype(float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.max()) # 1.0\n",
        "print(X_train.min()) # 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZAasVSy9Nzp",
        "outputId": "890f6599-2d2a-4777-8fcb-a3cf48d00049"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, although the correct answer labels are integers from 0 to 9, they are converted to one-hot expressionswhen performing multi-class classification in a neural network. The code using OneHotEncoderof scikit-learn is as follows. Because the value by this one-hot expression indicates the probability of being the label, it is treated as a float type.\n",
        "\n",
        "《sample code》"
      ],
      "metadata": {
        "id": "UBmY-E0Z9juP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "print(y_train.shape) # (60000,)\n",
        "print(y_train_one_hot.shape) # (60000, 10)\n",
        "print(y_train_one_hot.dtype) # float64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7keSdgu9yTf",
        "outputId": "5a3b2463-d077-457b-d80e-53d7873e3a2f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000,)\n",
            "(60000, 10)\n",
            "float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "link: sklearn.preprocessing.OneHotEncoder — scikit-learn 0.21.3 documentation\n",
        "\n",
        "Furthermore, 20% of the 60,000 sheets of training data should be divided as verification data. There will be 48,000 sheets of training data and 12,000 sheets of verification data.\n",
        "\n",
        "《sample code》"
      ],
      "metadata": {
        "id": "6_z12kt299Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "print(X_train.shape) # (48000, 784)\n",
        "print(X_val.shape) # (12000, 784)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kcofJ9s-JR6",
        "outputId": "1a2e840f-0665-4f01-e407-ff27d9cf1492"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48000, 784)\n",
            "(12000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimation and Visualization"
      ],
      "metadata": {
        "id": "n4qhTFUk_m6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "\n",
        "# Assuming X_train and y_train are your dataset variables\n",
        "\n",
        "# Preprocessing\n",
        "X_train = X_train.astype(float)\n",
        "X_test = X_test.astype(float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Visualize an image\n",
        "index = 0\n",
        "image = X_train[index].reshape(28, 28)\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title('label : {}'.format(np.argmax(y_train[index])))\n",
        "plt.show()\n",
        "\n",
        "# Build a simple neural network model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss=CategoricalCrossentropy(),\n",
        "              metrics=[Accuracy()])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Loss: {val_loss}')\n",
        "print(f'Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "# Plot the training and validation loss/accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "H9cbQZfYA9WZ",
        "outputId": "e918db98-f5b7-430e-bb9c-2bc9cc498b0d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with dim 3. None expected <= 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6a3584561437>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# One-hot encode the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0my_train_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0my_test_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_infrequent_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         fit_results = self._fit(\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         X_list, n_samples, n_features = self._check_X(\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m             )\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    916\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "\n",
        "# Assuming X_train and y_train are your dataset variables\n",
        "\n",
        "# Preprocessing\n",
        "X_train = X_train.astype(float)\n",
        "X_test = X_test.astype(float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_one_hot = enc.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Visualize an image\n",
        "index = 0\n",
        "image = X_train[index].reshape(28, 28)\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title('label : {}'.format(np.argmax(y_train[index])))\n",
        "plt.show()\n",
        "\n",
        "# Build a simple neural network model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss=CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Loss: {val_loss}')\n",
        "print(f'Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "# Plot the training and validation loss/accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "4sqP0sh3BYVh",
        "outputId": "88cc8614-1549-4450-c969-28ccde24a9bf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [38400, 384000]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-af657b4f3532>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Split the data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Visualize an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [38400, 384000]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural network scratch\n",
        "We will create a neural network class from scratch, using only minimal libraries such as NumPy to implement the algorithm.\n",
        "\n",
        "This time we will create a 3-layer neural network that performs multi-class classification. We will learn the basics of neural networks with the number of layers fixed. We will make a design that can freely change the layers in the next Sprint.\n",
        "\n",
        "A template is provided below. Please add your code to this ScratchSimpleNeuralNetrowkClassifier class.\n",
        "\n",
        "《Model》"
      ],
      "metadata": {
        "id": "omhrFzqyBkEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchSimpleNeuralNetrowkClassifier():\n",
        "    \"\"\"\n",
        "    Simple three-layer neural network classifier\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose = True):\n",
        "        self.verbose = verbose\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn a neural network classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : The following form of ndarray, shape (n_samples,)\n",
        "            Correct answer value of training data\n",
        "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            Features of verification data\n",
        "        y_val : The following form of ndarray, shape (n_samples,)\n",
        "            Correct value of verification data\n",
        "        \"\"\"\n",
        "\n",
        "        if self.verbose:\n",
        "            #verboseをTrueにした際は学習過程などをoutputする\n",
        "            print()\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using a neural network classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            The following form of ndarray, shape (n_samples, 1)\n",
        "            Estimated result\n",
        "        \"\"\"\n",
        "\n",
        "        pass\n",
        "        return"
      ],
      "metadata": {
        "id": "Zhzv0GiWBl30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mini-batch processing\n",
        "Previous machine learning scratches calculated all the samples at once. However, in neural networks, the the stochastic gradient descentmethod, in which data is divided and input, is common. The group when divided is called a mini-batch, and the number of samples is called a Batch size.\n",
        "\n",
        "In this case, the batch size is set to 20. Since the training data used this time is 48,000 sheets, 48,000 divided by 20 means that 2400 updates will be repeated.In neural networks, this is called 2400 times iterationOnce all the training data is viewed, one epochis over. This epoch is repeated multiple times to complete training.\n",
        "\n",
        "We have a simple iterator to accomplish this. Call it with a for statement to get a mini-batch.\n",
        "\n",
        "《code》"
      ],
      "metadata": {
        "id": "hKoyYeMRB0zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "Iterator to get a mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "ekfkIoruB2sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini batches can be retrieved by instantiating this class and using a for statement."
      ],
      "metadata": {
        "id": "RZ3ufzO4CAgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=20)\n",
        "\n",
        "print(len(get_mini_batch)) # 2400\n",
        "print(get_mini_batch[5]) # You can get the 5th mini batch\n",
        "for mini_X_train, mini_y_train in get_mini_batch:\n",
        "    # You can use a mini batch in this for statement\n",
        "    pass"
      ],
      "metadata": {
        "id": "w6e4xGc5CD8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Creating code to determine the initial values of the weights.\n",
        "Create a code to determine the initial values of the weights for each layer of the neural network.\n",
        "\n",
        "Various methods have been proposed for initializing the weights, but in this case we will use a simple initialization with a Gaussian distribution. The same is true for bias.\n",
        "\n",
        "Please refer to the following code. The standard deviation value sigma is a hyperparameter. The method of initializing the developmental weights will be covered in the next Sprint."
      ],
      "metadata": {
        "id": "dR3i2xjfCTrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 784\n",
        "n_nodes1 = 400\n",
        "sigma = 0.01 # Standard deviation of Gaussian distribution\n",
        "W1 = sigma * np.random.randn(n_features, n_nodes1)\n",
        "# W1: (784, 400)"
      ],
      "metadata": {
        "id": "dq5E7vX2CVUB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Implementation of Forward Propagation\n",
        "Create a  forward propagation of a three-layer neural network. In the following description, the number of nodes is 400 for the first layer and 200 for the second layer, but may be changed.\n",
        "\n",
        "The formulas for each layer are shown below. This time, we have also included a description of what ndarray shape each symbol represents in terms of implementation."
      ],
      "metadata": {
        "id": "FUq7JnMqCdSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size = 20 # batch size\n",
        " n_features = 784 # number of features\n",
        " n_nodes1 = 400 # number of first layer nodes\n",
        " n_nodes2 = 200 # number of second layer nodes\n",
        " n_output = 10 # number of output classes (number of nodes in the 3rd layer)"
      ],
      "metadata": {
        "id": "YlqiBNDVCjD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3 Implementation of Cross-Entropy Error\n",
        "objective function (loss function).\n",
        "\n",
        "The cross-entropy error $L$, which is the objective function of the multiclass classification, is the following formula"
      ],
      "metadata": {
        "id": "8r5gEJKUCrI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4: Back-propagation implementation\n",
        "Create a three-layer neural network backpropagation. This is the part where the stochastic gradient descent method is performed.\n",
        "\n",
        "The formula is shown below.\n",
        "\n",
        "First, here are the updated weights and bias formulas for the i-th layer. For $W_i$ and $B_i$, the updated $W_i^{\\prime}$ and $B_i^{\\prime}$ are obtained by the following formula"
      ],
      "metadata": {
        "id": "DJHYJIqMCwAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6 Learning and Estimation\n",
        "Learn and estimate MNIST data and calculate Accuracy.\n",
        "\n",
        "Problem 7 Plotting the learning curve\n",
        "Plot the learning curve.\n",
        "\n",
        "Since neural networks are prone to over-training, it is important to verify the learning curve. It should be possible to record the loss per epoch (cross-entropy error) for training and validation data.\n",
        "\n",
        "Problem 8 (Advance Assignment) Confirmation of Misclassification\n",
        "Identify which images were misclassified. Prepare an estimate and run the following code."
      ],
      "metadata": {
        "id": "-2pJuzKWC_x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Display word classification results side by side. The display above the image is \"estimated result / correct answer\".\n",
        "\n",
        "Parameters:\n",
        "----------\n",
        "y_pred : Estimated ndarray (n_samples,)\n",
        "y_val : Correct label of verification data (n_samples,)\n",
        "X_val : Verification data features (n_samples, n_features)\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num = 36 # How many to display\n",
        "\n",
        "true_false = y_pred==y_val\n",
        "false_list = np.where(true_false==False)[0].astype(np.int)\n",
        "\n",
        "if false_list.shape[0] < num:\n",
        "    num = false_list.shape[0]\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
        "for i in range(num):\n",
        "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
        "    ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
        "    ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')\n"
      ],
      "metadata": {
        "id": "km5MkHNxCiYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}