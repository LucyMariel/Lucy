{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnQu/sdZlmLwQCowEBTQ4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyMariel/Lucy/blob/master/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network is a mathematical model that imitates the information transmission of the brain (nervous system).\n",
        "In nerve cells (neurons), when the input stimulus exceeds the threshold value, it fires (activates), generates an action potential, and transmits information to other cells.\n",
        "This firing occurs in a chain to convey information and realize calculations. The change in the strength of neuron connections is called learning.\n",
        "\n",
        "Neural networks, on the other hand, map an input vector to another vector space by means of weights (joint weights) and activation functions.\n",
        "This is done in a chain of steps to extract information. In this process The emphasis on weight is called learning."
      ],
      "metadata": {
        "id": "qjEwyzYgTdkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MNIST dataset**\n",
        "\n",
        "Here, in explaining the neural network, we will explain using a data set consisting of image data of handwritten characters called MNIST.\n",
        "\n",
        "For example, in addition to a normal neural network that calculates prediction results from one line of data (three-layer shallow neural network (Network training) as shown in the figure at the bottom of the image), prediction results from two-dimensional data such as images. For example, a convolutional neural network (CNN) that calculates.\n",
        "\n",
        "Originally, the image given as input is two-dimensional data, but by converting each element into one line and interpreting it as one-dimensional input data, we will create a normal neural network.\n",
        "\n",
        "In this text, you will learn shallow neural networks (NN), deep neural networks (DNN), and convolutional neural networks (CNN) in that order.\n",
        "In addition, we will explain all neural networks using the MNIST dataset as an example.\n",
        "\n",
        "Originally, CNN-based methods are often used for image data, but for ease of understanding, this text uses the MNIST dataset to unify NN / DNN / CNN. I will implement it."
      ],
      "metadata": {
        "id": "FsJq4F0wTnov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the dataset**\n",
        "\n",
        "\n",
        "Handwritten character data set (MNIST data set) is input to a neural network to perform 0~9 (number) multi-class classification.\n",
        "\n",
        "Idea: (y_train: for correct answer labels)\n",
        "To divide into two classes: 1.\n",
        "The correct answer label corresponding to one sample is used for training as a scalar (int type) such as 0 or 1.\n",
        "In the case of dividing into multiple classes: The correct answer label corresponding to one sample is used for training.\n",
        "Given an outcome label corresponding to one sample, transform it into a one-hot vector with the length of the class and use it for training."
      ],
      "metadata": {
        "id": "WdFEKqBJToN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PUBLICATION\n",
        "\n",
        "Left: Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner \"Gradient-Based Learnign Applied to Document Recognition\".10page, Fig.4 Size-Normlized examples from the MNIST database.Yann LeCun`s Publications. November 1998.http://yann. lecun.com/exdb/publis/pdf/lecun-98.pdf (reference 2022-05-20)"
      ],
      "metadata": {
        "id": "B65e0qlWU8nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "A neural network imitates the function of the human brain\n",
        "There are various types of neural networks (NN, DNN, CNN)"
      ],
      "metadata": {
        "id": "krhtE7gbVHf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Structure**\n",
        "\n",
        "Get an overview of neural networks\n",
        "Understand the scratch code template of neural networks.\n",
        "Neural networks, as the name implies, are algorithms that mimic the human nervous system.\n",
        "\n",
        "As the whole structure, as shown in the image below, a large number of neurons are gathered and composed, the leftmost neuron is input (MNIST data in this case), and it is finally passed through various neurons. The value output from the rightmost neuron is the predicted value."
      ],
      "metadata": {
        "id": "Bfm0pKLEVcA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each \"0\" in the above image is called a node, and nodes constitute a layer. The layers consist of one input layer and one output layer, and any number of intermediate layers can be added (the image shows one intermediate layer).\n",
        "\n",
        "In addition, each node is connected to all the nodes in the next layer. That's why we use the word \"Dense\" in other explanations and tutorials.\n",
        "\n",
        "In the calculation flow, the values of MNIST data (28 pixels x 28 pixels = 784) smoothed as shown in the above image are input to the input layer as input values, passed through the intermediate layer through various calculations, and the predicted value is output at output layer.\n",
        "As for the number of nodes in the output layer, since MNIST data is classified into 10 classes (because it is handwritten data from 0 to 9), there are 10 nodes, and values from 0 to 1 are calculated from each of the 10 nodes, and the maximum value. Adopt the node of as a class.\n",
        "\n",
        "For example, if each output of a node in the output layer is as follows, 0.87 is the maximum value, meaning that the predicted result is that the input image is the character \"5\". (In this case, the handwritten character in the above figure is 3, so we conclude that the prediction result is different.)"
      ],
      "metadata": {
        "id": "CxOY8BDnXGBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqw4q6eZML1g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "herefore, the node with the highest value is considered.\n",
        "This is an implementation story, but when using the largest node, you can use np.argmax (NumPy's method), which returns the index of the maximum value along the axis.\n",
        "LINK: NumPy API reference --numpy.argmax"
      ],
      "metadata": {
        "id": "d5U81SM-XPpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning and learning**\n",
        "\n",
        "Inference processing is called forward propagation.\n",
        "\n",
        "In forward propagation, as the word forward means, input data travels forward in the network (here from left to right).\n",
        "After that, each hidden layer (intermediate layer) receives the input data, processes it according to the activation function, and passes it to the next layer.\n",
        "\n",
        "At this time, the forward propagation sequentially calculates and saves the intermediate variables in the calculation graph defined by the neural network, and proceeds from the input layer to the output layer.\n",
        "\n",
        "Next, the learning process is called backpropagation (backpropagation method).\n",
        "\n",
        "Backpropagation is a method of calculating the gradients of neural network parameters.\n",
        "Specifically, it is a method of tracing the network from the output layer to the input layer in reverse according to the chain rule of calculus.\n",
        "This algorithm stores the intermediate variables (partial derivatives) needed to calculate the gradient with respect to the parameter.\n",
        "\n",
        "Backpropagation sequentially calculates and saves the gradients of intermediate variables and parameters in the neural network in reverse order.\n",
        "\n",
        "The detailed processing of each will be introduced in the following texts.\n",
        "Here, let's get an overview only."
      ],
      "metadata": {
        "id": "lB9LJ0E-Xi03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data to use**\n",
        "\n",
        "This time, we will use a data set called MNIST that stores image data of handwritten characters.\n",
        "However, since it is originally 2D data, we will create a normal neural network using the smoothed (converted from 2D to 1D) data.\n",
        "\n",
        "Originally, the image data uses a CNN algorithm, but in order to make it easier to understand, we will implement NN / DNN / CNN uniformly using the MNIST dataset.\n",
        "\n",
        "As we saw above, the MNIST dataset is an image with a size of 28 pixels x 28 pixels = 784 pixels, with 0-9 handwritten characters."
      ],
      "metadata": {
        "id": "T-YC3Pz5YJJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Scratch code overview**"
      ],
      "metadata": {
        "id": "kEpz96XAYcE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class ScratchSimpleNeuralNetrowkClassifier():\n",
        "    def __init__(self,batch_size = 20,n_features = 784,n_nodes1 = 400,n_nodes2 = 200,n_output = 10,sigma = 0.02,lr = 0.01,epoch = 10, verbose=True):\n",
        "        self.verbose = verbose\n",
        "        self.batch_size = batch_size\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.sigma = sigma\n",
        "        self.lr = lr\n",
        "        self.epoch = epoch\n",
        "        self.loss_train = []\n",
        "        self.loss_val = []\n",
        "   \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        pass\n",
        "            \n",
        "    def tanh_function(self, A):\n",
        "        pass\n",
        "    \n",
        "    def softmax(self, A):\n",
        "        pass\n",
        "\n",
        "    def cross_entropy_error(self, y, Z):\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        pass"
      ],
      "metadata": {
        "id": "15Y272oxYiK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Class definition\n",
        "Second line: Constructor definition. As arguments, batch_size (number of data to be trained at one time), n_features (number of feature quantities of explanatory variables), n_nodes1 (number of nodes in the first layer), n_nodes2 (number of nodes in the second layer), n_output (nodes in the output layer) Number) ・ sigma (mean value of normal distribution) ・ lr (learning rate) ・ epoch (number of learnings) ・ verbose (whether or not to output the learning process)\n",
        "3rd to 11th lines: Argument member variables\n",
        "Lines 12 to 13: Variable definition for loss storage\n",
        "Lines 15-16: Learning function. Since it is a template, function definition with pass\n",
        "Lines 18-19: Forward Propagation Function\n",
        "Lines 21-22: Backpropagation function\n",
        "Lines 24 to 25: Activation function (tanh)\n",
        "Lines 27-28: Activation function (softmax)\n",
        "Lines 30-31: Loss function Cross entropy error function\n",
        "Lines 33-34: Prediction function"
      ],
      "metadata": {
        "id": "pZ53ZBOWYmzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "The structure of a neural network includes an input layer, one or more intermediate layers, and an output layer."
      ],
      "metadata": {
        "id": "e0-ko0scYsFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Neural Network Computation**\n",
        "\n",
        "Understand the calculation outline of neural networks\n",
        "Understand that the calculated output of each node will be the input of the next layer"
      ],
      "metadata": {
        "id": "gIslSPecY1sP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation at each node\n",
        "Now that you understand the general structure of the neural network, what kind of processing is being performed on each of the large number of nodes?\n",
        "\n",
        "As shown in the image above, the input layer contains 784 MNIST data as they are.\n",
        "\n",
        "Each node in the middle layer receives the total value (A) of the value obtained by multiplying 784 pieces of data as the input value of each node. Then, the value (f (A)) obtained by passing the total value (A) through the function (f) is the output value of each node. The same is true if there are multiple intermediate layers, and the output from the previous intermediate layer is received by the next intermediate layer as input.\n",
        "\n",
        "The output layer is treated in the same way as the middle layer."
      ],
      "metadata": {
        "id": "ZueSdPDBZDV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bias**\n",
        "\n",
        "Each middle layer needs to have a node that acts as a constant term in linear regression called bias.\n",
        "\n",
        " Summary\n",
        "The calculation is done at each node, and the output of one layer becomes the input of the next layer."
      ],
      "metadata": {
        "id": "bgGf-9oKZfbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural network activation function**\n",
        "\n",
        "Learn the theory and implementation of various types of activation functions\n",
        "\n",
        "What is the activation function?\n",
        "\n",
        "The activation function is the function f introduced earlier.\n",
        "\n",
        "The purpose of the activation function is to introduce non-linearity into the network. In fact, without the activation function, only linearly separable problems can be solved.\n",
        "\n",
        "There are various types of activation functions. The typical ones are as follows.\n",
        "\n",
        "Identity function\n",
        "Mathematical formulas and Python programs\n",
        "f(x)=x. depending THE FUNCTION"
      ],
      "metadata": {
        "id": "CJ_h4Nkras7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_function(self,X):\n",
        "    return X"
      ],
      "metadata": {
        "id": "y34ohACScyji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step function\n",
        "Mathematical formulas and Python programs\n",
        "f\n",
        "(\n",
        "x\n",
        ")\n",
        "=\n",
        "{\n",
        "0\n",
        "\n",
        "(\n",
        "x\n",
        "<\n",
        "0\n",
        ")\n",
        "1\n",
        "\n",
        "(\n",
        "x\n",
        ">=\n",
        "0\n",
        ")\n"
      ],
      "metadata": {
        "id": "_KXo1sb0dTDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step_function(self,X):\n",
        "    result = np.array(X >= 0, dtype=np.int)\n",
        "    return result"
      ],
      "metadata": {
        "id": "9teUODotdUwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "relu function\n",
        "Mathematical formulas and Python programs\n",
        "r\n",
        "e\n",
        "l\n",
        "u\n",
        "(\n",
        "x\n",
        ")\n",
        "=\n",
        "m\n",
        "a\n",
        "x\n",
        "(\n",
        "0\n",
        ",\n",
        "x\n",
        ")"
      ],
      "metadata": {
        "id": "s2p1PhR3c0aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(self,X):\n",
        "    result = np.max([np.zeros(X.shape), X], axis=0)\n",
        "    return result"
      ],
      "metadata": {
        "id": "kO4m6CZCdDrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "softmax function (softmax)\n",
        "Mathematical formulas and Python programs\n",
        "f\n",
        "(\n",
        "x\n",
        ")\n",
        "=\n",
        "e\n",
        "x\n",
        "p\n",
        "(\n",
        "x\n",
        "i\n",
        ")\n",
        "∑\n",
        "n\n",
        "i\n",
        "=\n",
        "1\n",
        "e\n",
        "x\n",
        "p\n",
        "(\n",
        "x\n",
        "i\n",
        ")\n"
      ],
      "metadata": {
        "id": "ebFw2jZydkGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(self,X):\n",
        "    result = np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n",
        "    return result"
      ],
      "metadata": {
        "id": "Xo7EiwafdlUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sigmoid function (sigmoid)\n",
        "Mathematical formulas and Python programs\n",
        "s\n",
        "i\n",
        "g\n",
        "m\n",
        "o\n",
        "i\n",
        "d\n",
        "(\n",
        "x\n",
        ")\n",
        "=\n",
        "1/\n",
        "1\n",
        "+\n",
        "e\n",
        "−\n",
        "x\n"
      ],
      "metadata": {
        "id": "A5IX2b4dd1kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(self,X):\n",
        "    result = 1 / (1 + np.exp(-X))\n",
        "    return result"
      ],
      "metadata": {
        "id": "-0YkzOgsd2p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tanh function (hyperbolic tangent)\n",
        "Mathematical formulas and Python programs\n",
        "t\n",
        "a\n",
        "n\n",
        "h\n",
        "(\n",
        "x\n",
        ")\n",
        "=\n",
        "e\n",
        "x\n",
        "−\n",
        "e\n",
        "−\n",
        "x\n",
        "e\n",
        "x\n",
        "+\n",
        "e\n",
        "−\n",
        "x\n"
      ],
      "metadata": {
        "id": "vxRVIYp_eEWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(self,X):\n",
        "    result = (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
        "    # or\n",
        "    #  result = np.tanh(X)\n",
        "    return result"
      ],
      "metadata": {
        "id": "T9Kfn3YSeFX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "The activation function helps to introduce non-linearity into the model\n",
        "There are many activation functions"
      ],
      "metadata": {
        "id": "mSm4xc2peNyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural network calculation graph**"
      ],
      "metadata": {
        "id": "M67LAGNaeaaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand the computational graphs of different processes.\n",
        "Computational graph: a visualisation of a computational process represented by multiple nodes and edges.\n",
        "\n",
        "Chain rule: the derivative of a composite function is represented by the derivative of each of the functions that make up that composite function.\n"
      ],
      "metadata": {
        "id": "HcBDWLXDecGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculation graphs\n",
        "First, let's represent a simple formula using a calculation graph.\n",
        "\n",
        "Given the following two equations.\n",
        "\n",
        "z=t2\n",
        "\n",
        "t=x+y\n",
        "This can be expressed using a calculation graph as follows.\n",
        "Following the stream from the left represents the next stream.\n",
        "\n",
        "x,y comes in as input.\n",
        "Addition of x,y is performed at the addition node.\n",
        "The result of the addition is output as t\n",
        "t is squared at the multiplication node\n",
        "The squared value is output as z\n",
        "Now let us assume that we want to find the derivative of z with respect to x. The formula for the derivative is given below."
      ],
      "metadata": {
        "id": "kbDGiK0AeuBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "Provides an overview of computational graphs and explains how to calculate partial derivatives.\n",
        "Forward and back propagation of a neural network, computed using the concept of computational graphs."
      ],
      "metadata": {
        "id": "l9cmNhxEgOOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural network weights**\n",
        "Understand and implement initialization of neural network weights\n",
        "Creating code to determine initial values for \"Problem 1\" weights\n",
        "Add the following program to the end of the constructor"
      ],
      "metadata": {
        "id": "Rb9xeAI0guAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.W1 = self.sigma * np.random.randn(self.n_features, self.n_nodes1)\n",
        "self.W2 = self.sigma * np.random.randn(self.n_nodes1, self.n_nodes2)\n",
        "self.W3 = self.sigma * np.random.randn(self.n_nodes2, self.n_output)\n",
        "self.B1 = self.sigma * np.random.randn(1, self.n_nodes1)\n",
        "self.B2 = self.sigma * np.random.randn(1, self.n_nodes2)\n",
        "self.B3 = self.sigma * np.random.randn(1, self.n_output)"
      ],
      "metadata": {
        "id": "jgngllz2hEjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Initialization of weights. The shape of the weights is (n_features (number of explanatory variables (28 * 28)),n_nodes1 (number of nodes in the first layer)), so the input of the neural network is (batch_size (number of data to be trained at once),n_features (number of explanatory variables)) Since the data of shape comes in, the shape (shape of output) after calculation using this weight becomes (batch_size,n_nodes1)\n",
        " (The reason why the shape of weight becomes (self.n_features, self.n_nodes1). The data of the shape of (batch_size (number of data to be trained at a time),n_features (number of explanatory variables (28 * 28))) comes in as the input of the neural network. For example, the shape (shape of the output) after the calculation using the weight W1 for this input should be (batch_size,n_nodes1), so the shape of the weights is (n_features (number of explanatory variables),n_nodes1 (number of nodes in the first layer)).\n",
        "\n",
        "(batch_size,n_features)×(n_features,n_nodes1)=(batch_size,n_nodes1)\n",
        "\n",
        "Line 2: Initialize the weights. shape is (n_nodes1,n_nodes2), so the output of the previous layer (batch_size,n_nodes1) is received as input, so the calculated shape (output shape) is (batch_size,n_nodes2)\n",
        "\n",
        "(batch_size,n_nodes1)×(n_nodes1,n_nodes2)=(batch_size,n_nodes2)\n",
        "\n",
        "Line 3: Initialize the weights. shape is (n_nodes2,n_output), so the output of the previous layer (batch_size,n_nodes2) is received as input, so the calculated shape (output shape) is (batch_size,n_output)\n",
        "\n",
        "(n_nodes1,n_nodes2)×(n_nodes2,n_output)=(batch_size,n_output)\n",
        "\n",
        "Line 4: Initialize the bias, where shape is (1,n_nodes1). The role of the bias is that of a constant term in linear regression. It is added to the output of the first layer during the calculation.\n",
        "\n",
        "Line 5: Initialize bias. shape is (1,n_nodes2). Add to the output of the second layer when calculating\n",
        "\n",
        "Line 6: Initialize bias. shape is (1,n_output). Add to the output of the third layer in the calculation."
      ],
      "metadata": {
        "id": "OpBITQhphIJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "Weights and biases can be initialized in a variety of ways\n",
        "Here we use a Gaussian distribution"
      ],
      "metadata": {
        "id": "ZVq_xAwJhTof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Neural Networks**\n",
        "\n",
        "purpose\n",
        "Understanding Forward Propagation\n",
        "Know the cross-entropy error\n",
        "Understanding Backpropagation\n",
        "\n",
        "What is forward propagation?\n",
        "\n",
        "In forward propagation, as the word \"forward\" implies, the input data travels through the network in the forward direction (here from left to right).\n",
        "Each hidden layer (intermediate layer) then receives the input data, processes it according to its activation function, and passes it to the next layer.\n",
        "\n",
        "Thus, forward propagation sequentially computes and stores intermediate variables in the computational graph defined by the neural network. It proceeds from the input layer to the output layer.\n"
      ],
      "metadata": {
        "id": "B6rfe7DkhfaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is cross-entropy error (cross-entropy loss)?\n",
        "\n",
        "When working with machine learning and deep learning problems, loss functions and cost functions are used to optimize the model being trained.\n",
        "The goal is almost always to minimize the loss function. The smaller the loss, the better the model. Cross-entropy error is an important cost function. It is used to optimize classification models.\n",
        "Although many XX functions have appeared, a simple picture is the relationship: objective function ⊃ cost function, error function, loss function.\n",
        "Although strictly different, these functions are motivated by the desire to minimize the error between the estimates and the labels.)\n",
        "\n",
        "The purpose of cross-entropy is to obtain the output probability (P) and measure its distance from the value of the correct answer label.\n",
        "\n",
        "As an example, suppose the desired output of a class and the output of a model are as follows\n",
        "\n",
        "The goal is to bring the output of the model as close as possible to the desired output (the value of the correct label). During model training, the weights of the model are iteratively adjusted with the goal of minimizing cross-entropy error.\n",
        "The process of adjusting the weights is what model learning is all about, and when a model continues to learn and losses are minimized, we say that the model is learning.\n",
        "\n",
        "What is back-propagation?\n",
        "\n",
        "Back propagation, or error back propagation, is a method of calculating the gradient of the parameters of a neural network. In other words, it is a method that follows the chain rule of calculus, tracing the network backwards from the output layer to the input layer.\n",
        "This algorithm preserves the intermediate variables (partial derivatives) needed to compute the gradients for the parameters.\n",
        "\n",
        "Backpropagation sequentially calculates and saves the gradients of intermediate variables and parameters in the neural network in reverse order.\n",
        "\n",
        " Summary\n",
        "Forward propagation, cross-entropy error, and back propagation are key points in the learning process of neural networks"
      ],
      "metadata": {
        "id": "cN8XMjeJjF-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward propagation of neural networks (forward propagation process)**\n",
        "\n",
        "Understand the flow of the forward propagation process of a neural network\n",
        "Implementing a neural network forward propagation process using Python\n",
        "Calculation at each node\n",
        "Now that you understand the general structure of the neural network, what kind of processing is being performed on each of the large number of nodes?\n",
        "\n",
        "Like linear regression and the steepest descent methods that have been implemented in other texts, neural nets also have mechanisms to increase estimation accuracy as they are trained.\n",
        "\n",
        "It is called the error back propagation method. While inference propagated values from left to right, the error back propagation method, as the name implies, propagates from right to left, updating the weights w in the process as in the steepest descent method.\n",
        "Forward propagation is called forward propagation and back propagation is called back propagation.\n",
        "\n"
      ],
      "metadata": {
        "id": "bPZIyXBvkXNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation\n",
        "The forward propagation process we have seen above can be implemented using Python as follows."
      ],
      "metadata": {
        "id": "6oi-wtGioZQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, X):\n",
        "    self.A1 = X @ self.W1 + self.B1\n",
        "    self.Z1 = self.tanh_function(self.A1)\n",
        "    self.A2 = self.Z1 @ self.W2 + self.B2\n",
        "    self.Z2 = self.tanh_function(self.A2)\n",
        "    self.A3 = self.Z2 @ self.W3 + self.B3\n",
        "    self.Z3 = self.softmax(self.A3)"
      ],
      "metadata": {
        "id": "18JCY1AcoaQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. It takes an explanatory variable X as an argument.\n",
        "\n",
        "Line 2: Add bias B1 to the result of the matrix product of explanatory variable X and weight W1 to compute A1 in the first layer\n",
        "\n",
        "Line 3: The result A1 from row 2 is passed through the function (f) (here the activation function tanh) to obtain the output of the first layer\n",
        "\n",
        "Line 4: Result Z1 from row 3 is the input for the second layer, and bias B2 is added to the result of the matrix product of Z1 and weight W2 to compute A2 for the second layer\n",
        "\n",
        "Line 5: The result of row 4 is passed through the function (f) (here the activation function tanh) to obtain the output of the second layer\n",
        "\n",
        "Line 6: Result Z2 from row 3 is the input for the third layer, and bias B3 is added to the result of the matrix product of Z2 weights W3 to compute A3 for the third layer\n",
        "\n",
        "Line 7: The result of line 6 is passed through function (f) (softmax for the activation function at output).\n",
        "\n",
        " Summary\n",
        "Forward propagation is essential in the learning process of neural networks\n",
        "This process is described in the forward function above"
      ],
      "metadata": {
        "id": "US3Ut7u6ogQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of cross-entropy error in neural networks**\n",
        "\n",
        "When working with machine learning and deep learning problems, loss functions and cost functions are used to optimize the model being trained. The goal is almost always to minimize the loss function.\n",
        "The smaller the loss, the better the model. Cross-entropy error is an important cost function. It is used to optimize classification models.\n",
        "\n",
        "The purpose of cross-entropy is to obtain the output probability (P) and measure its distance from the value of the correct label.\n",
        "\n",
        "As an example, suppose the desired output of a class and the output of a model are as follows XXX The goal is to bring the output of the model as close as possible to the desired output (the value of the correct label).\n",
        "During model training, the weights of the model are iteratively adjusted with the goal of minimizing cross-entropy loss. The process of adjusting the weights is what model learning is all about, and as the model continues to learn and losses are minimized, we say that the model is learning.\n"
      ],
      "metadata": {
        "id": "Xk7o2UM5pI1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of \"Problem 3\" Cross-Entropy Error\n",
        "Let's implement it based on the cross-entropy error function described above"
      ],
      "metadata": {
        "id": "InZEnbN4pj2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_error(self, y, Z):\n",
        "    L = - np.sum(y * np.log(Z+1e-7)) / len(y)\n",
        "    return L"
      ],
      "metadata": {
        "id": "Gy_e_RgLpmhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Definition of the cross-entropy error function. The following are passed as arguments: y is the label, Z is the estimated value (the value output from the neural network)\n",
        "Line 2: Implement the formula for the cross-entropy error. Add 1e-7 to avoid errors.\n",
        "3rd line: Return the cross-entropy error."
      ],
      "metadata": {
        "id": "Y44J1mk-ppxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "Cross-entropy error is a loss function used in class classification.\n",
        "When calculating the cross-entropy error, 1e-7is added to the calculation of np.logto avoid errors"
      ],
      "metadata": {
        "id": "yZg4KBelpwK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural network backpropagation (error backpropagation)**\n",
        "Understand the process of error back propagation\n",
        "Implement the process of error back propagation using Python\n",
        "\n",
        "What is back propagation (error back propagation)?\n",
        "As with the steepest descent method, the error is determined and each weight is updated by multiplying the gradient calculated from the error by the learning rate.\n",
        "\n",
        "In the steepest descent method, each value has been updated so that the loss function is minimized. The error back propagation method similarly uses the cross-entropy error as the loss function when classifying classes.\n",
        "weight update\n",
        "As we introduced that the weights are updated to minimize the cross-entropy error function, the weights are updated according to the learning rate using the slope obtained by differentiation. This is called the gradient descent method, and there are two types of gradient descent methods\n",
        "\n",
        "The \"normal\" gradient descent method\n",
        "As you will see in the formulas that follow, the weights are updated for all data (batches).\n",
        "\n",
        "The \"stochastic\" gradient descent method\n",
        "Unlike the \"normal\" gradient descent method,weights are updated on a subset (mini-batch) of data.\n",
        "\n",
        "This stochastic gradient descent method is used to update the weights of the neural network.\n",
        "\n",
        "Expressed in mathematical form, the stochastic gradient descent method is as follows\n",
        "First, here is some basic knowledge about the differential and partial derivatives that are used in these calculations.\n",
        "\n",
        "Differentiation and Partial Differentiation\n",
        "Differentiation is the determination of the slope of an arbitrary function at an arbitrary point.\n",
        "\n",
        "Differentiation\n",
        "\n",
        "Consider the case of differentiating a function f(x). The derivative formula is as follows.\n",
        "\n",
        "d\n",
        "f\n",
        "d\n",
        "x\n",
        "=\n",
        "lim\n",
        "h\n",
        "→\n",
        "0\n",
        "\n",
        "f\n",
        "(\n",
        "x\n",
        "+\n",
        "h\n",
        ")\n",
        "−\n",
        "f\n",
        "(\n",
        "x\n",
        ")\n",
        "h\n",
        "Based on the above formula, the function for differentiation is implemented as follows."
      ],
      "metadata": {
        "id": "7NEmM2taxZwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_diff(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    return (f(x+h) - f(x)) / (h)"
      ],
      "metadata": {
        "id": "XfxS1Pv6x6KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's actually differentiate the following function using the above function.\n",
        "\n",
        "formula\n",
        "f\n",
        "(\n",
        "x\n",
        ")\n",
        "=\n",
        "0.01\n",
        "x\n",
        "2\n",
        "+\n",
        "0.1\n",
        "x\n",
        "differential expression\n",
        "\n",
        "d\n",
        "f/\n",
        "d\n",
        "x\n",
        "=\n",
        "0.02\n",
        "x\n",
        "+\n",
        "0.1\n",
        "The following is a Python description."
      ],
      "metadata": {
        "id": "ouBvsSIcyCeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function_1(x):\n",
        "    return 0.01*x**2 + 0.1*x"
      ],
      "metadata": {
        "id": "Kg7LRnQzyDy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's find the derivative of this function at x=5,10.\n",
        "Let's compare the results of the actual manual differentiation with those carried out with the function for differentiation."
      ],
      "metadata": {
        "id": "MIG8MwqLySkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_diff(function_1,5) # 0.20000099999917254\n",
        "numerical_diff(function_1,10) # 0.3000009999976072"
      ],
      "metadata": {
        "id": "vwidZJS0yVhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " partial differentiation\n",
        "\n",
        "Next, let us review the definition of partial differentiation.\n",
        "\n",
        "Partial differentiation is the differentiation of multiple variables by any variable alone. Suppose we actually have the following variables\n",
        "\n",
        "formula\n",
        "\n",
        "f\n",
        "(\n",
        "x\n",
        "0\n",
        ",\n",
        "x\n",
        "1\n",
        ")\n",
        "=\n",
        "x\n",
        "2\n",
        "0\n",
        "+\n",
        "x\n",
        "2\n",
        "1"
      ],
      "metadata": {
        "id": "L-ddNATVyadg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us find the derivative of this function when x0=3.0 x1=4.\n",
        "Due to the specification of the function for differentiation, it is written as follows"
      ],
      "metadata": {
        "id": "TODHkq14yhS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function_tmp1(x0):\n",
        "    return x0*x0 + 4.0 ** 2.\n",
        "def function_tmp2(x1):\n",
        "    return 3.0**2 + x1*x1"
      ],
      "metadata": {
        "id": "4j2Wj8_zyh-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the results of the actual manual differentiation with those carried out with the function for differentiation."
      ],
      "metadata": {
        "id": "bV0MvuSVyp6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_diff(function_tmp1,3.0)# 6.000099999994291\n",
        "numerical_diff(function_tmp1,4.0)# 8.00009999998963"
      ],
      "metadata": {
        "id": "ibRyJIz2yshn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numerical_diffdefined so far required the partial derivatives of $x_0$ and $x_1$ to be computed separately during partial differentiation. Next, let us define a function of partial differentiation that can compute the partial differentiation simultaneously."
      ],
      "metadata": {
        "id": "ZAlHWhV6yzH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formula for Error Back Propagation Method\n",
        "As the name implies, the error back propagation method computes from the output layer in reverse order of the forward propagation. Therefore, the calculation starts from the third layer."
      ],
      "metadata": {
        "id": "A96ypxyKy2u0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\ frac {\\ partial L} {\\ partial A_3} $: Gradient of loss $ L $ for $ A_3 $ (batch_size, n_output)\n",
        "\n",
        "$ \\ frac {\\ partial L} {\\ partial A_ {3_j}} $: Gradient of loss $ L $ for $ A_3 $ in the jth sample (n_nodes2,)\n",
        "\n",
        "$ \\ frac {\\ partial L} {\\ partial B_3} $: Gradient of loss $ L $ for $ B_3 $ (n_output,)\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_3}$ : gradient of loss $L$ with respect to $W_3$ (n_nodes2, n_output)\n",
        "\n",
        "$ \\ frac {\\ partial L} {\\ partial Z_2} $: Gradient of loss $ L $ for $ Z_2 $ (batch_size, n_nodes2)\n",
        "\n",
        "$Z_{3}$ : output of softmax function (batch_size, n_nodes2)\n",
        "\n",
        "$ Y $: Correct label (batch_size, n_output)\n",
        "\n",
        "$Z_{2}$ : output of the second layer activation function (batch_size, n_nodes2)\n",
        "\n",
        "$W_3$ : 3rd layer weights (n_nodes2, n_output)\n",
        "\n",
        "$ n_ {b} $: batch size, batch_size\n",
        "\n",
        "\"Second layer\"\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_2}$ : gradient of loss $L$ with respect to $W_2$ (n_nodes1, n_nodes2)\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_2}$ : gradient of loss $L$ with respect to $W_2$ (n_nodes1, n_nodes2)\n",
        "\n",
        "$ \\ frac {\\ partial L} {\\ partial B_2} $: Gradient of loss $ L $ for $ B_2 $ (n_nodes2,)\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_2}$ : gradient of loss $L$ with respect to $W_2$ (n_nodes1, n_nodes2)\n",
        "\n",
        "$ \\ frac {\\ partial L} {\\ partial Z_2} $: Gradient of loss $ L $ for $ Z_2 $ (batch_size, n_nodes2)\n",
        "\n",
        "$ A_2 $: Second layer output (batch_size, n_nodes2)\n",
        "\n",
        "$Z_{1}$ : output of the first layer activation function (batch_size, n_nodes1)\n",
        "\n",
        "$W_2$ : second layer weights (n_nodes1, n_nodes2)\n",
        "\n",
        "⊙\n",
        "The symbol of means \"Hadamard product\".\n",
        "The Hadamard product is the calculation of the product of each component of matrices of the same size. The output size will be the same.\n",
        "\n",
        "\"First layer\""
      ],
      "metadata": {
        "id": "CfV1n0Iny7UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formula for Error Back Propagation Method\n",
        "ディープロに記載の数式に則り、逆伝播の処理をプログラムとして実装すると下記のようになります。"
      ],
      "metadata": {
        "id": "Qm3Nd_rnzMv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(self, X, y):\n",
        "    dA3 = (self.Z3 - y)/self.batch_size\n",
        "    dW3 = self.Z2.T @ dA3\n",
        "    dB3 = np.sum(dA3, axis=0)\n",
        "    dZ2 = dA3 @ self.W3.T\n",
        "    dA2 = dZ2 * (1 - self.tanh_function(self.A2)**2)\n",
        "    dW2 = self.Z1.T @ dA2\n",
        "    dB2 = np.sum(dA2, axis=0)\n",
        "    dZ1 = dA2 @ self.W2.T\n",
        "    dA1 = dZ1 * (1 - self.tanh_function(self.A1)**2)\n",
        "    dW1 = X.T @ dA1\n",
        "    dB1 = np.sum(dA1, axis=0)\n",
        "    self.W3 -= self.lr * dW3\n",
        "    self.B3 -= self.lr * dB3\n",
        "    self.W2 -= self.lr * dW2\n",
        "    self.B2 -= self.lr * dB2\n",
        "    self.W1 -= self.lr * dW1\n",
        "    self.B1 -= self.lr * dB1"
      ],
      "metadata": {
        "id": "Av_msoWoy8bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Definition of the function. It takes as arguments the explanatory variable X and the objective variable y.\n",
        "Line 2: Third layer, the inverse of the activation function softmax, the part of $\\frac{\\partial L}{\\partial A_3} = \\frac{1}{n_b}(Z_{3} - Y)$\n",
        "Line 3: Third layer, the inverse of the weights, $\\frac {\\partial L}{\\partial W_3} = Z_{2}^{T}\\cdot \\frac{\\partial L}{\\partial A_3}$ part\n",
        "Line 4: third layer, the inverse of the bias, $\\frac{\\partial L}{\\partial B_3} = \\sum_{j}^{n_b}\\frac{\\partial L}{\\partial A_{3_j}}$ part\n",
        "Line 5: 2nd layer, calculation of inverse of output, $\\frac{\\partial L}{\\partial Z_2} = \\frac{\\partial L}{\\partial A_3} \\cdot W_3^T$ part\n",
        "Line 6: second layer, calculation of the inverse of the activation function tanh, $\\frac{\\partial L}{\\partial A_2} = \\frac{\\partial L}{\\partial Z_2} \\odot {1-tanh^2(A_{2})}$ part\n",
        "Line 7: second layer line 8: second layer, the inverse of the weights, $\\frac{\\partial L}{\\partial W_2} = Z_{1}^T \\cdot \\frac{\\partial L}{\\partial A_2}$ part\n",
        "Line 8: second layer, the inverse of the biases, $\\frac{\\partial L}{\\partial B_2} = \\sum_{j}^{n_b}\\frac{\\partial L}{\\partial A_{2_j}}$ part\n",
        "Line 9: 1st layer, calculation of the inverse of output $\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_2} \\cdot W_2^T$.\n",
        "Line 10: 1st layer, inverse calculation of activation function tanh, $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_1} \\odot {1-tanh^2(A_{1})}$.\n",
        "Line 11: First layer, the inverse of the weights, $\\frac{\\partial L}{\\partial W_1} = X^T \\cdot \\frac{\\partial L}{\\partial A_1}$\n",
        "Line 12: First layer, the inverse of the bias, $\\frac{\\partial L}{\\partial B_1} = \\sum_{j}^{n_b}\\frac{\\partial L}{\\partial A_{1_j}}$ part\n",
        "Lines 13 to 18: update weights and bias by applying learning rate to each calculated slope"
      ],
      "metadata": {
        "id": "ZNsyi-0UzXUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "Backpropagation is an integral part of the neural network training process, updating weights and biases\n",
        "This process is described in the backward function above"
      ],
      "metadata": {
        "id": "DJeYPdqrzkMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural network estimation**\n",
        "\n",
        "purpose\n",
        " Understanding the process flow of neural network estimation\n",
        " Programmatically implement the estimation process\n",
        "\n",
        " Estimation\n",
        "\n",
        " Let's add a predict function to the classScratchSimpleNeuralNetrowkClassifierthat we have created so far."
      ],
      "metadata": {
        "id": "SL_WoRSyz1QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "    self.forward(X)\n",
        "    return np.argmax(self.Z3, axis=1)"
      ],
      "metadata": {
        "id": "IuPIERqP0O9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Definition of a function.\n",
        "Line 2: The function of forward propagation is executed by passing the variable X.\n",
        "Line 3: The output layer has 10 nodes, and the index of the maximum value is determined as the classified class, so the index of the maximum value is obtained by np.argmax."
      ],
      "metadata": {
        "id": "UAOZ1et10UNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With regard to the processing of the third line, in the case of class classification, the process is as described above (adopting the index of the maximum value).\n",
        "However, in the case of continuous value prediction, there is one node in the output layer, and it is common to adopt a constant function for the activation function of the output layer and an MSE for the loss function."
      ],
      "metadata": {
        "id": "q1HzFhCW1AvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "Estimation, performed by the predict function."
      ],
      "metadata": {
        "id": "Vo1AF61L1IUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural network learning and estimation**\n",
        "Implement a function to perform learning using the neural network class you created.\n",
        "Perform learning functions\n",
        "Visualize recorded accuracy and loss\n",
        "\n"
      ],
      "metadata": {
        "id": "1OiLINoI1iiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6\" Learning and Estimation\n",
        "Let's implement a new fit functionin the neural network class we have created. After implementing thefit function, perform training and calculate Accuracy."
      ],
      "metadata": {
        "id": "Ba4P1asZ1uKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of learning functions\n",
        "First, the fit functionis implemented using Python as follows."
      ],
      "metadata": {
        "id": "k5i3uT711zC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self, X, y, X_val=None, y_val=None):\n",
        "    for _ in range(self.epoch):\n",
        "        get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "        for mini_X_train, mini_y_train in get_mini_batch:\n",
        "            self.forward(mini_X_train)\n",
        "            self.backward(mini_X_train, mini_y_train)\n",
        "        self.forward(X)\n",
        "        self.loss_train.append(self.cross_entropy_error(y, self.Z3))\n",
        "        if X_val is not None:\n",
        "            self.forward(X_val)\n",
        "            self.loss_val.append(self.cross_entropy_error(y_val, self.Z3))\n",
        "    if self.verbose:\n",
        "        if X_val is None:\n",
        "            print(self.loss_train)\n",
        "        else:\n",
        "            print(self.loss_train,self.loss_val)"
      ],
      "metadata": {
        "id": "CUYJj17_1vN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. It receives as arguments the explanatory variable X for the training data, the objective variable y for the training data, the explanatory variable X_val for the evaluation data, and the objective variable y_val for the evaluation data.\n",
        "Line 2: Looping through the training count.\n",
        "Line 3:X and yto theGetMiniBatchiterator, and create an iterator get_mini_batchfor mini-batch\n",
        "Line 4: A for statement is executed using the get_mini_batchterator created. The iterator generates explanatory variables and objective variables for the mini-batch size, which are received as mini_X_train, mini_y_train.\n",
        "Line 5: Forward propagation process\n",
        "Line 6: Back propagation process\n",
        "Line 7: Once all mini batches have been trained, the entire set of explanatory variables is passed through the forward propagation process.\n",
        "Line 8: The output layer output self.Z3calculated in line 7, along with the correct answer data y, is passed to the cross-entropy error function to obtain the loss. The loss is stored in loss_trainThis loss_trainwill be used to visualize the learning process in the future.\n",
        "Line 9: Determine whether or not X_val, the evaluation data, contains a value.\n",
        "Line 10: Forward propagation of the evaluation data\n",
        "Line 11: Pass the output layer output self.Z3calculated in line 10, together with the correct answer data y_val, to the cross-entropy error function to obtain the loss. The loss is stored in loss_val. Thisloss_valis used to visualize the learning process in the future.\n",
        "Line 12: Determine whether or not to output the learning process.\n",
        "Line 13: If X_valcontains no value, printonlyloss_train\n",
        "Line 14: If X_valcontains a value, printbothloss_trainandloss_val"
      ],
      "metadata": {
        "id": "vZf87owD194g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Study Implementation\n",
        "Conduct the study using the scratch classes you have created so far."
      ],
      "metadata": {
        "id": "Fn5jxaGU2I74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn = ScratchSimpleNeuralNetrowkClassifier(epoch=10)\n",
        "nn.fit(X_train,y_train_one_hot, X_val, y_val_one_hot)"
      ],
      "metadata": {
        "id": "oUfoolrZ2L_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Instantiation of scratch classnn\n",
        "Line 2: Executing fit functinoof instancenn​"
      ],
      "metadata": {
        "id": "mhkef-NT2Oqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation of Accuracy\n",
        "Calculate Accuracy using the learned model!"
      ],
      "metadata": {
        "id": "2zbE_rTQ2SrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "pred_train = nn.predict(X_train)\n",
        "accuracy = accuracy_score(y_train, pred_train)"
      ],
      "metadata": {
        "id": "HRZpn_Mg2VcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: imports the function for calculating accuracy\n",
        "Line 2: executes the predict function of the learned instance nnand receives the return value (predicted value) as pred_train​\n",
        "Line 3: passes the correct answer data (y_train) and predicted value (pred_train) toaccuracy_scoreand calculates accuracy."
      ],
      "metadata": {
        "id": "XHgxGW9Y2YyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the \"Problem 7\" learning curve\n",
        "\n",
        "The loss at each learning frequency is recorded inloss_train and loss_valin the fit function, and is visualized by referring to them via instances.\n",
        "\n",
        "plt.plot(range(nn.epoch), nn.loss_train)\n",
        "Line 1: The X and y to be drawn are put into the matplotlib functions for visualization. Here, the x-axis is the number of training runs, and the y-axis is the loss_train\n",
        "\n",
        "5. Summary\n",
        "Visualization of recorded accuracies and losses can help you know if the learning process went smoothly"
      ],
      "metadata": {
        "id": "t4YVqQz52dOQ"
      }
    }
  ]
}