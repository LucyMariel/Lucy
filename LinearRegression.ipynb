{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM911E2if+1uIeJwaroRChd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyMariel/Lucy/blob/master/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "864sNyKYRfQJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding $a,b$ that best fits the data is called solving linear regressionThere are various known methods, but this text describes a method using the steepest descent method. The steepest descent method is a basic method that can be used for a wide range of problems that are difficult to solve analytically, and it is also very relevant to neural networks, so it is important to learn it here.\n",
        "\n",
        "**Overview of scratch code**\n",
        "\n",
        "Based on the following template, we will now implement the linear regression algorithm. The scratch code below combines the various functions into one class. The processing of the function is described here as PASS, but it will approach completion as you progress through the text.\n",
        "\n"
      ],
      "metadata": {
        "id": "krVii0gpRyJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        self.num_iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.bias = bias\n",
        "        self.verbose = verbose\n",
        "        self.theta = np.array([])\n",
        "        self.loss = np.array([])\n",
        "        self.val_loss = np.array([])\n",
        "\n",
        "    # problem6（Learning and estimation ）\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Learning linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "    # problem1\n",
        "    def _linear_hypothesis(self):\n",
        "        \"\"\"\n",
        "        Hypothetical function\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    # problem2\n",
        "    def _gradient_descent(self):\n",
        "        \"\"\"\n",
        "        Calculation of updated parameter values using the steepest descent method.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    # problem3\n",
        "    def predict(self):\n",
        "        \"\"\"\n",
        "        Estimation by linear regression.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    # problem4\n",
        "    def _mse(self):\n",
        "        \"\"\"\n",
        "        Calculation of mean square error\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    # problem5\n",
        "    def _loss_func(self):\n",
        "        \"\"\"\n",
        "         loss function\n",
        "        \"\"\"\n",
        "        pass\n",
        ""
      ],
      "metadata": {
        "id": "KYJm4TtESRm7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Class definition\n",
        "\n",
        "Line 2: Constructor definition. The arguments are num_iter (number of training), lr (learning rate), bias (whether to include a constant term or not), verbose (whether to output the learning process or not).\n",
        "\n",
        "Line 3 ~: Definition of necessary member variables. Variables to be used across various functions (theta), variables to be passed as arguments when instantiating (num_iter, lr, bias, verbose), and variables to be referenced via instances (loss, val_loss) are defined as member variables.\n",
        "\n",
        "We will now find $a,b$ that best fits the HousePrice data. A linear regression with one explanatory variable, as in this equation, is specifically called a linear simple regression. A linear multiple regression with two or more explanatory variables is called a linear multiple regression, and the two are collectively called a linear regression"
      ],
      "metadata": {
        "id": "c2Yq7HgGScqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, a human hypothetically defines a function (equation) that the data will approximately follow. This is called a hypothetical function.\n",
        "\n",
        "Also, it is unlikely that the relationships between variables will be clear at the time the data is given, and in general it is necessary to consider what variables are being analyzed and how they are affected. I have.\n",
        "This work is called modeling.​\n",
        "The relationship between variables obtained by modeling is shown in a diagram, and the function itself is called a model, and the hypothetical function can be said to be one means of expressing the model."
      ],
      "metadata": {
        "id": "NPzM7uk4TqQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation using inner product\n",
        "import numpy as np\n",
        "\n",
        "# y=ax1+b\n",
        "a = 1\n",
        "b = 2\n",
        "x1 = 3\n",
        "y = a * x1 + b\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7eIqVXST5as",
        "outputId": "c08f3bfb-8535-4a82-d322-edc230770934"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's confirm that the inner product matches the calculation result of $ y = a x_1 + b $.\n",
        "In Python you can use $ @ $ to calculate the inner product.\n",
        "Equivalent processing can be performed by setting the part corresponding to $ b $ as the first component of $ \\ theta $ and setting the first component of the input $ X $ as $ 1 $."
      ],
      "metadata": {
        "id": "rwyVMnD1UFb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = np.array([[b], [a]])\n",
        "X = np.array([[1, x1]])\n",
        "y = X@theta\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYMKYHBsUDV6",
        "outputId": "416ebc7d-a17f-4531-9af5-7d710fcece46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to inner-product with the explanatory variables $X$ and $theta$, we need to match the columns of the previous matrix of the two matrices to be inner-producted with the rows of the second matrix. Checking shape withshapewithprint(X.shape), we get(1, 2)Checking the shaoewithprint(theta.shape) yields(2, 1). Therefore,X@thetais(1, 2)@(2, 1), which is a normal computation since the columns of the previous matrix and the rows of the second matrix match.\n",
        "\n",
        "In other words, the $y = X@theta$ part defines the assumed function of linear regression.\n",
        "\n",
        "It was confirmed that (1) above can be replaced with (2) using the expression of the inner product. Keep in mind that this notation is commonly used."
      ],
      "metadata": {
        "id": "0hLU2BOOUewr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Implementation**\n",
        "\n",
        "Let's try to run the implemented hypothetical function with the generated $\\theta$ and $X$ as arguments.\n",
        "In the following example, the hypothetical function is defined as _linear_hypothesis(theta, X).\n",
        "The single underscore in front of the function indicates that the function is used only within the class, and it is common practice to add an underscore to functions used only within the classto improve program readability. In Problem 1, it is not necessary to incorporate the function into the class, but if you want to incorporate it into a class to complete the scratch class for linear regression, give it self as its first argument."
      ],
      "metadata": {
        "id": "Ty1xRv4EUmdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_hypothesis(self, X):\n",
        "    \"\"\"\n",
        "    Hypothetical functionの出力を計算する\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    Returns\n",
        "    -------\n",
        "    of the following form. ndarray, shape (n_samples, 1)\n",
        "    線形のHypothetical functionによる推定結果\n",
        "    \"\"\"\n",
        "    pred = X @ self.theta\n",
        "    return pred"
      ],
      "metadata": {
        "id": "JL0AQTvIUzjC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. It takes an explanatory variable X as an argument. The first argument, self, specifies that this function is a member function.\n",
        "\n",
        "Lines 2-12: docstring (written function description)\n",
        "\n",
        "Line 13: Calculation of the predicted value and the output place of the assumed function. theta is then initialized in the fit function and treated as a member variable.\n",
        "\n",
        "Line 14: Return as return value"
      ],
      "metadata": {
        "id": "9FMLnvq6VEN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hypothetical function is a means of expressing what kind of model is assumed.\n",
        "The hypothetical function of linear regression can be calculated using matrix multiplication"
      ],
      "metadata": {
        "id": "BxVzTLRuVJ5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Machine Learning Scratch Understand the knowledge required to solve \"Problem 2\" in \"Machine Learning Scratch: Linear Regression - Problem 2 steepest descent method**\n",
        "\n",
        "In the previous text, we introduced that solving a regression equation means finding the best value of $\\theta$ (coefficient) for the data. This section explains how to find this $\\theta$."
      ],
      "metadata": {
        "id": "hNBoaYJ5VW9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best fit to the data is generally defined as the smallest average of the sum of the squares of the differences between the predicted and true values (sum of squares error).\n",
        "\n",
        "The difference between the predicted value and the true value is called the error, and the average of the sum of the squares is called the mean squared error (MSE)."
      ],
      "metadata": {
        "id": "OcWrEyMoV6tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE is often used as a goodness-of-fit measure because it is easy to implement.\n",
        "\n",
        "Mean squared error (MSE)\n",
        "The mean squared error is expressed by the following equation\n",
        "\n",
        "l\n",
        "(\n",
        "θ\n",
        ")\n",
        "=\n",
        "1\n",
        "m\n",
        "m\n",
        "∑\n",
        "i\n",
        "=\n",
        "1\n",
        "\n",
        "(\n",
        "h\n",
        "θ\n",
        "(\n",
        "x\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "−\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "2\n",
        ".\n",
        "$x^{(i)}$:$x$ value of $i$th data\n",
        "$y^{(i)}$:$y$ value of $i$th data\n",
        "$h_\\theta(x^{(i)})$: Predicted value of $i$th data\n",
        "The goal of the steepest descent method is to find $\\theta$ such that the value of this equation is minimized."
      ],
      "metadata": {
        "id": "4ge5Ox2aWLWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The steepest descent method (how to find $\\theta$)**\n",
        "\n",
        "In linear regression, when the horizontal axis is $\\theta$ and the vertical axis is the mean squared error, the function draws a parabola.\n",
        "To explain why it draws a parabola, a simple regression equation $ax+b$ is substituted into $H_\\theta(x^{(i)})$\n",
        "As you can see from this equation, it is a quadratic function that is convex downwards for the parameters $A, B$.\n",
        "Thus, if we plot the equation for the mean squared error with $L$ on the vertical axis and $\\theta$ on the horizontal axis, we see that it would look like the following figure (originally, we do not know the shape of the overall mean squared error).\n",
        "You can see that the point where the value of the mean squared error is the smallest is at\n",
        "\n",
        "The location of this $\\theta$ is found by the following procedure.\n",
        "\n",
        "First, plot based on a random $\\theta$\n",
        "Calculate MSE.\n",
        "Find the slope of a randomly plotted $\\theta$ position.\n",
        "\n",
        "Calculate the expression $\\frac{\\partial L(\\theta)}{\\partial \\theta}$ for the slope (gradient).\n",
        "It is computed in this way. Note that in the linear regression the coefficient of $\\theta_j$ is $x_{j}^{(i)}$.\n",
        "\n",
        "Thus, the equation representing the update is as follows"
      ],
      "metadata": {
        "id": "hLEkoRVfWNeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Functions**\n",
        "\n",
        "The update expression is implemented using python as follows.\n",
        "The following is a function extracted from the completed code, so the function does not work by itself.\n",
        "If you want the following function to work by itself, you need to define it as a Class definition and adjust the argument self, the fourth line that calls the function in the same Class, self.theta defined in the constructor, and so on."
      ],
      "metadata": {
        "id": "woRt-8UrXMKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient_descent(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        n = X.shape[1]\n",
        "        pred = self.linear_hypothesis(X)\n",
        "        for j in range(n):\n",
        "            gradient = 0\n",
        "            for i in range(m):\n",
        "                gradient += (pred[i] - y[i]) * X[i, j]\n",
        "            self.theta[j] = self.theta[j] - self.lr * (gradient / m)"
      ],
      "metadata": {
        "id": "5R0wrqYlXbXC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. It takes as arguments the explanatory variable X and the objective variable y.\n",
        "\n",
        "Line 2: The number of data is stored in m\n",
        "\n",
        "Line 3: The number of explanatory variables is stored in n\n",
        "\n",
        "Line 4: linear_hypothesis is executed to compute the predictions\n",
        "\n",
        "Line 5: Looping with the number n of explanatory variables.\n",
        "\n",
        "Line 6: Assigning 0 to gradient (the result of the calculation of Σ in the update formula)\n",
        "\n",
        "Line 7: Looping with m number of data.\n",
        "\n",
        "Line 8: (pred[i] - y[i]) * X[i, j]is the calculation inside Σ in the update formula, which is calculated to be the final Σ value by making it gradient +=​\n",
        "\n",
        "Line 9: The Σ(gradient) of the update formula has been calculated, and the corresponding theta (self.theta[j]) is updated using it.\n",
        "\n",
        "3. Summary\n",
        " The steepest descent method is a method for finding the values of parameters that minimize the output value of the objective function.\n",
        " The update equation is derived by differentiating the mean squared error (loss function).\n",
        " The image of the steepest descent method is like descending through a mean-square error trough"
      ],
      "metadata": {
        "id": "HTGrC1dBXn9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estimation**\n",
        "\n",
        "Finding output values using parameters is calledestimation/prediction. Let's create apredictmethod and have it return the predicted value as the return value"
      ],
      "metadata": {
        "id": "i2ERwUiLX9eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "    if self.bias == True:\n",
        "        bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "        X = np.hstack([bias, X])\n",
        "    pred_y = self._linear_hypothesis(X)\n",
        "    return pred_y"
      ],
      "metadata": {
        "id": "EIHTxH4mYRKb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. As arguments, we receive the definition of self and the explanatory variable X so that we can use the instance itself\n",
        "\n",
        "Row 2: Whether to use a bias term or not\n",
        "\n",
        "Row 3: Definition of the bias term (constant term). It is initialized at 1 for the number of data in the explanatory variable and reshapeso that the number of matrices is (number of data,1).\n",
        "\n",
        "Row 4: The bias term is combined with the explanatory variable X received as an argument.\n",
        "\n",
        "Row 5: Pass the explanatory variable X to the linear_hypothesis function, run it, and calculate the predictions.\n",
        "\n",
        "Line 6: The calculated predicted value is returned as the return value.\n",
        "\n",
        "4. Summary\n",
        "Estimation is the use of updated parameters to obtain predictions"
      ],
      "metadata": {
        "id": "TnjPebyNYTKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is mean square error (MSE)**\n",
        "\n",
        "The squared value of the \"difference between the correct and predicted values\" for each of the training data and the average of the squared values is called the mean squared error. The formula is as follows.\n",
        "$x^{(i)}$:$x$ value of $i$th data\n",
        "\n",
        "$y^{(i)}$:$y$ value of $i$th data\n",
        "\n",
        "$h_\\theta(x^{(i)})$: Predicted value of $i$th data\n",
        "\n",
        "Creating Functions\n",
        "The MSE defined above can be written using python as follows.\n",
        "Variables appearing in the function should be considered as follows.\n",
        "y_pred : the predicted value\n",
        "y : the correct value\n",
        "mse : the result of the mean squared error calculation"
      ],
      "metadata": {
        "id": "ISJSZz0QZBcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(self,y_pred, y):\n",
        "    mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "    return mse"
      ],
      "metadata": {
        "id": "f19zNBozZYp1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. It receives the predicted value y_pred and the correct answer y as arguments.\n",
        "\n",
        "Line 2: Calculation of MSE. The square of the error is calculated by (y_pred - y) ** 2, and the sum is calculated by ((y_pred - y) ** 2).sum(). The sum is divided by / X.shape[0] to compute the average.\n",
        "\n",
        "Line 3: MSE is returned as the return value."
      ],
      "metadata": {
        "id": "sD_Es4-UZaS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean squared error is the residual between the correct label and the prediction result squared and calculated for all training data\n",
        "NumPy makes it easy to calculate."
      ],
      "metadata": {
        "id": "cjkdNBS2Zi2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Objective function**\n",
        "\n",
        "The objective function can be expressed by the following equation\n",
        "\n",
        "It is the formula for the mean squared error further divided by two. It is called the objective function in the sense that it is the function of the objective to be minimized.\n",
        "\n",
        "Almost in the same sense is the term loss function. The function that determines how to evaluate the residuals is called the loss function, and the resultant function is the function of the objective (objective function) that should be minimized (maximized).\n",
        "\n",
        "There is also a term cost function, which can be safely assumed to mean almost the same thing.\n",
        "\n",
        "Here, the mean squared error could be expressed by the following equation.\n",
        "\n",
        "If you compare the formulas, the only difference is whether or not they are divided by 2. Therefore, the calculation can be performed using the MSE function we have created so far. Please refer to the text on the steepest descent method for the difference between this loss function and the mean squared error."
      ],
      "metadata": {
        "id": "sp4kaMxBZ0T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Functions\n",
        "\n",
        "The loss function defined above can be written using Python as follows."
      ],
      "metadata": {
        "id": "YYExBKDVala7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _loss_func(self,y_pred, y):\n",
        "    loss = self.MSE(pred, y)/2\n",
        "    return loss"
      ],
      "metadata": {
        "id": "EiybgEpzapkq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. It receives the predicted value y_pred and the correct answer y as arguments.\n",
        "\n",
        "Row 2: Perform the function MSEto calculate the mean squared error and divide by 2\n",
        "\n",
        "Line 3: Returns the calculated loss loss as the return value"
      ],
      "metadata": {
        "id": "q5DQ3xV8ay-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective function is equivalent to the correct label and the mean squared error divided by 2\n",
        "NumPy makes it easy to calculate"
      ],
      "metadata": {
        "id": "WN14Nd-ebAay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning and Estimation**\n",
        "\n",
        "In the problem so far, we have created the following function.\n",
        "\n",
        "_linear_hypothesis : Output computation of a hypothetical function\n",
        "\n",
        "_gradient_descent : Update $\\theta$.\n",
        "\n",
        "The purpose of this problem is to use these two functions (which can be modified) to implement theScratchLinearRegressionshown in the introduction. So let's implement the fit() method, which we have not yet done.\n",
        "\n",
        "Thefit()method is a function used for learning in scikit-learn. In this text, we follow this convention and use the fit()method as the learning method.\n",
        "\n",
        "Following the flow of the steepest descent algorithm, it is implemented as follows\n",
        "\n",
        "1.Initializelr(learning rate) and num_iter (number of training/iterations) & initialize $\\theta$ with __init__()(no_bias (presence of bias) andverbose (output of learning process) need not be considered for advanced tasks)\n",
        "\n",
        "2.Initializelr(learning rate) and num_iter(number of times learned) & initialize $\\theta$ with __init__()(no_bias (presence of bias) and verbose (output of learning process) need not be considered for advanced tasks)\n",
        "\n",
        "3.Estimates are calculated with _linear_hypothesis​\n",
        "\n",
        "4.Update $\\theta$ with_gradient_descent​\n",
        "Don't forget to calculate the loss value and save it in the list, because you can draw a learning curve by saving the loss value (solution $J(\\theta)$ of problem 5) each time here.\n",
        "5. Repeat 2 and 3 for num_iter (number of times learned)\n",
        "\n",
        "6.Estimate predict​"
      ],
      "metadata": {
        "id": "JFWkWCWXbOMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Functions / Now let's write fit() in the class.\n",
        "def fit(self, X, y, X_val, y_val):\n",
        "    if self.bias == True:\n",
        "\t      bias = np.ones((X.shape[0], 1))\n",
        "\t      X = np.hstack((bias, X))\n",
        "\t      bias = np.ones((X_val.shape[0], 1))\n",
        "\t      X_val = np.hstack((bias, X_val))\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "    for i in range(self.num_iter):\n",
        "        pred = self._linear_hypothesis(X)\n",
        "        pred_val = self._linear_hypothesis(X_val)\n",
        "        self._gradient_descent(X, y)\n",
        "        loss = self._loss_func(pred, y)\n",
        "        self.loss = np.append(self.loss, loss)\n",
        "        loss_val = self._loss_func(pred_val, y_val)\n",
        "        self.val_loss = np.append(self.val_loss, loss_val)\n",
        "        if verbose == True:\n",
        "            print('{}回目の学習の損失は{}'.format(i,loss))"
      ],
      "metadata": {
        "id": "UKKoNwRxbxPi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. As arguments, explanatory variables and objective variables of training and evaluation data are received, respectively.\n",
        "\n",
        "Line 2: Determine whether to use the bias term.\n",
        "\n",
        "Line 3-4: The bias term is initialized and combined with the explanatory variables of the \"training data\".\n",
        "\n",
        "Line 5-6: The bias term is initialized and combined with the explanatory variables for the \"evaluation data\".\n",
        "\n",
        "Line 7: theta is initialized to 1. the number of data in theta matches the number of data in the explanatory variable.\n",
        "\n",
        "Line 8: Shape is reshaped so that matrix operations can be performed.  （X.shape[1],）→（X.shape[1],1）\n",
        "\n",
        "Line 9: Loop for the number of studies\n",
        "\n",
        "Line 10: The hypothetical function is executed and the predicted value of the \"training data\" is calculated.\n",
        "\n",
        "Line 11: The hypothetical function is executed and the predicted value of the \"evaluation data\" is calculated.\n",
        "\n",
        "Line 12: The function of the steepest descent method is executed and theta is updated.\n",
        "\n",
        "Line 13: \"Calculating\" the loss of \"training data\".\n",
        "\n",
        "Line 14: \"Calculate\" the loss of \"valuation data\".\n",
        "\n",
        "Line 15: \"Storing\" the loss of \"training data\".\n",
        "\n",
        "Line 16: \"Stores\" the loss of \"valuation data\".\n",
        "\n",
        "Line 17: Determine whether to display the output of the learning process.\n",
        "\n",
        "Line 18: Output the learning process"
      ],
      "metadata": {
        "id": "ZMV81zAIcM7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preparation**\n",
        "\n",
        "Next, let's define the data to be used for training"
      ],
      "metadata": {
        "id": "W_DlCIVEcRqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "y = dataset.loc[:, ['SalePrice']]\n",
        "X = X.values\n",
        "y = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
      ],
      "metadata": {
        "id": "OmVAz7QfcWd7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lines 1-3: Importing the required libraries\n",
        "\n",
        "Line 4: train.csv is read using pandas\n",
        "\n",
        "Line 5: Only the variables you want to use as \"explanatory variables\" are retrieved.\n",
        "\n",
        "Line 6: Only the variable you want to use as the \"objective variable\" is retrieved.\n",
        "\n",
        "Lines 7-8: The pandas data type cannot be calculated, so it is converted to a numpy array.\n",
        "\n",
        "Line 9: Split training data and test data at a ratio of 8:2"
      ],
      "metadata": {
        "id": "U9GrQ7Afcowk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run**\n",
        "\n",
        "Next, let's move the model and check if it is implemented.\n",
        "\n",
        "Here, num_iter (number of iterations) is set to 10,lr(learning rate) to 0.01,no_bias (bias) is present, and verbose (learning process) is output."
      ],
      "metadata": {
        "id": "CabdAQascuvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slr = ScratchLinearRegression(num_iter=10, lr=0.01, no_bias=True, verbose=True)\n",
        "slr.fit(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "PVSR8SpSc2RM",
        "outputId": "e2402d6e-9faa-42a1-965e-2c43a12afc61"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'bias' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9b2e4218fea1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mslr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratchLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mslr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-374f3ffcd66d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_iter, lr, no_bias, verbose)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bias' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Instantiation by putting arguments into the created class\n",
        "Line 2: Learning execution"
      ],
      "metadata": {
        "id": "nkF8NZcvdCiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**predict**\n",
        "\n",
        "Next, let's make a prediction with the trained model."
      ],
      "metadata": {
        "id": "IyWKtd13dHz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slr.predict(X_test)"
      ],
      "metadata": {
        "id": "j7tidO93dMp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Prediction using test data"
      ],
      "metadata": {
        "id": "GFS-dfDkdRTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the \"Problem 7\" learning curve**\n",
        "\n",
        "learning curve\n",
        "In machine learning, we determine if the learning is going well or not, and make corrections and parameter adjustments.\n",
        "\n",
        "One way to check the progress of learning is to visualize the value of the loss function (objective function).\n",
        "\n",
        "A plot of the transition of loss values output by the loss function is called a learning curve.\n",
        "\n",
        "In solving the problem so far, the loss is stored in self.loss of the scratch class created. This is used to verify that the loss value is decreasing.\n",
        "\n",
        "Since the training and test data losses are stored inlossandval_loss, they are referenced and drawn respectively via instances"
      ],
      "metadata": {
        "id": "YHo0TqQMd1b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(reg.loss)\n",
        "plt.plot(reg.val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "5XSHdTXjeIY1",
        "outputId": "578c8839-5be8-4957-b87f-8dbcbff261b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reg' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-33ae008917cd>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Import required libraries\n",
        "Line 2: Setup to be able to draw on jupyter if using jupyter notebook\n",
        "Line 3: Draw training data loss\n",
        "Line 4: Draw test data loss"
      ],
      "metadata": {
        "id": "rUK-HXDceVZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw a learning curve with losses to see how learning is progressing\n",
        "Visualization of losses (plotting the learning curve) is also important to verify that the learning and implementation is working!"
      ],
      "metadata": {
        "id": "7S0Sz78gfPaj"
      }
    }
  ]
}