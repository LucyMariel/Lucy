{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV2iApwrGdOPfujuzBZL7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyMariel/Lucy/blob/master/TENSORFLOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal\n",
        "Understand the knowledge and skills necessary to solve the \"assignment Tensorflow\"\n",
        "\n",
        "2.purpose\n",
        "Understand the mathematics of TensorFlow\n",
        "\n",
        "3. How to learn\n",
        "Let's proceed while comparing it with the table of contents of \"assignment TensorFlow\".\n",
        "\n",
        "4. What is Tensorflow?\n",
        "TensorFlow is a free and open source software library for machine learning and artificial intelligence.\n",
        "\n",
        "5. Review \"Problem 1\" scratch\n",
        "- I had to initialize the weights\n",
        "- Forward propagation matrix calculations were required.\n",
        "- Matrix calculations for back propagation were required.\n",
        "- I needed an epoch loop\n",
        "- Losses had to be calculated."
      ],
      "metadata": {
        "id": "1YIUHo-uWey6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow is an open-source library developed by Google primarily for machine learning and artificial intelligence. It is widely used for building, training, and deploying deep learning models and other types of machine learning algorithms. Here are some of the main purposes and uses of TensorFlow:\n",
        "\n",
        "1. Building and Training Deep Learning Models\n",
        "TensorFlow is designed to facilitate the creation, training, and evaluation of deep neural networks, including:\n",
        "\n",
        "Convolutional Neural Networks (CNNs) for image recognition and computer vision tasks.\n",
        "Recurrent Neural Networks (RNNs) for sequence data, such as time series analysis and natural language processing (NLP).\n",
        "Transformer models for advanced NLP tasks like language translation and text generation.\n",
        "2. Implementation of Machine Learning Algorithms\n",
        "Besides deep learning, TensorFlow supports various traditional machine learning algorithms, such as:\n",
        "\n",
        "Linear and logistic regression\n",
        "Decision trees and random forests\n",
        "Clustering algorithms like k-means\n",
        "3. Natural Language Processing (NLP)\n",
        "TensorFlow is widely used in NLP tasks, including:\n",
        "\n",
        "Text classification and sentiment analysis\n",
        "Machine translation\n",
        "Named entity recognition (NER)\n",
        "Text summarization\n",
        "4. Computer Vision\n",
        "TensorFlow is used in many computer vision applications, including:\n",
        "\n",
        "Image classification\n",
        "Object detection\n",
        "Image segmentation\n",
        "Face recognition\n",
        "5. Reinforcement Learning\n",
        "TensorFlow supports reinforcement learning, where agents learn to make decisions by interacting with an environment to maximize some notion of cumulative reward.\n",
        "\n",
        "6. Time Series Analysis\n",
        "TensorFlow can be used for analyzing and predicting time series data, which is useful in applications like:\n",
        "\n",
        "Financial forecasting\n",
        "Demand prediction\n",
        "Anomaly detection\n",
        "7. Deployment and Scalability\n",
        "TensorFlow provides tools for deploying models in production, including:\n",
        "\n",
        "TensorFlow Serving: For serving machine learning models in production environments.\n",
        "TensorFlow Lite: For deploying models on mobile and embedded devices.\n",
        "TensorFlow.js: For running models in web browsers using JavaScript.\n",
        "8. Research and Development\n",
        "TensorFlow is widely used in academic and industrial research for developing new machine learning algorithms and techniques.\n",
        "\n",
        "9. Interoperability\n",
        "TensorFlow integrates well with other libraries and frameworks, making it easier to use in different environments and workflows."
      ],
      "metadata": {
        "id": "iROeHKp_YLzZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZncV4VV7oz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Problem 2\" Consider the correspondence between Scratch and TensorFlow.\n",
        "As you can see from the text of the assignment, the scratch code is a long code, so let's break it up by key points.\n",
        "\n",
        "The basic way to use TensorFlow is to define the flow of computation, such as forward and back propagation, and the structure of the network in the computation graph, and then submit the actual data during the execution of the computation graph.\n",
        "\n",
        "Let's understand the sample code in small pieces.\n",
        "\n",
        "Library import section\n",
        "The necessary libraries are imported."
      ],
      "metadata": {
        "id": "b1P4qFNzW7sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "yjZ_6tmcXZx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation\n",
        "The process for preparing this data is as follows.\n",
        "\n",
        "Data loading\n",
        "Discarding data for binary classification\n",
        "Split into explanatory and objective variables\n",
        "Convert to numpy array\n",
        "Converting natural language data into numerical values\n",
        "Split into training and test data\n",
        "Further divides training data into training data and evaluation data"
      ],
      "metadata": {
        "id": "Fib5Qf18YXci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Iris.csv\")\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "tyP6qiECYaUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Data loading\n",
        "\n",
        "Line 2: Discard data for binary classification\n",
        "\n",
        "Line 3 to 4: Split the data into explanatory and objective variables\n",
        "\n",
        "Lines 5 to 6: Convert to numpy array\n",
        "\n",
        "Lines 7 to 8: Convert natural language data to numeric values\n",
        "\n",
        "Line 9: Conversion of the shape of the objective variable\n",
        "\n",
        "Line 10: Split training data and test data\n",
        "\n",
        "Line 11: Further split training data into training data and evaluation data\n",
        "\n",
        "\n",
        "Creating classes for mini-batch processing\n",
        "\n",
        "Functions such as getitem, iter, and next may not be familiar to you from your study up to this point, but like init, these are special functions that are given specific functions from the beginning in the python specification. In the case of init, it is automatically called when an instance is created.\n",
        "\n",
        "\n",
        "A simple example of a homebrew iterator (Homebrew is a free and open-source package management system for macOS and Linux. It simplifies the process of installing, updating, and managing software packages and libraries. Homebrew is popular among developers because it makes it easy to install and maintain the tools and dependencies needed for software development.)"
      ],
      "metadata": {
        "id": "RA-c0_WGYnRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleIterator():\n",
        "    def __init__(self):\n",
        "        self.X = [1,2,3,4,5]\n",
        "        self.counter = 0\n",
        "        self.stop = len(self.X)\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self.counter >= self.stop:\n",
        "            raise StopIteration()\n",
        "        x = self.X[self.counter]\n",
        "        self.counter += 1\n",
        "        return x\n",
        "sample_iter = SampleIterator()\n",
        "for x in sample_iter:\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "-HfT-qlMZU-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Class definition.\n",
        "\n",
        "Lines 2 to 5: Definition of the constructor. Array X, current position counter, and end position stop are defined.\n",
        "\n",
        "Lines 6 to 7: Definition of the iterator. The instance itself is returned.\n",
        "\n",
        "Line 8: Definition of the next method. It is automatically called each time the iterator is called.\n",
        "\n",
        "Lines 9 to 10: Termination of the iterator when the end position defined in the constructor is reached.\n",
        "\n",
        "Line 11: The value returned by the iterator is retrieved\n",
        "\n",
        "Line 12: The current position is updated\n",
        "\n",
        "Line 13: Returns the value\n",
        "\n",
        "Line 14: Creating the iterator\n",
        "\n",
        "Line 15: The iterator created on line 14 is passed around with a for statement\n",
        "\n",
        "Line 16: The value generated by the iterator is output with a print statement"
      ],
      "metadata": {
        "id": "Td_taxwHZV-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "metadata": {
        "id": "zHRb4iWNZk6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "metadata": {
        "id": "Wb3lXEE8adZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Class definition.\n",
        "Line 2: Constructor definition. It takes as arguments an explanatory variable X, an objective variable y, a batch size batch_size, and a random number seed seed.\n",
        "Line 3: Making batch_size a member variable\n",
        "Line 4: Fixing the random seed\n",
        "Line 5: Shuffling of all indices\n",
        "Line 6 to 7: Reordering of X,y\n",
        "Line 8: Define the end position\n",
        "Lines 9 to 10: Definition of special methods when using the built-in function len()\n",
        "Lines 11 to 14: Definition of special method called when using dictionary type\n",
        "Lines 15 to 17: Necessary when defining the special method next, which returns the instance itself\n",
        "Line 18: Definition of the next method\n",
        "Lines 19 to 20: Terminate the iterator when the end position defined in the constructor is reached\n",
        "Lines 21 to 22: Obtain the starting and ending points of the index\n",
        "Line 23: Update the current position\n",
        "Line 24: Retrieve and return data according to the start and end points"
      ],
      "metadata": {
        "id": "U2ZKmA8xae1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter settings (Hyperparameters are the parameters of a machine learning model that are set before the learning process begins. Unlike model parameters (such as weights in neural networks) which are learned from the data during training, hyperparameters are configured manually and are not learned from the training data. They play a crucial role in the training process and can significantly affect the performance and efficiency of the model.)\n",
        "We will set up the necessary parameters for the study.\n",
        "\n",
        "In this case, we set the learning rate, batch size, number of training sessions, and number of nodes in the middle layer.\n",
        "\n",
        "The n_input and n_classes are used to determine the shape of the input and output layers, rather than to set hyperparameters (parameters that are naturally determined by the type of data used). (Parameters that are naturally determined by the type of data used)\n",
        "\n",
        "For N_SAMPLES, it is used when calculating LOSS."
      ],
      "metadata": {
        "id": "KdeM2aPBamsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1"
      ],
      "metadata": {
        "id": "FTToHgNpbW2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the shape of the inputs and outputs of the network\n",
        "\n",
        "The shape of the input/output is determined by specifying the type as the first argument and the appropriation as the second argument in tf.placeholder. The reason it is set to None is to allow for data of any length (mini-batch)."
      ],
      "metadata": {
        "id": "2LL4Yb6Abdtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])"
      ],
      "metadata": {
        "id": "qBO4kWlzbjYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minibatch generation iterator\n",
        "\n",
        "As we have just seen in a simple example, it generates an iterator. This iterator will output more and more different batches just by calling it. For now, we will just define it and use it in the for statement."
      ],
      "metadata": {
        "id": "dgUamS-_btFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "kBCKyms_bxh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "TensorFlow refers to an end-to-end open source platform for machine learning\n",
        "\n",
        "What we did with the Scratch Neural Network in the previous assignment can easily be done using TensorFlow"
      ],
      "metadata": {
        "id": "FC8DrJuEb4jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic regression created with TensorFlow**"
      ],
      "metadata": {
        "id": "xxRqgKJCdUAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "purpose\n",
        "Implement a neural network with TensorFlow\n",
        "\n",
        "How to learn\n",
        "Let's proceed while comparing it with the table of contents of \"assignment TensorFlow\".\n",
        "\n",
        "Implementation of neural network, Creating a network, Create an example_net function and define the network in the following order.\n",
        "\n",
        "- Weight initialization\n",
        "- Bias initialization\n",
        "- Layer definition\n",
        "- Return the layer you created (this will be the model for your network)"
      ],
      "metadata": {
        "id": "Im8QCc2CdhyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def example_net(x):\n",
        "    tf.random.set_random_seed(0)\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output"
      ],
      "metadata": {
        "id": "bgybwmBHd1jc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition\n",
        "\n",
        "2nd line: Random seed fixing\n",
        "\n",
        "Lines 3-7: Definition of weights\n",
        "\n",
        "Lines 8-12: Definition of Bias\n",
        "\n",
        "13th line: Calculate the product of input data (x) and weight (w1) → add bias (b1) (let's call it layer1)\n",
        "\n",
        "14th line: Apply the activation function relu to the result of the 13th line (let's call it layer1)\n",
        "\n",
        "15th line: Calculate the product of the result (layer1) and weight (w2) on the 14th line → add the bias (b2) (let's call it layer2)\n",
        "\n",
        "16th line: Apply the activation function relu to the result (layer2) on the 15th line (let's call it layer2).\n",
        "\n",
        "17th line: Calculate the product of the result (layer2) and weight (w3) on the 16th line → add the bias (b3) (let's call it layer_output)\n",
        "\n",
        "Line 18: returns layer_output\n",
        "\n",
        "\n",
        "To initialize weights and biases, tf.random_normal is used to generate a random number with a normal distribution, and then tf.Variable is used to convert it to a format that can be handled by tensorflow. To generate the layer, tf.matmul is used to calculate the matrix of those weights, bias, and input array x as arguments.\n",
        "\n",
        "\n",
        "\n",
        "Definition of processing required for learning"
      ],
      "metadata": {
        "id": "alXeyv3ad7lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = example_net(X)\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "init = tf.global_variables_initializer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "U4vMI-tseMzp",
        "outputId": "187e7a7a-692c-446d-ad6e-10294f1df2d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8e4a213a8045>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorrect_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1st line: Read network structure. Call the function that creates the network you just created.\n",
        "\n",
        "Line 2: Definition of the objective function loss_op. Calculate the error using the cross entropy error function. The arguments are the correct answer data Y and the predicted value logits.\n",
        "\n",
        "Lines 3-4: Definition of optimization method. Instantiate Adam as an optimizer and in optimizer.minimize(loss_op), the optimizer will update the weights and biases to minimize loss_op. optimizer.minimize(loss_op) as train_op.\n",
        "\n",
        "Line 5: Define the estimation result. tf.sign(tf.sigmoid(logits) - 0.5)) is negative if the predicted label is 0 (tf.sigmoid (logits) is 0.5 or less), and conversely returns positive if it is predicted to be 1. Next, in the part of tf.sign(Y - 0.5), the label value (0 or 1) is converted to positive or negative. In other words, if the prediction is correct, tf.equal will be true, and if not, it will be false.\n",
        "\n",
        "6th line: Index value calculation. ACC (correct answer rate) is calculated.\n",
        "\n",
        "Line 7: Initialization of variable\n",
        "\n",
        "\n",
        "Execution of calculation graph"
      ],
      "metadata": {
        "id": "kWmvRRLgZrBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "Q3IDj350ehsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of these, the important parts are only the following parts. Other than that, it is the part where the loss and the correct answer rate are calculated."
      ],
      "metadata": {
        "id": "pyYG321Oelgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session()  as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in  range(num_epochs):\n",
        "        for i,  (mini_batch_x, mini_batch_y)  in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})"
      ],
      "metadata": {
        "id": "ErYUGIQ-emnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lines 1 to 2: Start of Session\n",
        "\n",
        "3rd line: Loop for the number of learnings\n",
        "\n",
        "Line 4: Loop with mini-batch iterator"
      ],
      "metadata": {
        "id": "HYQeDtKfetYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "Implemented neural network with TensorFlow"
      ],
      "metadata": {
        "id": "6YMaGyOVezqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensorflow and various models**"
      ],
      "metadata": {
        "id": "94mGlUKOfExx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solve classification problems, regression problems, and NNs using Tensorflow\n",
        "\n",
        "\"Problem 3\" Create a model of Iris using all three objective variables\n",
        "Based on Problem 2, this question has the following changes:\n",
        "\n",
        "- One-hot vectorization at the data preparation stage\n",
        "- Change the number of nodes in the output layer\n",
        "- Changed loss function and loss calculation from sigmoid to softmax\n",
        "- Calculation method of estimation result\n",
        "\n",
        "\n",
        "Data creation"
      ],
      "metadata": {
        "id": "RXkCxdQBfHVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "id": "RI11ASmFfZB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lines 1 to 2: Import library\n",
        "3rd to 7th lines: Creating explanatory variables and objective variables\n",
        "8th to 9th lines: Divided into 3 parts: training, test, and evaluation data\n",
        "10th to 13th lines: one-hot vectorization\n",
        "Lines 14-17: Normalization\n",
        "\n",
        "Internet work changes\n",
        "\n",
        "Only the changes from the binary classification that has been implemented so far are described."
      ],
      "metadata": {
        "id": "hUVznEfTflAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 3\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))"
      ],
      "metadata": {
        "id": "QZndins5fqdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st line: Change the number of nodes in the output layer\n",
        "\n",
        "2nd line: Changed loss function and loss calculation from sigmoid to softmax\n",
        "\n",
        "3rd line: Calculation method of estimation result"
      ],
      "metadata": {
        "id": "ECO8fJ0FfwHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create a model for \"Problem 4\" House Prices\n",
        "Based on Problem 2, this question has the following changes:\n",
        "\n",
        "No need for one-hot vectorization during data preparation\n",
        "For regression problems, the number of nodes in the output layer is 1.\n",
        "Use the root-mean square error to calculate the loss function and loss\n",
        "\n",
        "\n",
        "Data creation"
      ],
      "metadata": {
        "id": "9yKux9Mgf1oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path =\"train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "id": "p1Fute_Mf_UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lines 1 to 2: Read data\n",
        "\n",
        "Lines 3-8: Creating explanatory variables and objective variables\n",
        "\n",
        "9th to 10th lines: Divided into 3 parts: training, test, and evaluation data\n",
        "\n",
        "Lines 11-14: Normalization"
      ],
      "metadata": {
        "id": "8W3q-T1vgDIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Internet work changes\n",
        "\n",
        "Only the changes from the binary classification that has been implemented so far are described."
      ],
      "metadata": {
        "id": "qkulONqvgKah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 1\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)"
      ],
      "metadata": {
        "id": "gLVzTeTigPfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st line: Change the number of nodes in the output layer\n",
        "\n",
        "2nd line: Change the loss function to mean square error\n",
        "\n",
        "\n",
        "4. \"Problem 5\" Create a model of MNIST\n",
        "\n",
        "The basic flow and structure are the same as in Problem 3, but the number of classes to be predicted (the number of nodes in the output layer) is significantly different.\n",
        "\n",
        "\n",
        "Data creation"
      ],
      "metadata": {
        "id": "F7jv_9pTgTm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "metadata": {
        "id": "EeoNVQO8gYMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st line: Read data\n",
        "\n",
        "Lines 2-9: Creating explanatory and objective variables\n",
        "\n",
        "Lines 10-12: one-hot vectorization\n",
        "\n",
        "Line 13: Divide training data into training data and evaluation data"
      ],
      "metadata": {
        "id": "XLKRKDQngeZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network changes\n",
        "\n",
        "Only the changes from the binary classification that has been implemented so far are described."
      ],
      "metadata": {
        "id": "pvIvYn31gjiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 10\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))"
      ],
      "metadata": {
        "id": "bJ481lXugnWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st line: Change the number of nodes in the output layer\n",
        "\n",
        "2nd line: Changed loss function and loss calculation from sigmoid to softmax\n",
        "\n",
        "3rd line: Calculation method of estimation result\n",
        "\n",
        "\n",
        "Summary\n",
        "\n",
        "You can use TensorFlow to prepare various models and solve various problems."
      ],
      "metadata": {
        "id": "fUBohHIxgr9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorFlow - Asignment**\n"
      ],
      "metadata": {
        "id": "4twE4fdCh_ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code reading\n",
        "I have posted a sample code that performs binary classification by TensorFlow. This time, we will proceed based on this.\n",
        "\n",
        "We don't use high level APIs like tf.keras or tf.estimator. Let's start from a low level.\n",
        "\n",
        "[Problem 1] Looking back on scratch\n",
        "Look back at the scratches so far and list what you needed to do to implement deep learning.\n",
        "\n",
        "(example)\n",
        "\n",
        "I had to initialize the weights\n",
        "I needed an epoch loop\n",
        "Let's learn how they are implemented in the framework.\n",
        "\n",
        "Preparing the dataset\n",
        "We will use the Iris dataset that we have been using previously. The following sample code assumes thatIris.csvis in the same hierarchy.\n",
        "\n",
        "Iris Species\n",
        "\n",
        "The objective variable isSpecies, but only the following two types are used among the three types.\n",
        "\n",
        "Iris-versicolor\n",
        "Iris-virginica\n",
        "[Problem 2] Consider the correspondence between scratch and TensorFlow\n",
        "Check the sample code below to see how TensorFlow implements the \"things you need to implement deep learning\" that we listed earlier.\n",
        "\n",
        "Please summarize it in simple words. It is not always a simple one-to-one correspondence.\n",
        "\n",
        "《sample code》\n",
        "\n",
        "* Operation has been confirmed for TensorFlow versions 1.5 to 1.14."
      ],
      "metadata": {
        "id": "atIVb13GinKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# Condition extraction from data frame\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# Convert to NumPy array\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Convert labels to numbers\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(int)  # Replace np.int with int\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Model definition\n",
        "class ExampleNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ExampleNet, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
        "        self.dense3 = tf.keras.layers.Dense(n_classes)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = ExampleNet()\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, tf.sigmoid(predictions))\n",
        "\n",
        "@tf.function\n",
        "def val_step(inputs, labels):\n",
        "    inputs = tf.expand_dims(inputs, axis=0)  # Expand dimensions to simulate batch size of 1\n",
        "    predictions = model(inputs, training=False)\n",
        "    loss = loss_object(labels, predictions)\n",
        "    val_loss(loss)\n",
        "    val_accuracy(labels, tf.sigmoid(predictions))\n",
        "\n",
        "# Train and validate the model\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    val_loss.reset_states()\n",
        "    val_accuracy.reset_states()\n",
        "\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        train_step(mini_batch_x, mini_batch_y)\n",
        "\n",
        "    for val_x, val_y in zip(X_val, y_val):\n",
        "        val_step(val_x, val_y)\n",
        "\n",
        "    template = \"Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}\"\n",
        "    print(template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result()*100,\n",
        "                          val_loss.result(),\n",
        "                          val_accuracy.result()*100))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
        "test_predictions = model(X_test, training=False)\n",
        "test_accuracy.update_state(y_test, tf.sigmoid(test_predictions))\n",
        "print(\"Test Accuracy: {:.3f}\".format(test_accuracy.result().numpy() * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNl0wioloO36",
        "outputId": "81526284-2a2e-491d-b6c1-71ced50c82bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6963118314743042, Accuracy: 53.125, Val Loss: 0.7282048463821411, Val Accuracy: 37.5\n",
            "Epoch 2, Loss: 0.6574878692626953, Accuracy: 53.125, Val Loss: 0.7000768184661865, Val Accuracy: 37.5\n",
            "Epoch 3, Loss: 0.639312207698822, Accuracy: 53.125, Val Loss: 0.6759079694747925, Val Accuracy: 37.5\n",
            "Epoch 4, Loss: 0.627842903137207, Accuracy: 53.125, Val Loss: 0.6655042767524719, Val Accuracy: 43.75\n",
            "Epoch 5, Loss: 0.6169877052307129, Accuracy: 56.25, Val Loss: 0.6602530479431152, Val Accuracy: 43.75\n",
            "Epoch 6, Loss: 0.605694591999054, Accuracy: 56.25, Val Loss: 0.6507806181907654, Val Accuracy: 43.75\n",
            "Epoch 7, Loss: 0.5942298173904419, Accuracy: 56.25, Val Loss: 0.6372605562210083, Val Accuracy: 50.0\n",
            "Epoch 8, Loss: 0.5831251740455627, Accuracy: 64.0625, Val Loss: 0.6246067881584167, Val Accuracy: 50.0\n",
            "Epoch 9, Loss: 0.5722336173057556, Accuracy: 70.3125, Val Loss: 0.613747775554657, Val Accuracy: 68.75\n",
            "Epoch 10, Loss: 0.5612433552742004, Accuracy: 73.4375, Val Loss: 0.6032903790473938, Val Accuracy: 75.0\n",
            "Epoch 11, Loss: 0.549869179725647, Accuracy: 78.125, Val Loss: 0.5914978384971619, Val Accuracy: 75.0\n",
            "Epoch 12, Loss: 0.5383124351501465, Accuracy: 79.6875, Val Loss: 0.5789435505867004, Val Accuracy: 75.0\n",
            "Epoch 13, Loss: 0.5266875624656677, Accuracy: 81.25, Val Loss: 0.5664483904838562, Val Accuracy: 81.25\n",
            "Epoch 14, Loss: 0.5148201584815979, Accuracy: 82.8125, Val Loss: 0.553965151309967, Val Accuracy: 87.5\n",
            "Epoch 15, Loss: 0.5027405023574829, Accuracy: 89.0625, Val Loss: 0.5413230061531067, Val Accuracy: 87.5\n",
            "Epoch 16, Loss: 0.4904877245426178, Accuracy: 89.0625, Val Loss: 0.528427243232727, Val Accuracy: 87.5\n",
            "Epoch 17, Loss: 0.47818684577941895, Accuracy: 89.0625, Val Loss: 0.5155569314956665, Val Accuracy: 87.5\n",
            "Epoch 18, Loss: 0.465848445892334, Accuracy: 89.0625, Val Loss: 0.5026937127113342, Val Accuracy: 87.5\n",
            "Epoch 19, Loss: 0.4535128176212311, Accuracy: 89.0625, Val Loss: 0.4897458255290985, Val Accuracy: 93.75\n",
            "Epoch 20, Loss: 0.4412349760532379, Accuracy: 89.0625, Val Loss: 0.4767966866493225, Val Accuracy: 93.75\n",
            "Epoch 21, Loss: 0.4291205406188965, Accuracy: 89.0625, Val Loss: 0.46415847539901733, Val Accuracy: 93.75\n",
            "Epoch 22, Loss: 0.41706234216690063, Accuracy: 89.0625, Val Loss: 0.4515472948551178, Val Accuracy: 93.75\n",
            "Epoch 23, Loss: 0.40505555272102356, Accuracy: 89.0625, Val Loss: 0.4387677311897278, Val Accuracy: 93.75\n",
            "Epoch 24, Loss: 0.39326077699661255, Accuracy: 92.1875, Val Loss: 0.42625343799591064, Val Accuracy: 93.75\n",
            "Epoch 25, Loss: 0.38171836733818054, Accuracy: 92.1875, Val Loss: 0.4141033887863159, Val Accuracy: 100.0\n",
            "Epoch 26, Loss: 0.3702722489833832, Accuracy: 92.1875, Val Loss: 0.401969850063324, Val Accuracy: 100.0\n",
            "Epoch 27, Loss: 0.3583449423313141, Accuracy: 92.1875, Val Loss: 0.3888002336025238, Val Accuracy: 100.0\n",
            "Epoch 28, Loss: 0.3461304008960724, Accuracy: 92.1875, Val Loss: 0.3719186782836914, Val Accuracy: 100.0\n",
            "Epoch 29, Loss: 0.3335980474948883, Accuracy: 96.875, Val Loss: 0.35984477400779724, Val Accuracy: 100.0\n",
            "Epoch 30, Loss: 0.3231869637966156, Accuracy: 96.875, Val Loss: 0.35002171993255615, Val Accuracy: 100.0\n",
            "Epoch 31, Loss: 0.3119608461856842, Accuracy: 96.875, Val Loss: 0.33808815479278564, Val Accuracy: 100.0\n",
            "Epoch 32, Loss: 0.3017410635948181, Accuracy: 96.875, Val Loss: 0.3269997239112854, Val Accuracy: 100.0\n",
            "Epoch 33, Loss: 0.29230180382728577, Accuracy: 96.875, Val Loss: 0.31722044944763184, Val Accuracy: 100.0\n",
            "Epoch 34, Loss: 0.282768577337265, Accuracy: 96.875, Val Loss: 0.3071654736995697, Val Accuracy: 100.0\n",
            "Epoch 35, Loss: 0.27372807264328003, Accuracy: 96.875, Val Loss: 0.29744288325309753, Val Accuracy: 100.0\n",
            "Epoch 36, Loss: 0.26525992155075073, Accuracy: 96.875, Val Loss: 0.28844723105430603, Val Accuracy: 100.0\n",
            "Epoch 37, Loss: 0.25701773166656494, Accuracy: 96.875, Val Loss: 0.2796781361103058, Val Accuracy: 100.0\n",
            "Epoch 38, Loss: 0.24914292991161346, Accuracy: 96.875, Val Loss: 0.27121859788894653, Val Accuracy: 100.0\n",
            "Epoch 39, Loss: 0.24168433248996735, Accuracy: 96.875, Val Loss: 0.26320645213127136, Val Accuracy: 100.0\n",
            "Epoch 40, Loss: 0.23452425003051758, Accuracy: 96.875, Val Loss: 0.2554905414581299, Val Accuracy: 100.0\n",
            "Epoch 41, Loss: 0.22769096493721008, Accuracy: 96.875, Val Loss: 0.24812667071819305, Val Accuracy: 100.0\n",
            "Epoch 42, Loss: 0.2211650162935257, Accuracy: 96.875, Val Loss: 0.24110208451747894, Val Accuracy: 100.0\n",
            "Epoch 43, Loss: 0.21483708918094635, Accuracy: 96.875, Val Loss: 0.23430009186267853, Val Accuracy: 100.0\n",
            "Epoch 44, Loss: 0.20870907604694366, Accuracy: 96.875, Val Loss: 0.22750845551490784, Val Accuracy: 100.0\n",
            "Epoch 45, Loss: 0.20251548290252686, Accuracy: 96.875, Val Loss: 0.2199372947216034, Val Accuracy: 100.0\n",
            "Epoch 46, Loss: 0.19538883864879608, Accuracy: 96.875, Val Loss: 0.2113504260778427, Val Accuracy: 100.0\n",
            "Epoch 47, Loss: 0.18788786232471466, Accuracy: 96.875, Val Loss: 0.20346194505691528, Val Accuracy: 100.0\n",
            "Epoch 48, Loss: 0.18139846622943878, Accuracy: 96.875, Val Loss: 0.19682377576828003, Val Accuracy: 100.0\n",
            "Epoch 49, Loss: 0.17597274482250214, Accuracy: 96.875, Val Loss: 0.19094158709049225, Val Accuracy: 100.0\n",
            "Epoch 50, Loss: 0.1708735078573227, Accuracy: 96.875, Val Loss: 0.18540412187576294, Val Accuracy: 100.0\n",
            "Epoch 51, Loss: 0.16622185707092285, Accuracy: 96.875, Val Loss: 0.18029288947582245, Val Accuracy: 100.0\n",
            "Epoch 52, Loss: 0.16190576553344727, Accuracy: 96.875, Val Loss: 0.1755111664533615, Val Accuracy: 100.0\n",
            "Epoch 53, Loss: 0.15784896910190582, Accuracy: 96.875, Val Loss: 0.17098645865917206, Val Accuracy: 100.0\n",
            "Epoch 54, Loss: 0.15409550070762634, Accuracy: 96.875, Val Loss: 0.16674533486366272, Val Accuracy: 100.0\n",
            "Epoch 55, Loss: 0.15055809915065765, Accuracy: 96.875, Val Loss: 0.16273640096187592, Val Accuracy: 100.0\n",
            "Epoch 56, Loss: 0.14721791446208954, Accuracy: 96.875, Val Loss: 0.15893061459064484, Val Accuracy: 100.0\n",
            "Epoch 57, Loss: 0.14407601952552795, Accuracy: 96.875, Val Loss: 0.1553184539079666, Val Accuracy: 100.0\n",
            "Epoch 58, Loss: 0.1410994678735733, Accuracy: 96.875, Val Loss: 0.15187999606132507, Val Accuracy: 100.0\n",
            "Epoch 59, Loss: 0.1382833868265152, Accuracy: 96.875, Val Loss: 0.14860980212688446, Val Accuracy: 100.0\n",
            "Epoch 60, Loss: 0.13560722768306732, Accuracy: 96.875, Val Loss: 0.1454857885837555, Val Accuracy: 100.0\n",
            "Epoch 61, Loss: 0.13305984437465668, Accuracy: 96.875, Val Loss: 0.1425022929906845, Val Accuracy: 100.0\n",
            "Epoch 62, Loss: 0.13063740730285645, Accuracy: 96.875, Val Loss: 0.13964995741844177, Val Accuracy: 100.0\n",
            "Epoch 63, Loss: 0.1283269077539444, Accuracy: 96.875, Val Loss: 0.13691796362400055, Val Accuracy: 100.0\n",
            "Epoch 64, Loss: 0.12612156569957733, Accuracy: 96.875, Val Loss: 0.13430044054985046, Val Accuracy: 100.0\n",
            "Epoch 65, Loss: 0.12401539087295532, Accuracy: 96.875, Val Loss: 0.13178980350494385, Val Accuracy: 100.0\n",
            "Epoch 66, Loss: 0.12199600040912628, Accuracy: 96.875, Val Loss: 0.1293783187866211, Val Accuracy: 100.0\n",
            "Epoch 67, Loss: 0.12006530910730362, Accuracy: 96.875, Val Loss: 0.12706051766872406, Val Accuracy: 100.0\n",
            "Epoch 68, Loss: 0.11821520328521729, Accuracy: 96.875, Val Loss: 0.12483101338148117, Val Accuracy: 100.0\n",
            "Epoch 69, Loss: 0.11643970012664795, Accuracy: 96.875, Val Loss: 0.12268500030040741, Val Accuracy: 100.0\n",
            "Epoch 70, Loss: 0.11473409831523895, Accuracy: 96.875, Val Loss: 0.12061741948127747, Val Accuracy: 100.0\n",
            "Epoch 71, Loss: 0.11309614032506943, Accuracy: 96.875, Val Loss: 0.11862388253211975, Val Accuracy: 100.0\n",
            "Epoch 72, Loss: 0.11151539534330368, Accuracy: 96.875, Val Loss: 0.11670106649398804, Val Accuracy: 100.0\n",
            "Epoch 73, Loss: 0.10999990999698639, Accuracy: 96.875, Val Loss: 0.11484414339065552, Val Accuracy: 100.0\n",
            "Epoch 74, Loss: 0.10853298008441925, Accuracy: 96.875, Val Loss: 0.11305093765258789, Val Accuracy: 100.0\n",
            "Epoch 75, Loss: 0.10712505877017975, Accuracy: 96.875, Val Loss: 0.11131635308265686, Val Accuracy: 100.0\n",
            "Epoch 76, Loss: 0.10576038062572479, Accuracy: 96.875, Val Loss: 0.10964107513427734, Val Accuracy: 100.0\n",
            "Epoch 77, Loss: 0.10444603860378265, Accuracy: 96.875, Val Loss: 0.10801844298839569, Val Accuracy: 100.0\n",
            "Epoch 78, Loss: 0.10317986458539963, Accuracy: 96.875, Val Loss: 0.10644587129354477, Val Accuracy: 100.0\n",
            "Epoch 79, Loss: 0.10194623470306396, Accuracy: 96.875, Val Loss: 0.10492565482854843, Val Accuracy: 100.0\n",
            "Epoch 80, Loss: 0.1007578894495964, Accuracy: 96.875, Val Loss: 0.10345131903886795, Val Accuracy: 100.0\n",
            "Epoch 81, Loss: 0.09960800409317017, Accuracy: 96.875, Val Loss: 0.1020202487707138, Val Accuracy: 100.0\n",
            "Epoch 82, Loss: 0.09849190711975098, Accuracy: 96.875, Val Loss: 0.10063265264034271, Val Accuracy: 100.0\n",
            "Epoch 83, Loss: 0.09740907698869705, Accuracy: 96.875, Val Loss: 0.0992857962846756, Val Accuracy: 100.0\n",
            "Epoch 84, Loss: 0.09636139869689941, Accuracy: 96.875, Val Loss: 0.0979783833026886, Val Accuracy: 100.0\n",
            "Epoch 85, Loss: 0.09534434229135513, Accuracy: 96.875, Val Loss: 0.096705861389637, Val Accuracy: 100.0\n",
            "Epoch 86, Loss: 0.09435643255710602, Accuracy: 96.875, Val Loss: 0.09546955674886703, Val Accuracy: 100.0\n",
            "Epoch 87, Loss: 0.09336573630571365, Accuracy: 96.875, Val Loss: 0.09463613480329514, Val Accuracy: 100.0\n",
            "Epoch 88, Loss: 0.09312634915113449, Accuracy: 96.875, Val Loss: 0.09299803525209427, Val Accuracy: 100.0\n",
            "Epoch 89, Loss: 0.09159135818481445, Accuracy: 96.875, Val Loss: 0.09200716763734818, Val Accuracy: 100.0\n",
            "Epoch 90, Loss: 0.0906381830573082, Accuracy: 96.875, Val Loss: 0.09098261594772339, Val Accuracy: 100.0\n",
            "Epoch 91, Loss: 0.08996976166963577, Accuracy: 96.875, Val Loss: 0.08971879631280899, Val Accuracy: 100.0\n",
            "Epoch 92, Loss: 0.08895720541477203, Accuracy: 96.875, Val Loss: 0.08871683478355408, Val Accuracy: 100.0\n",
            "Epoch 93, Loss: 0.08811578899621964, Accuracy: 96.875, Val Loss: 0.08772464841604233, Val Accuracy: 100.0\n",
            "Epoch 94, Loss: 0.0873827412724495, Accuracy: 96.875, Val Loss: 0.08665578812360764, Val Accuracy: 100.0\n",
            "Epoch 95, Loss: 0.08654888719320297, Accuracy: 96.875, Val Loss: 0.0856947973370552, Val Accuracy: 100.0\n",
            "Epoch 96, Loss: 0.0857786312699318, Accuracy: 96.875, Val Loss: 0.0847538560628891, Val Accuracy: 100.0\n",
            "Epoch 97, Loss: 0.085057832300663, Accuracy: 96.875, Val Loss: 0.08380089700222015, Val Accuracy: 100.0\n",
            "Epoch 98, Loss: 0.0843157172203064, Accuracy: 96.875, Val Loss: 0.0828995332121849, Val Accuracy: 100.0\n",
            "Epoch 99, Loss: 0.0836053341627121, Accuracy: 96.875, Val Loss: 0.08201836049556732, Val Accuracy: 100.0\n",
            "Epoch 100, Loss: 0.08292119950056076, Accuracy: 96.875, Val Loss: 0.08114459365606308, Val Accuracy: 100.0\n",
            "Test Accuracy: 90.000\n"
          ]
        }
      ]
    }
  ]
}