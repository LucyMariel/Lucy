{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO329lgeBdAgA2c8Qu/2JUj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyMariel/Lucy/blob/master/CNN1Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose\n",
        "Understand the mathematical knowledge of convolutional neural networks.\n",
        "More in-depth tips for scratch implementation of convolutional neural networks are provided, so that you can scratch implement them.\n",
        "\n",
        "What is convolutional neural network (CNN)?\n",
        "Convolutional Neural Networks are called Convolutional Neural Networks (CNN). It is used to detect and classify objects in an image.\n",
        "\n",
        "We have learned that so-called normal NNs are also algorithms designed to mimic human neural circuits, but CNNs are also devised using human vision as a model.\n",
        "\n",
        " Human Visual Processing Flow​ ​\n",
        "\n",
        "Light reflected from an object projects an image onto the retina at the back of the eye.\n",
        "Instead of grasping the entire image of an object at once, it recognizes that the image is scanned for each limited area. This limited area is called the \"local receptive field\".\n",
        "A myriad of simple-type cells that recognize certain simple shapes interact with each other to recognize objects of complex shapes.\n",
        "Complex cells, which recognize the entire space, spatially complement the recognition results of simple cells\n",
        " CNN visual processing flow\n",
        "\n",
        "The flow is the same as the flow of human visual processing, with the following correspondences for each.\n",
        "\n",
        "Simple type cell: convolution layer\n",
        "\n",
        "Complex-type cells: Pooling layer\n",
        "\n",
        "Simple and complex cells act in combination with many cells by means of filters that act to expand the folding and pooling layers into multiple cells.\n",
        "\n",
        "This convolution layer, pooling layer, and filter will be explained in detail in the next section."
      ],
      "metadata": {
        "id": "df5hRFN7S4b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link: ImageNet Classification with Deep Convolutional Neural Networks. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 5 pages. Figure 2: An illustration of the architecture of our CNN.\n",
        "\n",
        "The image above is a network of CNNs called Alex net, which won the ImageNet LSVRC-2012 competition, a breakthrough for DeepLearning.\n",
        "\n",
        "Currently, CNN is being developed based on Alex net. The following is a summary of the innovations and the flow of processing.\n",
        "\n",
        "The process proceeds from left to right. First, handwritten characters such as MNIST data are input to the network as input (Input). Since this is image data, it is 2-dimensional data. In reality, the input is a mini-batch of 3-dimensional data, plus one dimension, but this is an example of a single image being input. It may also be plus 1 dimensional or 4 dimensional data with color information.\n",
        "Pass through the convolution layer (Convolution). The output to the next layer is obtained by computing the weights, called kernels, and the input. It is important to note that the kernel is two-dimensional (n × m) and that there are multiple kernels. By preparing multiple kernels, various different features can be extracted from the image. These extracted features are called feature maps.\n",
        "It is passed through a pooling layer (Pooling). Pooling is responsible for extracting representative values from the resulting feature map and correcting misalignment.\n",
        "It repeatedly passes through the convolution layer and the pooling layer, and then inputs the data to a layer called the full concatenation layer (Flatten). The Flatten layer is responsible for converting image data into row data.\n",
        "Since it has been smoothed by all the coupled layers, it is now in a form that can be fed into a normal neural network, and from here, the process is passed through the normal neural network that has been learned so far. For example, if the final output is a classification of \"male or female\", the output layer has only as many nodes as the number of classes, and if it is a regression of \"age prediction\", the output layer has only one node.\n",
        "4. Summary\n",
        " CNN combines convolutional and pooling layers for processing​"
      ],
      "metadata": {
        "id": "wGjoLtI6UepG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolution layer of CNN1**"
      ],
      "metadata": {
        "id": "FW7A2d2_U6KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "purpose\n",
        "Understand how to compute 1D convolutional layers.\n",
        "What is the convolution layer of a neural network?\n",
        "CNN is mainly used in image recognition, and unlike conventional neural networks, it receives 2-dimensional input data. In the following, we will explain using one-dimensional data for ease of understanding.\n",
        "\n",
        "Convolution layer\n",
        "A \"weight\" called a kernel is given to the input values, and the result of the sum-of-products operation is passed to the next layer. The following is an example of calculation when the kernel [-1,2,-1]is applied to the input data [1,5,0,2,8,1]. The final output is [9,-7,-4,13].\n",
        "\n",
        "bias\n",
        "Bias was present in regular neural networks, and it is the same in CNNs.\n",
        "Bias is added to the data after the sum-of-products calculation. The following is a bias term [10] added to the previous calculation. The detailed calculation process is omitted, but the final output will be [19,3,6,23].\n"
      ],
      "metadata": {
        "id": "e515WtyzU8OW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "padding\n",
        "\n",
        "The convolution process seen above has the problem that data at the edges are neglected in the network (fewer operations) and that the size of the data decreases as the layers get deeper.\n",
        "For this reason, convolution processing is sometimes performed by giving pseudo data (e.g., 0) around the input data. This pseudo data is called paddding The example below is a calculation example in which zeros are given at both ends of the data. In this case, one 0 is given at a time, so it is called​ ​0 padding 1If two zeros are given, it is called 0 padding 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "esfKkdCYWVOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stride\n",
        "\n",
        "The interval value at which to apply the filter at what interval is called the stride.\n",
        "The size of the output data varies depending on the stride value. Below is an example of stride 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "AxRkdiLwWb8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning\n",
        "Learning a CNN network involves updating the kernel and bias values. The updating method is similar to that of a regular neural network, as we have already learned.\n",
        "\n",
        "5. Summary\n",
        "The convolution layer changes the shape of the output depending on the number of padding strides"
      ],
      "metadata": {
        "id": "IYipk7pwWu4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pooling layer of CNN1**"
      ],
      "metadata": {
        "id": "R7y5MphnZ6xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "purpose\n",
        "Understand how to compute 1-dimensional pooling layers\n",
        "What is a pooling layer?\n",
        "The process of taking a representative value such as the maximum value is called pooling.\n",
        "Pooling has the following purposes and benefits\n",
        "\n",
        "The model is more resistant to variability and distortion (because we get the main features and filter out all the noise)\n",
        "Dimensions and computational complexity can be reduced.\n",
        "Fewer parameters reduce over-learning (overfitting)\n",
        "In addition to maximum value pooling, there is also average value pooling, etc., but maximum value is the most common. Below is an example with kernel size 1 x 3 and stride 1. Increasing the stride reduces the size of the output."
      ],
      "metadata": {
        "id": "bAojHF2na8il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "Pooling layer is sandwiched after the convolution layer\n",
        "Pushing a pooling layer in between makes for a robust model."
      ],
      "metadata": {
        "id": "yJM6Ow-_bfxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel and channels of CNN1**"
      ],
      "metadata": {
        "id": "spJyd9h8buhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "purpose\n",
        "Understanding the relationship with CNN"
      ],
      "metadata": {
        "id": "E1DRLIyybw0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the Kernel?\n",
        "\n",
        "A kernel is a feature detector.\n",
        "Kernels have been used in convolutional operations, and a number of different types of these kernels can be prepared.\n",
        "In normal image recognition, kernels with various initial values are generated, and these values are updated by training to generate a network that can handle complex problems. The number of kernels is expressed as the number of kernels.\n",
        "\n",
        "Below is an example of operations when kernel 1[-1,2,-1] and kernel 2 [3,-1,2] are applied with stride 1 to input data [1,5,0,2,8,1]. The number of channels in the output depends on the number of kernels."
      ],
      "metadata": {
        "id": "3i56j0EPcR2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a channel?\n",
        "\n",
        "Image data is. It is a three-dimensional composition of length x width x depth.\n",
        "The length x width is easy to imagine since it is just as you see it, but what is the depth?\n",
        "\n",
        "In fact, image data is represented by a combination of three colors, as they are called the three primary colors.\n",
        "When the data is also viewed numerically, it is clear that each image is composed of data for three colors.\n",
        "\n",
        "This three-color configuration is called depth and is generally referred to as a channel.\n",
        "\n",
        "The number of channels 3 is for color images, and the number of channels is 1 for so-called black-and-white images, which are called grayscale.\n",
        "This is because black and white is represented by shades of black, not by two colors, white and black.\n",
        "\n",
        "Imagine, if you will, that there is a difference in the amount of recognition when a human being sees a black and white photo and a color photo. I don't think that just because it is a black and white photo, it does not mean that we cannot recognize buildings or automobiles.\n",
        "\n",
        "The same is true for CNNs, which in most cases use grayscale images (with 1 channel) as input values unless the color is meaningful.\n",
        "The advantage of using grayscale images as input values is not only that there is no difference in recognition accuracy, but also that the process is simpler and requires less computation.\n",
        "\n",
        "Use color images when color is meaningful, for example, when recognizing stained cancer cells.\n",
        "\n",
        "Also, as the layers advance, the number of channels equals the number of kernels."
      ],
      "metadata": {
        "id": "v4yfl6hteJIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Summary\n",
        "The number of kernels represents the number of kernels used in the operation, and the number of channels after the operation is equal to the number of kernels"
      ],
      "metadata": {
        "id": "M86kYJ_Lera2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a convolution layer class for CNN1**\n",
        "\n",
        "purpose\n",
        "Be able to implement 1D convolution classes"
      ],
      "metadata": {
        "id": "7dirC0a4iDEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a 1D convolutional neural network?\n",
        "Let's create the class Convolutional Neural Network (CNN) from scratch. We will implement the algorithm using only the minimum library such as NumPy.\n",
        "\n",
        "In this unit, you will create a 1D convolutional layer to understand the basics of convolution. In the next unit, you will create a two-dimensional convolutional layer and a pooling layer to complete a CNN that is generally used for images.\n",
        "\n",
        "The name of the class is Scratch1dCNNClassifier.\n",
        "\n"
      ],
      "metadata": {
        "id": "N7NtUPA0ifgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a one-dimensional convolutional layer?\n",
        "In CNN, a two-dimensional convolutional layer for images is a standard, but here, to make it easier to understand, we first implement a one-dimensional convolutional layer. One-dimensional convolution is practically often used in Series data such as natural language and waveform data.\n",
        "\n",
        "Convolution can be considered for any dimension, and frameworks generally provide up to 3D convolution for 3D data."
      ],
      "metadata": {
        "id": "6EnkUBHsi54O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall code flow\n",
        "There are various types of activation functions, weight initialization methods, and gradient update methods, which are defined independently as classes in order to use them flexibly for different purposes.\n",
        "Define the [1D convolution [layer] class], which is the main topic of this text.\n",
        "Define [1D convolutional neural network class] using the classes defined in 1 and 2.\n",
        "Instantiate the class created in 3, pass data to the fit function, and learn.\n",
        "Once the study is completed in 4, all that remains is to evaluate & visualize"
      ],
      "metadata": {
        "id": "RwndrJOAjTJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Assumptions\n",
        "Before creating a class for a 1D CNN, we will introduce you to the program you will need, but for a detailed line-by-line explanation, please review and confirm what you have learned so far."
      ],
      "metadata": {
        "id": "pYEqlEYsjbdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Library import"
      ],
      "metadata": {
        "id": "v7fOrjn3jvem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "i9bHJ5BxjwXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of various activation functions"
      ],
      "metadata": {
        "id": "8vvfc_swj2B-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDFk4oXcQGgr"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By giving each activation function the same name of forward function(forward propagation) and backward function (backward propagation), the following image can be easily used. The following program does not work because of the * image"
      ],
      "metadata": {
        "id": "tIvwe-kRj_kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [ReLU(),ReLU(),Tanh(),Softmax()]\n",
        "for layer in layers:\n",
        "    layer.forward()\n",
        "for layer in layers:\n",
        "    layer.backward()"
      ],
      "metadata": {
        "id": "tnuRXiwakGum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put an instance of each activation function class in thelayers variable. Then, simply by using a for statement, retrieve one instance at a time, and call forward(), you can perform forward/backward propagation. What is convenient about this is that when adding a new layer to the layers variable, there is no need to rewrite the entire program, but only to add an instance to the layers variable."
      ],
      "metadata": {
        "id": "KeqGkBhRkTvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of normal layer classes\n",
        "As with the activation function, a normal layer is defined, with forward (forward propagation)andbackward (backward propagation) functions defined."
      ],
      "metadata": {
        "id": "26MIxcuJkZDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X@self.W + self.B\n",
        "        return A\n",
        "    def backward(self, dA):\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ"
      ],
      "metadata": {
        "id": "BAWlGmwUkc-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define weight initialization class\n",
        "\n",
        "Various weights can be initialized in different ways. Depending on the initialization method, initial values are given to the gradient (weights) and bias, calculated according to specific rules. By doing so, the output from the intermediate layer is made unbiased, thereby solving the gradient vanishing problem and the loss of expressive power.\n",
        "\n",
        "Initial value of He (Hu): valid when the activation function of the intermediate layer is Relu\n",
        "\n",
        "Xavier's (Xavier) initial value: valid when the activation function of the middle layer is sigmoid/tanh"
      ],
      "metadata": {
        "id": "ZkIwZbIpkwcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XavierInitializer:\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(1 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class HeInitializer():\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    def B(self, *shape):\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B"
      ],
      "metadata": {
        "id": "nUNwJZGkk1iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of gradient update class\n",
        "Up to now, we have used the steepest descent method for updating the slope, but there are various types. The following two are defined in this article."
      ],
      "metadata": {
        "id": "jbJJKXU6k7ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return\n",
        "\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 1\n",
        "        self.HB = 1\n",
        "    def update(self, layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
        "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB"
      ],
      "metadata": {
        "id": "6-JUXo4YlMEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the mini-batch generation iterator\n",
        "\n",
        "Use the mini-batch iterator created previously.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KPPp__HxlQRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "gFS2Gx7XlRCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1\" Creation of 1D convolutional layer class with the number of channels limited to 1\n",
        "\n",
        "Create a 1D convolutional layer class SimpleConv1d with the number of channels limited to 1.\n",
        "The basic structure is the same as the FC class of the all-combining layer created in the previous NN unit. The class for initialization of weights can be modified as necessary.\n",
        "\n",
        "Here Padding is not considered, and stride is also fixed to 1. Also, don't worry about processing multiple data at the same time, only support batch size 1. Extension of this part is an advanced task.\n",
        "\n",
        "The formula for forward propagation is as follows"
      ],
      "metadata": {
        "id": "kEiMy2Axlx19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The major difference from the fully connected layer is that the weights are shared for multiple features. In this case, the gradient is calculated by adding all the shared errors. For branching on the calculation graph, the error should be added at the time of backpropagation.\n",
        "\n",
        "\n",
        "Programming"
      ],
      "metadata": {
        "id": "Q6mF06z0l80e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConv1d():\n",
        "    def forward(self, x, w, b):\n",
        "        a = []\n",
        "        for i in range(len(w) - 1):\n",
        "            a.append((x[i:i+len(w)] @ w) + b[0])\n",
        "        return np.array(a)\n",
        "    def backward(self, x, w, da):\n",
        "        db = np.sum(da)\n",
        "        dw = []\n",
        "        for i in range(len(w)):\n",
        "            dw.append(da @ x[i:i+len(da)])\n",
        "        dw = np.array(dw)\n",
        "        dx = []\n",
        "        new_w = np.insert(w[::-1], 0, 0)\n",
        "        new_w = np.append(new_w, 0)\n",
        "        for i in range(len(new_w)-1):\n",
        "            dx.append(new_w[i:i+len(da)] @ da)\n",
        "        dx = np.array(dx[::-1])\n",
        "        return db, dw, dx"
      ],
      "metadata": {
        "id": "q3UVrHTml-tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Class definition\n",
        "\n",
        "Line 2: Definition of the forward function. It takes as arguments the explanatory variables x, weight w, and bias b.\n",
        "\n",
        "Line 3: Definition of variableato store the result of the calculation\n",
        "\n",
        "Row 4: Since padding is not considered and stride is fixed at 1, a for statement is used to turn it around.\n",
        "\n",
        "Line 5: Neural network calculation, summing input values and weights and adding bias. The calculation part of $a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b$. The part of x[i:i+len(w)] @ w corresponds to $\\sum_{s=0}^{F-1}x_{(i+s)}w_s$.\n",
        "\n",
        "Line 6: Return value is returned.\n",
        "\n",
        "Line 7: Definition of the backward function. It takes as arguments the input array x and the backward propagation value da.\n",
        "\n",
        "Line 8: The slope of the bias is calculated. The calculation part of $\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}$.\n",
        "\n",
        "Line 9: Array for storing the gradient of the weights is prepared. Array for storing the results of $\\frac{\\partial L}{\\partial w_s}$ calculations.\n",
        "\n",
        "Lines 10-11: The gradient of the weights is calculated and added to dw. $\\frac{\\partial L}{\\partial w_s}=\\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}$ of $\\sum_{i=0}^{N_{out}-1} L}{\\partial a_i}x_{(i+s)}$ corresponds to da @ x[i:i+len(da)​\n",
        "\n",
        "Line 12: Although handled in list form, numpy arrays of dw are performed for various matrix calculations.\n",
        "\n",
        "Line 13: Calculation of the input after back-propagation is performed. Definition of the array for storing the calculation results of $\\frac{\\partial L}{\\partial x_j}$.\n",
        "\n",
        "Lines 14-15: w is inverted. Also, w does not fit the size as it is, so 0 is added to both sides.\n",
        "\n",
        "Lines 16-17: turning in the for statement and calculating $\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s$.\n",
        "\n",
        "Line 18: The result of the calculation is inverted and converted to a numpy array.\n",
        "\n",
        "Line 19: Return value is returned."
      ],
      "metadata": {
        "id": "bJH1dn94mPHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Problem 2\" Calculation of output size after 1D convolution\n",
        "You have seen that the data size after passing through the convolution layer changes with the input and output values of the data, and the formula for the output size when padding and stride are taken into account is as follows."
      ],
      "metadata": {
        "id": "yTl-G1LemVC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Programming"
      ],
      "metadata": {
        "id": "QXE9TF7umYXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_size_calculation(n_in, F, P=0, S=1):\n",
        "    n_out = int((n_in + 2*P - F) / S + 1)\n",
        "\n",
        "    return n_out"
      ],
      "metadata": {
        "id": "_hwBsh5JmQw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. It takes as arguments the input size n_in, the filter size F, the number of paddings P, and the number of strides S.\n",
        "\n",
        "Line 2: Calculation of output size. The calculation part of $N_{out} = \\frac{N_{in}+2P-F}{S} + 1$. The output size is calculated using each argument in accordance with the calculation formula.\n",
        "\n",
        "Line 3: The return value is returned.\n",
        "\n",
        "Summary\n",
        "\n",
        "\n",
        "We implemented a simple convolutional layer class\n",
        "The calculation of the output size was implemented. Without accurate calculation of the output size, the input array and weights cannot be calculated in the network calculation process."
      ],
      "metadata": {
        "id": "FrBQ3B1-mq6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confirmation of operation and expansion of created CNN1**"
      ],
      "metadata": {
        "id": "OLQ3kQNanHFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "purpose\n",
        "Running the created 1D convolution layer class\n",
        "Check the operation of the classes you have implemented so far."
      ],
      "metadata": {
        "id": "T3aTxSwHsgwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Problem 3\"\n",
        "\n",
        "Experiments on 1D convolutional layers with small arrays\n",
        "We will check for correct forward and back propagation in the small array shown next."
      ],
      "metadata": {
        "id": "rnYSqfYKsope"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([1,2,3,4])\n",
        "w = np.array([3, 5, 7])\n",
        "b = np.array([1])"
      ],
      "metadata": {
        "id": "KtSZImw6srOq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Definition of explanatory variable x\n",
        "\n",
        "Line 2: Definition of weight w\n",
        "\n",
        "Line 3: Definition of bias b"
      ],
      "metadata": {
        "id": "2GdRutRQs4a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_conv_1d = SimpleConv1d()\n",
        "simple_conv_1d.forward(x, w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "5ypLMOGps95_",
        "outputId": "49508f76-1cd6-4dda-d1f9-fb0aa815d5e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SimpleConv1d.__init__() missing 4 required positional arguments: 'num_filters', 'kernel_size', 'initializer', and 'optimizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7f34129c7ccb>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msimple_conv_1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleConv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msimple_conv_1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SimpleConv1d.__init__() missing 4 required positional arguments: 'num_filters', 'kernel_size', 'initializer', and 'optimizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Instantiation of 1D convolution class\n",
        "\n",
        "Line 2: Execution of forward propagation process\n",
        "\n",
        "The results of the above run are shown below."
      ],
      "metadata": {
        "id": "jmSEpQlluRDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's give the correct data to the above output results and perform the error back propagation process."
      ],
      "metadata": {
        "id": "d6WqZJy1ua1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([35, 50])\n",
        "a_actual = np.array([45, 70])\n",
        "da = np.array([10, 20])\n",
        "db, dw, dx = simple_conv_1d.backward(x, w, da)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "P007eVNrugpj",
        "outputId": "823da963-7b7e-404a-8ebf-6d9f4f4bcd1d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'simple_conv_1d' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3f567d458230>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_conv_1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'simple_conv_1d' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Value of the output of the forward propagation process\n",
        "\n",
        "Line 2: Correct data\n",
        "\n",
        "Line 3: Error between rows 1 and 2\n",
        "\n",
        "Line 4: Execution of back propagation process\n",
        "\n",
        "The results of executing the above program are shown below."
      ],
      "metadata": {
        "id": "QJAfyOaHujsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = np.array([30])\n",
        "dw = np.array([50, 80, 110])\n",
        "dx = np.array([30, 110, 170, 140])"
      ],
      "metadata": {
        "id": "Rbfv2bssurXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Problem 4\" Creation of 1D convolutional layer class without limiting the number of channels.\n",
        "\n",
        " First, define the constructor."
      ],
      "metadata": {
        "id": "3dId71vpuozW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1d:\n",
        "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0):\n",
        "        self.b_size = b_size\n",
        "        self.optimizer = optimizer\n",
        "        self.pa = pa\n",
        "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
        "        self.B = initializer.B(n_out_channels)\n",
        "        self.n_in_channels = n_in_channels\n",
        "        self.n_out_channels = n_out_channels\n",
        "        self.n_out = None"
      ],
      "metadata": {
        "id": "Jfus0QJgu3Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Class definition\n",
        "\n",
        "Line 2: Constructor definition. As arguments, filter size b_size, initialization method instance initializer, optimization method instance optimizer, number of input channels n_in_channels, number of output channels n_out_channels=1, number of padding pa\n",
        "\n",
        "Lines 3 to 10: Definition of various member variables. In particular, lines 6 and 7 initialize weights and biases using an instance initializer of the initialization method received as an argument."
      ],
      "metadata": {
        "id": "geL199TNu_xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Next, define the function for the forward propagation process."
      ],
      "metadata": {
        "id": "hM-F8gZtvKXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, X):\n",
        "    self.n_samples = X.shape[0]\n",
        "    self.n_in = X.shape[-1]\n",
        "    self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa, self.stride)\n",
        "    X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n",
        "    self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\n",
        "    self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
        "    for i in range(self.b_size):\n",
        "        self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
        "    A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
        "    return A"
      ],
      "metadata": {
        "id": "z2IYd4x5vL7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. Accepts input array X as argument.\n",
        "\n",
        "Line 2: Definition of sample size\n",
        "\n",
        "Line 3: Definition of the number of features in the input array X\n",
        "\n",
        "Line 4: Calculation of output size\n",
        "\n",
        "Line 5: reshape input array X\n",
        "\n",
        "Line 6: 0 filled in implementation\n",
        "\n",
        "Line 7: Prepare zero array X1 for calculation of output array (A)\n",
        "\n",
        "Line 8: Loop with filter size\n",
        "\n",
        "Line 9: Overwrite while shifting X1\n",
        "\n",
        "Line 10: Calculation of output A of forward propagation\n",
        "\n",
        "Line 11: Return the result of the calculation of line 10."
      ],
      "metadata": {
        "id": "8v0YDDLkvZ-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Next, define the function for the back propagation process.​"
      ],
      "metadata": {
        "id": "jEjXbttyvcXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(self, dA):\n",
        "    self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n",
        "    self.dB = np.sum(dA, axis=(0, -1))\n",
        "    self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n",
        "    self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
        "    for i in range(self.b_size):\n",
        "        self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
        "    dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
        "    self.optimizer.update(self)\n",
        "    return dX"
      ],
      "metadata": {
        "id": "RStHzVxuvbpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. Receives the error (back-propagated value) as an argument.\n",
        "\n",
        "Line 2: Gradient calculation of weights\n",
        "\n",
        "Line 3: Bias slope calculation\n",
        "\n",
        "Line 4: 0 filled in implementation\n",
        "\n",
        "Line 5: Prepare zero array dA1 for calculation of output array (dX)\n",
        "\n",
        "Line 6: Loop with filter size\n",
        "\n",
        "Line 7: Overwrite dA1 while shifting\n",
        "\n",
        "Line 8: Calculation of output dX for back propagation\n",
        "\n",
        "Line 9: This instance itself is passed to theupdatefunction of the optimization method\n",
        "\n",
        "Line 10: Returns the result of the calculation of Line 9."
      ],
      "metadata": {
        "id": "0IyJFg1lvldu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Summary\n",
        "\n",
        "One-dimensional convolutional neural network was tested and extended."
      ],
      "metadata": {
        "id": "EfirLrdFvrQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN1 training and estimation**\n"
      ],
      "metadata": {
        "id": "QKOa0JW8xjRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "purpose\n",
        "Create a class for learning"
      ],
      "metadata": {
        "id": "x7RQqYCPxn6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Problem 8\" Learning and Estimation\n",
        "Let's create a class for training by combining the 1D convolutional neural network and layers of activation functions created in the previous sections."
      ],
      "metadata": {
        "id": "Iv4JN53gxymm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, define the constructor"
      ],
      "metadata": {
        "id": "1fNwrgppx3p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchCNNClassifier:\n",
        "    def __init__(self, num_epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, verbose=True, Activater=Tanh, Optimizer=AdaGrad):\n",
        "        self.num_epoch = num_epoch\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose\n",
        "        self.batch_size = batch_size\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.Activater = Activater\n",
        "        if Activater == Sigmoid or Activater == Tanh:\n",
        "            self.Initializer = XavierInitializer\n",
        "        elif Activater == ReLU:\n",
        "            self.Initializer = HeInitializer\n",
        "        self.Optimizer = Optimizer"
      ],
      "metadata": {
        "id": "4e3e9zWSx4aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Class definition\n",
        "\n",
        "Line 2: Constructor definition. The arguments are: number of training times num_epoch, learning rate lr, batch size batch_size, number of features n_features, number of nodes in the first layer n_nodes1, number of nodes in the second layer n_nodes2, number of output layers n_output, whether to output assumptions verbose, activator function Activator ・Receives optimization method Optimizer\n",
        "\n",
        "Lines 3~15: Definition of member variables. In particular, lines 11~14 change the initialization method of the weights according to the activation function used in order to improve accuracy."
      ],
      "metadata": {
        "id": "0a7gwbdpyCXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Defines a forward propagation process"
      ],
      "metadata": {
        "id": "JiZ9l5i9yDxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(self, X):\n",
        "    A1 = self.Conv1d.forward(X)\n",
        "    A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
        "    Z1 = self.activation1.forward(A1)\n",
        "    A2 = self.FC2.forward(Z1)\n",
        "    Z2 = self.activation2.forward(A2)\n",
        "    A3 = self.FC3.forward(Z2)\n",
        "    Z3 = self.activation3.forward(A3)\n",
        "    return Z3"
      ],
      "metadata": {
        "id": "RFVvrXZfyH5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "self.Conv1D・self.activation1・self.FC2・self.activation2・self.FC3・self.activation3are defined in the fit function, respectively\n",
        "\n",
        "Line 1: Function definition. It takes an input array X as an argument.\n",
        "\n",
        "Line 2: Forward propagation process of convolutional layer\n",
        "\n",
        "Line 3: reshape A1 (reduce dimension)\n",
        "\n",
        "Line 4: Forward propagation process of activation function\n",
        "\n",
        "Line 5: Forward propagation process for all coupling layers\n",
        "\n",
        "Line 6: Forward propagation process of activation function\n",
        "\n",
        "Line 7: Forward propagation process for all coupling layers\n",
        "\n",
        "Line 8: Forward propagation process of activation function\n",
        "\n",
        "Line 9: Return forward propagation result"
      ],
      "metadata": {
        "id": "unT4WXkgyQif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Defines the back propagation process"
      ],
      "metadata": {
        "id": "-u8dS2EvyZhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propagation(self,y_true):\n",
        "    dA3 = self.activation3.backward(y_true)\n",
        "    dZ2 = self.FC3.backward(dA3)\n",
        "    dA2 = self.activation2.backward(dZ2)\n",
        "    dZ1 = self.FC2.backward(dA2)\n",
        "    dA1 = self.activation1.backward(dZ1)\n",
        "    dA1 = dA1[:, np.newaxis]\n",
        "    dZ0 = self.Conv1d.backward(dA1)"
      ],
      "metadata": {
        "id": "3l5xp1SCyf7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. Receives the correct answer data y_true as an argument.\n",
        "\n",
        "Line 2: Back propagation of the activation function, comparing the output of forward propagation with the output of forward propagation stored in the Softmax class, and determining the error.\n",
        "\n",
        "Line 3: Back propagation process for all coupling layers\n",
        "\n",
        "Line 4: Back propagation process of activation function\n",
        "\n",
        "Line 5: Back propagation process for all coupling layers\n",
        "\n",
        "Line 6: Back propagation process of activation function\n",
        "\n",
        "Line 7: Increase dimension\n",
        "\n",
        "Line 8: Back propagation process of convolutional layer"
      ],
      "metadata": {
        "id": "cIPLLn9kyn4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Defines a function for outputting predictions​ ​"
      ],
      "metadata": {
        "id": "5UPX-sXSyv7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "    return np.argmax(self.forward_propagation(X), axis=1)"
      ],
      "metadata": {
        "id": "dIBxftTYy3XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. It takes an input array X as an argument.\n",
        "\n",
        "Line 2: The input variable X is passed to the forward propagation process, and the corresponding class is obtained from the result using np.argmax."
      ],
      "metadata": {
        "id": "fHKEemPTy7NO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Define a function to run the training"
      ],
      "metadata": {
        "id": "3I7ODiCoy-Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self, X, y, X_val=None, y_val=None):\n",
        "    self.Conv1d = Conv1d(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=1, stride=1)\n",
        "    self.Conv1d.n_out = output_size_calculation(X.shape[-1], self.Conv1d.b_size, self.Conv1d.pa, self.Conv1d.stride)\n",
        "    self.activation1 = self.Activater()\n",
        "    self.FC2 = FC(1*self.Conv1d.n_out, self.n_nodes2, self.Initializer(), self.Optimizer(self.lr))\n",
        "    self.activation2 = self.Activater()\n",
        "    self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(), self.Optimizer(self.lr))\n",
        "    self.activation3 = Softmax()\n",
        "    self.loss = []\n",
        "    self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\n",
        "    for _ in range(self.num_epoch):\n",
        "        get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "        for mini_X, mini_y in get_mini_batch:\n",
        "            self.forward_propagation(mini_X)\n",
        "            self.back_propagation(mini_y)\n",
        "            self.loss.append(self.activation3.loss)\n",
        "        self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))"
      ],
      "metadata": {
        "id": "OFprJwrZzDfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Function definition. Receives as arguments the explanatory variable X for the training data, the objective variable y for the training data, the explanatory variable X for the evaluation data, and the objective variable y for the evaluation data.\n",
        "\n",
        "Line 2: Instantiation of the convolution layer.\n",
        "\n",
        "Line 3: Calculation of convolution layer output size\n",
        "\n",
        "Line 4: Create an instance of the activation function\n",
        "\n",
        "Line 5: Creation of an instance of all binding layers\n",
        "\n",
        "Line 6: Create an instance of the activation function\n",
        "\n",
        "Line 7: Creation of an instance of all binding layers\n",
        "\n",
        "Line 8: Instantiation of the output layer activation function Softmax\n",
        "\n",
        "Line 9: Definition of variables for loss recording (per mini-batch)\n",
        "\n",
        "Line 10: Definition of variables for loss recording (per epoch)\n",
        "\n",
        "Line 11: Loop by number of studies\n",
        "\n",
        "Line 12: Create mini-batch iterator\n",
        "\n",
        "Line 13: Loop with iterator\n",
        "\n",
        "Line 14: Execution of forward propagation function\n",
        "\n",
        "Line 15: Execution of back propagation function\n",
        "\n",
        "Line 16: Loss storage (per mini-batch)\n",
        "\n",
        "Line 17: Loss storage (per epoch)"
      ],
      "metadata": {
        "id": "je3Zf-6XzKY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning\n",
        "The convolutional neural network class you created is used to train the network."
      ],
      "metadata": {
        "id": "56bVUBVpzdyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = ScratchCNNClassifier(num_epoch=1, lr=0.01, batch_size=1, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, verbose=True, Activater=Sigmoid, Optimizer=SGD)\n",
        "cnn.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "DAk5KeJZzkG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Create an instance of the neural network class by giving various arguments.\n",
        "\n",
        "Line 2: Give explanatory and objective variables and run the study."
      ],
      "metadata": {
        "id": "ZlCX0lswzlcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict\n",
        "Calculate predictions and the percentage of correct answers (acc) using a trained convolutional neural network model"
      ],
      "metadata": {
        "id": "jfBz4D9rzt0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cnn.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "gmryGSNMzo2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Execution of thepredictfunction of the trained convolutional neural network instance cnn. Pass the explanatory variable of the test data as an argument. The return value is stored in y_pred.\n",
        "\n",
        "Line 2: Using the predicted value y_predobtained above and the objective variable y_testof the test data, calculate theaccuracy_score of skleran for the percentage of correct answers acc\n",
        "\n",
        " Summary\n",
        "\n",
        "\n",
        "Using the implemented convolutional layer, training and prediction of 1D convolutional neural network was implemented"
      ],
      "metadata": {
        "id": "9EhLuRb8z79u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SimpleConv1d**"
      ],
      "metadata": {
        "id": "pzRCnjzr9p8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this Sprint\n",
        "\n",
        "Understanding the basics of CNNs through Scratch\n",
        "\n",
        "How to learn\n",
        "\n",
        "After implementing a one-dimensional convolutional neural network from scratch, we will start training and validating it."
      ],
      "metadata": {
        "id": "ZaQXSvv39r3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-dimensional convolutional neural network scratch\n",
        "\n",
        "Let's create the class Convolutional Neural Network (CNN) from scratch. We will implement the algorithm using only the minimum library such as NumPy.\n",
        "\n",
        "In this Sprint, we will build a 1D Convolutional layer and try to understand the basics of convolution. The next Sprint completes the CNN commonly used for images by creating a two-dimensional convolutional layer and a pooling layer.\n",
        "\n",
        "Name the class Scratch1dCNNClassifier. Please refer to the ScratchDeepNeuralNetrowkClassifier created in the previous Sprint for the class structure."
      ],
      "metadata": {
        "id": "NVk6xE1x9_ZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the implementation of the Scratch1dCNNClassifier class, a 1D Convolutional Neural Network (CNN) built from scratch using only NumPy. This implementation includes an initializer, optimizer, and the convolutional layer, as well as forward and backward propagation methods.\n",
        "\n",
        "Class Definitions\n",
        "Initializer, Optimizer, and Convolutional Layer\n",
        "First, we define the SimpleInitializer, SGD, and Conv1D classes to handle initialization, optimization, and the 1D convolutional layer, respectively."
      ],
      "metadata": {
        "id": "VXZ2P5gb_wdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleInitializer:\n",
        "    def initialize(self, shape):\n",
        "        return np.random.randn(*shape) * 0.01\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, W, B, dW, dB):\n",
        "        W -= self.learning_rate * dW\n",
        "        B -= self.learning_rate * dB\n",
        "        return W, B\n",
        "\n",
        "class Conv1D:\n",
        "    def __init__(self, num_filters, kernel_size, initializer, optimizer):\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.W = initializer.initialize((num_filters, kernel_size))\n",
        "        self.B = np.zeros((num_filters, 1))  # Bias for each filter\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        batch_size, input_size = X.shape\n",
        "        output_size = input_size - self.kernel_size + 1\n",
        "        A = np.zeros((batch_size, output_size, self.num_filters))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for j in range(output_size):\n",
        "                receptive_field = X[i, j:j+self.kernel_size]\n",
        "                A[i, j] = np.dot(receptive_field, self.W.T) + self.B.T\n",
        "\n",
        "        # Flatten the output\n",
        "        A_flat = A.reshape(batch_size, -1)\n",
        "        return A_flat\n",
        "\n",
        "    def backward(self, dA):\n",
        "        batch_size, dA_flat_shape = dA.shape\n",
        "        output_size = dA_flat_shape // self.num_filters\n",
        "        dA = dA.reshape(batch_size, output_size, self.num_filters)\n",
        "\n",
        "        dX = np.zeros_like(self.X)\n",
        "        dW = np.zeros_like(self.W)\n",
        "        dB = np.zeros_like(self.B)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for j in range(output_size):\n",
        "                receptive_field = self.X[i, j:j+self.kernel_size]\n",
        "                dX[i, j:j+self.kernel_size] += np.dot(dA[i, j], self.W)\n",
        "                dW += np.outer(dA[i, j], receptive_field)\n",
        "\n",
        "        dB = np.sum(dA, axis=(0, 1)).reshape(self.num_filters, 1)\n",
        "\n",
        "        self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB)\n",
        "\n",
        "        return dX\n",
        "\n",
        "class Scratch1dCNNClassifier:\n",
        "    def __init__(self, num_filters, kernel_size, learning_rate=0.01):\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.initializer = SimpleInitializer()\n",
        "        self.optimizer = SGD(learning_rate=self.learning_rate)\n",
        "        self.conv1d = Conv1D(num_filters, kernel_size, self.initializer, self.optimizer)\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=1000, verbose=False):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.conv1d.forward(X_train)\n",
        "\n",
        "            # Compute loss (simple mean squared error)\n",
        "            loss = np.mean((output - y_train) ** 2)\n",
        "\n",
        "            # Compute gradient of the loss w.r.t. output\n",
        "            dA = 2 * (output - y_train) / y_train.shape[0]\n",
        "\n",
        "            # Backward pass\n",
        "            self.conv1d.backward(dA)\n",
        "\n",
        "            if verbose and epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.conv1d.forward(X)\n",
        "\n",
        "# Example data\n",
        "X_train = np.random.randn(10, 5)  # 10 samples, 5 features\n",
        "y_train = np.random.randn(10, 12)  # Adjusting target shape to match flattened output\n",
        "\n",
        "# Initialize the model\n",
        "num_filters = 3\n",
        "kernel_size = 2\n",
        "model = Scratch1dCNNClassifier(num_filters, kernel_size, learning_rate=0.01)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=1000, verbose=True)\n",
        "\n",
        "# Predict on new data\n",
        "X_new = np.random.randn(5, 5)  # 5 new samples, 5 features\n",
        "predictions = model.predict(X_new)\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Uv-rb-DRrO",
        "outputId": "df5898d4-1d8f-4112-cb97-d08a25bbd616"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.1369510058700076\n",
            "Epoch 100, Loss: 1.0526878787837732\n",
            "Epoch 200, Loss: 1.0526878707705767\n",
            "Epoch 300, Loss: 1.052687870770575\n",
            "Epoch 400, Loss: 1.052687870770575\n",
            "Epoch 500, Loss: 1.052687870770575\n",
            "Epoch 600, Loss: 1.052687870770575\n",
            "Epoch 700, Loss: 1.052687870770575\n",
            "Epoch 800, Loss: 1.052687870770575\n",
            "Epoch 900, Loss: 1.052687870770575\n",
            "Predictions:\n",
            "[[-0.12571056 -0.17592554  0.09056509 -0.3693332   0.0349213  -0.03991471\n",
            "  -0.34648972  0.27568382 -0.12925865 -0.15263923 -0.16053348  0.07922807]\n",
            " [-0.03141589 -0.01093942  0.04492331  0.20918891 -0.54269021  0.2999216\n",
            "   0.02957824 -0.28678976  0.16455977 -0.01560734 -0.23696315  0.13617958]\n",
            " [-0.47550885  0.21706912 -0.13197033 -0.27652274  0.2391381  -0.10114614\n",
            "   0.01798647 -0.30458681  0.16920174 -0.04875261 -0.17824921  0.10672009]\n",
            " [ 0.08220354 -0.21695504  0.14775966  0.1065339  -0.48146943  0.25571154\n",
            "  -0.16447972 -0.09919479  0.05296671 -0.12124495 -0.04432517  0.04014059]\n",
            " [ 0.29583114 -0.42166054  0.26990158  0.3291922  -0.72068164  0.39309661\n",
            "   0.16700069 -0.33495324  0.21056804  0.04527721 -0.48988035  0.24685298]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To measure the accuracy and show the parameters of the Scratch1dCNNClassifier, we need to add a few modifications. We'll add an accuracy function and a method to print the model parameters."
      ],
      "metadata": {
        "id": "Xetf46H3EXYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleInitializer:\n",
        "    def initialize(self, shape):\n",
        "        return np.random.randn(*shape) * 0.01\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, W, B, dW, dB):\n",
        "        W -= self.learning_rate * dW\n",
        "        B -= self.learning_rate * dB\n",
        "        return W, B\n",
        "\n",
        "class Conv1D:\n",
        "    def __init__(self, num_filters, kernel_size, initializer, optimizer):\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.W = initializer.initialize((num_filters, kernel_size))\n",
        "        self.B = np.zeros((num_filters, 1))  # Bias for each filter\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        batch_size, input_size = X.shape\n",
        "        output_size = input_size - self.kernel_size + 1\n",
        "        A = np.zeros((batch_size, output_size, self.num_filters))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for j in range(output_size):\n",
        "                receptive_field = X[i, j:j+self.kernel_size]\n",
        "                A[i, j] = np.dot(receptive_field, self.W.T) + self.B.T\n",
        "\n",
        "        # Flatten the output\n",
        "        A_flat = A.reshape(batch_size, -1)\n",
        "        return A_flat\n",
        "\n",
        "    def backward(self, dA):\n",
        "        batch_size, dA_flat_shape = dA.shape\n",
        "        output_size = dA_flat_shape // self.num_filters\n",
        "        dA = dA.reshape(batch_size, output_size, self.num_filters)\n",
        "\n",
        "        dX = np.zeros_like(self.X)\n",
        "        dW = np.zeros_like(self.W)\n",
        "        dB = np.zeros_like(self.B)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for j in range(output_size):\n",
        "                receptive_field = self.X[i, j:j+self.kernel_size]\n",
        "                dX[i, j:j+self.kernel_size] += np.dot(dA[i, j], self.W)\n",
        "                dW += np.outer(dA[i, j], receptive_field)\n",
        "\n",
        "        dB = np.sum(dA, axis=(0, 1)).reshape(self.num_filters, 1)\n",
        "\n",
        "        self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB)\n",
        "\n",
        "        return dX\n",
        "\n",
        "class Scratch1dCNNClassifier:\n",
        "    def __init__(self, num_filters, kernel_size, learning_rate=0.01):\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.initializer = SimpleInitializer()\n",
        "        self.optimizer = SGD(learning_rate=self.learning_rate)\n",
        "        self.conv1d = Conv1D(num_filters, kernel_size, self.initializer, self.optimizer)\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=1000, verbose=False):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.conv1d.forward(X_train)\n",
        "\n",
        "            # Compute loss (simple mean squared error)\n",
        "            loss = np.mean((output - y_train) ** 2)\n",
        "\n",
        "            # Compute gradient of the loss w.r.t. output\n",
        "            dA = 2 * (output - y_train) / y_train.shape[0]\n",
        "\n",
        "            # Backward pass\n",
        "            self.conv1d.backward(dA)\n",
        "\n",
        "            if verbose and epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.conv1d.forward(X)\n",
        "\n",
        "    def get_params(self):\n",
        "        return self.conv1d.W, self.conv1d.B\n",
        "\n",
        "    def accuracy(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        predicted_labels = np.argmax(predictions, axis=1)\n",
        "        true_labels = np.argmax(y, axis=1)\n",
        "        accuracy = np.mean(predicted_labels == true_labels)\n",
        "        return accuracy\n",
        "\n",
        "# Example data\n",
        "X_train = np.random.randn(10, 5)  # 10 samples, 5 features\n",
        "y_train = np.random.randn(10, 12)  # Adjusting target shape to match flattened output\n",
        "\n",
        "# Initialize the model\n",
        "num_filters = 3\n",
        "kernel_size = 2\n",
        "model = Scratch1dCNNClassifier(num_filters, kernel_size, learning_rate=0.01)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=1000, verbose=True)\n",
        "\n",
        "# Measure accuracy on training data\n",
        "accuracy = model.accuracy(X_train, y_train)\n",
        "print(f\"Training accuracy: {accuracy}\")\n",
        "\n",
        "# Show model parameters\n",
        "W, B = model.get_params()\n",
        "print(\"Weight matrix (W):\")\n",
        "print(W)\n",
        "print(\"\\nBias matrix (B):\")\n",
        "print(B)\n",
        "\n",
        "# Predict on new data\n",
        "X_new = np.random.randn(5, 5)  # 5 new samples, 5 features\n",
        "predictions = model.predict(X_new)\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAbsE_nDEQ5O",
        "outputId": "89590383-49bf-4d0d-d10c-886881aeee9b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0284621944745322\n",
            "Epoch 100, Loss: 0.9839221358786181\n",
            "Epoch 200, Loss: 0.983922124984558\n",
            "Epoch 300, Loss: 0.983922124984549\n",
            "Epoch 400, Loss: 0.9839221249845491\n",
            "Epoch 500, Loss: 0.9839221249845489\n",
            "Epoch 600, Loss: 0.9839221249845489\n",
            "Epoch 700, Loss: 0.9839221249845489\n",
            "Epoch 800, Loss: 0.9839221249845489\n",
            "Epoch 900, Loss: 0.9839221249845489\n",
            "Training accuracy: 0.2\n",
            "Weight matrix (W):\n",
            "[[ 0.19187743  0.17904298]\n",
            " [ 0.00612825 -0.01476933]\n",
            " [-0.02262517 -0.17572612]]\n",
            "\n",
            "Bias matrix (B):\n",
            "[[0.0393246 ]\n",
            " [0.04424347]\n",
            " [0.15128696]]\n",
            "Predictions:\n",
            "[[ 3.96796346e-02  4.75082502e-02  1.75797807e-01 -1.78386094e-01\n",
            "   5.87158697e-02  3.38651727e-01 -2.05963013e-01  4.15157677e-02\n",
            "   2.18746260e-01  3.95674436e-01  9.37560604e-03 -2.39760035e-01]\n",
            " [ 4.46684628e-02  1.61924474e-02 -6.23239166e-02  3.75656963e-01\n",
            "   4.67440098e-02  4.94333011e-02  6.82364301e-01  3.09892304e-05\n",
            "  -4.13186646e-01  8.96351163e-01  4.29386205e-02 -1.66184843e-01]\n",
            " [ 1.41623003e-01  3.97259919e-02  8.04754772e-02  3.98207743e-01\n",
            "   2.29818124e-02 -1.37987374e-01  5.83466535e-01  3.44265309e-02\n",
            "  -1.18115013e-01  1.78650070e-01  6.18958995e-02  2.34494511e-01]\n",
            " [-1.05726713e-01  9.24679717e-02  5.67288123e-01 -3.84280686e-01\n",
            "   2.25406646e-02  1.39552863e-01  2.94629357e-01  3.19427434e-02\n",
            "  -3.31830728e-02  1.73168406e-01  5.51236562e-02  1.85354022e-01]\n",
            " [-1.86918170e-01  4.96437137e-02  2.73249412e-01  1.10975842e-01\n",
            "   2.48017920e-02 -2.11523929e-02  4.11210143e-01  3.68542749e-02\n",
            "  -3.79631730e-02  2.80447174e-01  4.50008808e-02  7.04533998e-02]]\n"
          ]
        }
      ]
    }
  ]
}