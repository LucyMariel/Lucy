{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2IqaNP4vUTsTI371DSfBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyMariel/Lucy/blob/master/SegmentationSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "purpose\n",
        "Gain expertise in running publicly available implementations\n",
        "Try to run the method you read about in the paper.\n",
        "How to learn\n",
        "\n",
        "How to learn\n",
        "I will explain along the flow of \"Issue Faster R-CNN and YOLO v3\".\n",
        "\n",
        "Faster R-CNN is an object detection model that improves on Fast R-CNN by utilising a region proposal network (RPN) with the CNN model. The RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals.\n",
        "\n",
        "Link: Faster R-CNN: Towards Real-Time Object\n",
        "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 3page Figure 2: Faster R-CNN is a single, unified networkfor object detection. The RPN module serves as the 'attention' of this unified network.\n",
        "\n",
        "First, Faster R-CNN has a two-stage structure as a whole.\n",
        "\n",
        "We use two networks, \"a network that detects the position of an object\" and \"a network that determines what the object is\".\n",
        "\n",
        "As a flow, the output from the \"network that detects the position where the object is reflected\" is input to the \"network that determines what the object is\".\n",
        "\n",
        "Here, in particular, the \"network that detects the position of an object\" is called the Region Proposal Network (RPN).\n",
        "\n",
        "Bounding Box\n",
        "\n",
        "The rectangular area (rectangle) to which the object of the correct answer data is mainly transferred is called a Bounding Box or B Box.\n",
        "\n"
      ],
      "metadata": {
        "id": "A5v6pSw02ofl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Network to detect the position of an object (RPN)\"**\n",
        "anchor box\n",
        "\n",
        "First of all, if you give various values the center position and the length and width in the image and create various quadrilaterals, I think that the object will just fit in some square. This is called an anchor box.\n",
        "\n",
        "Object detection networks such as Faster R-CNN have a special configuration.\n",
        "\n",
        "In Faster R-CNN, an arbitrary number of anchor boxes are generated, a loss is generated, and each value of the anchor box is learned so that the loss is minimized.\n",
        "\n",
        "Loss is the degree of overlap and the object or background.\n",
        "\n",
        "network structure\n",
        "\n",
        "Create a feature map through a large CNN model (VGG16, etc.)\n",
        "Generate a network that inputs the feature map of 1 → Is the output an object or a background?\n",
        "Generate a network that inputs the feature map of 1 → Outputs overlap\n",
        "Learn a few networks\n",
        "It becomes a network of 1 input and 2 outputs."
      ],
      "metadata": {
        "id": "8vUh0LB14HJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Network to judge what the object is\"**\n",
        "network structure\n",
        "\n",
        "Acquire the feature map used in RPN (diverted)\n",
        "ROI Pooling, an algorithm that converts the input to a fixed length, is applied to convert it to a fixed length regardless of the size of the input feature map (it does not depend on the input image size).\n",
        "Generate a network that takes the output of 2 as an input → For class classification\n",
        "Generate a network that takes the output of 2 as an input → For rectangular deviation regression\n",
        "Learn each of the 3 and 4 networks\n",
        "It becomes a network of 1 input and 2 outputs."
      ],
      "metadata": {
        "id": "lYNpCb0i49ZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROI Pooling\n",
        "\n",
        "For example, in terms of pixels, it is a function to change the delimiter position well so that it becomes 7x7, for example, for both 10x10 BBox and 10x20 BBox.\n",
        "\n",
        "Also, when the BBox of the original image is projected on the feature map, the size has changed, so there will be an error at the pixel level, but this error can also be corrected well."
      ],
      "metadata": {
        "id": "lmRroLDq7WdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faster R-CNN\n",
        "\n",
        "A network that combines the above \"network that detects the position of an object (RPN)\" and \"network that determines what the object is\" is called Faster R-CNN."
      ],
      "metadata": {
        "id": "Q-p8HpLT7fMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "Faster R-CNN consists of two networks\n",
        "Each network plays an important role in the overall architecture"
      ],
      "metadata": {
        "id": "iSpZtNJw7mwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running**"
      ],
      "metadata": {
        "id": "Wg8Y9MWL8yCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Faster R-CNN README and run the implementation\n",
        "2. \"Problem 1\" learning and estimation\n",
        "If you get an error, the most suspicious is the version issue of TensorFlow / Keras and dependent libraries. For example, TensorFlow / Keras often gives an error and does not work properly if the version of NumPy is different. If you get an error, calm down and read the error content carefully, and if you don't understand it, investigate the error content (by doing a Google search etc.) and find a solution.\n",
        "\n",
        "Now let's take a look at the steps to get the published code working.\n",
        "\n",
        "Data download\n",
        "Download the data from the Kaggle page.\n",
        "\n",
        "Program preparation\n",
        "Download the complete program from GitHub.\n",
        "\n",
        "The folder obtained above is called the project folder.\n",
        "\n",
        "Data preparation\n",
        "- Unzip the-simpsons-characters-dataset.zip downloaded from Kaggle.\n",
        "- Unzip simpsons_dataset.zip.\n",
        "- Store the unzipped simpsons_dataset folder under the project folder.\n",
        "Program preparation\n",
        "In the project folder, create faster-rcnn.ipynb.\n",
        "\n",
        "Verification\n",
        "After performing the above steps, the structure of the files and folders under the project folder should be as follows. Now you are ready to go.\n",
        "\n",
        "- kaggle_simpson_testset / (directory)\n",
        "- simpsons_dataset / (directory)\n",
        "- model / (directory)\n",
        "- README.md (file)\n",
        "- annotation.txt (file)\n",
        "- predict.py (file)\n",
        "- train.py (file)\n",
        "- faster-rcnn.ipynb (file)\n",
        "Training\n",
        "Use the cd commandto move it into the project folder. If you are using Google Colaboratory, please execute the cd commandafter mounting with Google Drive.\n",
        "\n",
        "In faster-rcnn.ipynb, enter the following in the code cell and execute."
      ],
      "metadata": {
        "id": "5QWgwdXH8uZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln-0Y_pu1wAr"
      },
      "outputs": [],
      "source": [
        "!python train.py -p annotation.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the flow in train.py, we refer to various programs in the model folder, build a model, acquire training data and execute training according to the description in annotation.txt.\n",
        "\n",
        "The above program runs the train.py file in python train.py and-p annotation.txt passes the run-time arguments\n",
        "\n",
        "Evaluation"
      ],
      "metadata": {
        "id": "Lh7an4nJ9_tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py -i kaggle_simpson_testset/kaggle_simpson_testset -c model_name.pickle"
      ],
      "metadata": {
        "id": "BbYbrGSc-Hdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optional file name model_name.pickle of-c must specify the file name saved when executing its own learning. The file name of the saved model should be output in the output of the program at the time of learning, so let's check that."
      ],
      "metadata": {
        "id": "dDYKtI1B-Lde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "The code published on GitHub can be downloaded and run.\n",
        "Here I ran Faster R-CNN\n",
        "\n",
        "Official document / reference information\n",
        "Link: Kaggle page\n",
        "https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset\n",
        "\n",
        "https://github.com/duckrabbits/ObjectDetection/tree/master/model"
      ],
      "metadata": {
        "id": "LTJqmlQa-T9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faster R-CNN code reading**"
      ],
      "metadata": {
        "id": "_rwIi4zf-0xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal\n",
        "Understanding algorithms through code reading"
      ],
      "metadata": {
        "id": "tr80nJ12_LyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Problem 2\" code reading\n",
        "In the above algorithm overview, I think you can only grasp the image, but let's read the code and understand it.\n",
        "\n",
        "Overview\n",
        "First, when executing, I entered the following command."
      ],
      "metadata": {
        "id": "Izvhwj90_Y7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python train.py -p annotation.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "PfNBdeAM_aP3",
        "outputId": "415cff52-9f5d-4ffa-adb5-9561306a8108"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-a88aa3fba454>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a88aa3fba454>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python train.py -p annotation.txt\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this command, we know that we need to read train.py first.\n",
        "\n",
        "If you look at train.py, you can see that the general flow is as follows.\n",
        "\n",
        "- Library import (lines 1-22)\n",
        "- Parse of run-time arguments (lines 25-37)\n",
        "- Definition of main function (lines 39 to 190)\n",
        "- Execution of the main function (lines 191 to 192)\n",
        "Next, let's see what is written in the main function of train.py."
      ],
      "metadata": {
        "id": "IkUPLt-o_lv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "main function of train.py\n",
        "Outputs such as loss recording and learning process are omitted.\n",
        "\n",
        "1. Define various variables, time, file name, etc. (lines 40 to 7)\n",
        "2. Get the setting values of various models from Config.py & read the data (lines 48 to 55)\n",
        "3. Save the settings so that you can use the same settings for inference as when learning (lines 57-61)\n",
        "4. Divide the data for training and evaluation (lines 63 to 64)\n",
        "5. Creating a data expansion generator (lines 66-67)\n",
        "6. Create three models, model_rpn, model_classifier, and model_all (line 69)\n",
        "7. Definition of variables for saving loss, rpn, and acc (lines 71 to 72)\n",
        "8. Learning loop for the number of learnings (line 85)\n",
        "9. Batch learning loop until the specified number of times is reached (lines 89 to 91)\n",
        "10. Create X / Y / img_data with a generator (line 97)\n",
        "X: Image data\n",
        "Y: Anchor box\n",
        "img_data: Dictionary type with filepath, width, height, bboxes, imageset as keys\n",
        "11. Calculate loss_rpn with X and Y as arguments (line 98)\n",
        "loss_rpn: Loss of each rpn (not directly related to learning or estimation, for progress display)\n",
        "12. Calculate P_rpn with X as an argument (line 99)\n",
        "P_rpn: Prediction result\n",
        "BBoxes showing the best results are extracted using an algorithm called non-max-suppression.\n",
        "13. Select BBox with P_rpn as an argument and set the output to R (line 100)\n",
        "R: BBox Choice\n",
        "14. Create classification network data with R as an argument (line 103)\n",
        "X2: BBox\n",
        "Y1: category one-hot vector\n",
        "Y2: Category and resized BBox\n",
        "15. Combine negative / positive of Y1 in half (lines 105 to 128)\n",
        "16: Enter images and RPN results into the classification network (line 130)\n",
        "\n",
        "The implementations other than train.py are briefly introduced below."
      ],
      "metadata": {
        "id": "jNeoz5Sd_r2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RPN is implemented below in resnet.py."
      ],
      "metadata": {
        "id": "Xfjeb2DsA0uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rpn(base_layers,num_anchors)"
      ],
      "metadata": {
        "id": "f9KWyuvHA2Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROI pooling is implemented below in RoiPoolingConv.py."
      ],
      "metadata": {
        "id": "x5r3glL7A8Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RoiPoolingConv(Layer)"
      ],
      "metadata": {
        "id": "WsheLsgLA9iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Various loss calculations are implemented below in losses.py."
      ],
      "metadata": {
        "id": "3rtRuZN0BDOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rpn_loss_regr(num_anchors)\n",
        "def rpn_loss_cls(num_anchors)\n",
        "def class_loss_regr(num_classes)\n",
        "def class_loss_cls(y_true, y_pred)"
      ],
      "metadata": {
        "id": "6ZGxybX6A_lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Summary\n",
        "Read the code and get a general idea of the algorithm proposed this time"
      ],
      "metadata": {
        "id": "JO9yHvbqBOH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview of Faster R-CNN and YOLO v3**\n"
      ],
      "metadata": {
        "id": "VnZs-x5HJO5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand the knowledge and techniques necessary to solve \"assignment Faster R-CNN and YOLO v3\""
      ],
      "metadata": {
        "id": "1cCyi5HmJQ0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm overview\n",
        "YOLOv3 is a derivative model of the model YOLO (You Only Look Once), so you need to understand YOLO before you understand YOLOv3.\n",
        "\n",
        "Unlike Faster-RCNN, YOLO is an epoch-making model that predicts from object detection to classification at once with one CNN.\n"
      ],
      "metadata": {
        "id": "ndA3nLwGJXr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO\n",
        "1. Divide the image into S × S grid cells.\n",
        "Prepare B Bounding Boxes for each grid cell in advance. The Bounding Box value is called the confidence score.\n",
        "The formula for the confidence score is as follows\n",
        "p\n",
        "​ ​\n",
        "radius\n",
        "​ ​\n",
        "(\n",
        "O\n",
        "​ ​\n",
        "b\n",
        "​ ​\n",
        "J\n",
        "​ ​\n",
        "e\n",
        "​ ​\n",
        "C\n",
        "​ ​\n",
        "t\n",
        ")\n",
        "​ ​\n",
        "∗\n",
        "​ ​\n",
        "I\n",
        "​ ​\n",
        "O\n",
        "​ ​\n",
        "u\n",
        "​ ​\n",
        "t\n",
        "​ ​\n",
        "radius\n",
        "​ ​\n",
        "u\n",
        "​ ​\n",
        "t\n",
        "​ ​\n",
        "indecent\n",
        "​ ​\n",
        "p\n",
        "​ ​\n",
        "radius\n",
        "​ ​\n",
        "e\n",
        "​ ​\n",
        "d\n",
        "\n",
        "=\n",
        "​ ​\n",
        "p\n",
        "​ ​\n",
        "radius\n",
        "​ ​\n",
        "(\n",
        "O\n",
        "​ ​\n",
        "b\n",
        "​ ​\n",
        "J\n",
        "​ ​\n",
        "e\n",
        "​ ​\n",
        "C\n",
        "​ ​\n",
        "t\n",
        ")\n",
        "​ ​\n",
        "∗\n",
        "​ ​\n",
        "p\n",
        "​ ​\n",
        "radius\n",
        "​ ​\n",
        "e\n",
        "​ ​\n",
        "d\n",
        "​ ​\n",
        "∩\n",
        "​ ​\n",
        "t\n",
        "​ ​\n",
        "radius\n",
        "​ ​\n",
        "u\n",
        "​ ​\n",
        "t\n",
        "​ ​\n",
        "indecent\n",
        "​ ​\n",
        "p\n",
        "​ ​\n",
        "radius\n",
        "​ ​\n",
        "e\n",
        "​ ​\n",
        "d\n",
        "​ ​\n",
        "∪\n",
        "​ ​\n",
        "t\n",
        "​ ​\n",
        "radius\n",
        "​ ​\n",
        "u\n",
        "​ ​\n",
        "t\n",
        "​ ​\n",
        "indecent\n"
      ],
      "metadata": {
        "id": "cjHI_QsZJsjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. For each Bounding Box, the coordinate values (x, y, w, h) of the Bounding Box and the confidence score for which the Bounding Box is an object are output, for a total of five values.\n",
        "\n",
        "4. Determine whether it is an object or a background based on IoU.\n",
        "\n",
        "5. Determine which class the grid cell belongs to if it is an object.\n",
        "\n",
        "Combining the class probabilities estimated here with the Bounding Boxes mentioned earlier yields multiple Bounding Boxes that indicate what the object is.\n",
        "\n",
        "These Bounding Boxes, which also include overlapping areas, are sorted by a method called NMS (Non-Maximum Suppression) based on the Bounding Box with a high reliability score.\n",
        "NMS suppresses areas with high IoU values (high degree of overlap) with thresholds.\n",
        "\n",
        "This will give you the object detection result.\n",
        "\n",
        "In other words, YOLO is a network where one grid cell has B x 5 + C outputs, and the total output is S x S x (B x 5 + C).\n",
        "In addition, YOLO uses a uniquely designed CNN without using an existing network.\n",
        "\n",
        "Link: https://arxiv.org/pdf/1506.02640"
      ],
      "metadata": {
        "id": "BYAUQgmzJ1ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLOv3\n",
        "The basic YOLO algorithm is as described above, but yolov3, which is a derivative model of yolo, has the following features.\n",
        "\n",
        "The main differences from YOLO are as follows.\n",
        "\n",
        "- Use deep network and extract features\n",
        "- Classification by logistic regression\n",
        "- Feature map calculated on 3 different scales"
      ],
      "metadata": {
        "id": "Lul9_TWJK-fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data download\n",
        "https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset"
      ],
      "metadata": {
        "id": "xI0D7_2kLSo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program preparation\n",
        "https://github.com/qqwweee/keras-yolo3\n",
        "The folder obtained above is called the project folder, download"
      ],
      "metadata": {
        "id": "OSlMJ7C6LX_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation\n",
        "\n",
        "Unzip the-simpsons-characters-dataset.zip downloaded from Kaggle.\n",
        "\n",
        "Unzip simpsons_dataset.zip.\n",
        "\n",
        "Store the unzipped simpsons_dataset folder under the project folder."
      ],
      "metadata": {
        "id": "6Im_0sflLwGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting a trained model\n",
        "Go file: yolov3.weight to get the trained model and place it in your project folder.\n",
        "\n",
        "Program preparation\n",
        "In the project folder, create yolov3.ipynb.\n",
        "\n",
        "Verification\n",
        "After performing the above steps, the structure of the files and folders under the project folder should be as follows. Now you are ready to go.\n",
        "\n",
        "Files and directories (files and directories) in the above repository\n",
        "kaggle_simpson_testset / (directory)\n",
        "simpsons_dataset / (directory)\n",
        "yolov3.ipynb (file)\n",
        "yolov3.weights\n",
        "Now, from here, we will write the execution program in yolov3.ipynb.\n",
        "\n",
        "Model format change\n",
        "Convert the downloaded trained model to the specified format"
      ],
      "metadata": {
        "id": "5C3PsS4PL2sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert.py yolov3.cfg yolov3.weights model_data/yolo_weights.h5"
      ],
      "metadata": {
        "id": "9xODFIIOMKha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimated by trained weights\n",
        "Make an estimate using a trained model"
      ],
      "metadata": {
        "id": "gMBbVRdCMPiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python yolo_video.py --image"
      ],
      "metadata": {
        "id": "GPS0VyDTMQoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a file for learning\n",
        "Annotation.txt used in Faster-RCNN and annotation.txt used in yolov3 have different formats, so convert them.\n",
        "\n",
        "Load the required libraries."
      ],
      "metadata": {
        "id": "tKFEP49vMUwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "pfDH389JM4p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load annotation.txt used in Faster-RCNN as annotation_rcnn.txt in pandas."
      ],
      "metadata": {
        "id": "EbqV_JRvM-zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_df = pd.read_csv('annotation_rcnn.txt',header=None)"
      ],
      "metadata": {
        "id": "YBgMjxHaNCKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the character name to a number (class label)"
      ],
      "metadata": {
        "id": "nHCqM-IbNFW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "annotation_df.iloc[:,5] = le.fit_transform(annotation_df.iloc[:,5])"
      ],
      "metadata": {
        "id": "_uOrosw1NItO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create annotation.txt that can be used in YOLOv3."
      ],
      "metadata": {
        "id": "4GkpzcpJNLr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcnn_path = 'annotation_rcnn.txt'\n",
        "yolo_path = 'annotation.txt'\n",
        "n_sample, n_col = annotation_df.shape\n",
        "with open(rcnn_path) as f:\n",
        "    lines = f.readline()\n",
        "    for i in range(n_sample):\n",
        "        split_line = lines.split(',')\n",
        "        image_path = split_line[0]\n",
        "        split_line[0] = './' + image_path\n",
        "        split_line[-1] = str(annotation_df.iloc[i,5]) + '\\n'\n",
        "        with open(yolo_path, mode='a') as out_f:\n",
        "            join_line = ','.join(split_line)\n",
        "            join_line = join_line.replace('.jpg,','.jpg ')\n",
        "            out_f.write(join_line)"
      ],
      "metadata": {
        "id": "fNF8RWFONQPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 1: Define the file path for annotation.txt in faster-rcnn\n",
        "\n",
        "2nd line: Define the file path of annotation.txt of yolov3\n",
        "\n",
        "Line 3: Open annotation.txt for rcnn\n",
        "\n",
        "4th line: Get the number of data and the number of columns\n",
        "\n",
        "Line 5: Read the data\n",
        "\n",
        "6th line: Loop by the number of data (number of lines)\n",
        "\n",
        "Line 7: Convert comma-separated data to a list\n",
        "\n",
        "8th line: Get the path of the image because it is stored at the front\n",
        "\n",
        "Line 9: Change path\n",
        "\n",
        "Line 10: Change the end from the character name to a numerical value (class label) → add a line feed code\n",
        "\n",
        "Lines 11-14: Open and save annotation.txt for YOLO"
      ],
      "metadata": {
        "id": "5AqSpaOTNVL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirmation that learning can be done\n",
        "Use the file you created in Problem 4 to learn. If it takes a long time to learn in the execution environment, you can just confirm that you can learn.\n",
        "\n",
        "Following this sample code approach, you need to change line 17 of train.py annotation_path = 'train.txt' to annotation_path = 'annotation.txt'.\n",
        "\n",
        "!python train.py"
      ],
      "metadata": {
        "id": "c_6fYF0eNrB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "GVSOetdpNyV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "YOLO, the origin of YOLOv3, is an epoch-making model at that time that one CNN can predict from object detection to classification at once."
      ],
      "metadata": {
        "id": "nOqp8iGiN8km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running**\n",
        "The purpose of this Sprint\n",
        "Gain expertise in running publicly available implementations\n",
        "Try to run the method you read about in the paper.\n",
        "How to learn\n",
        "We will actually run the published implementation of the method we read about in the paper.\n",
        "\n",
        "Faster R-CNN\n",
        "Run the implementation of Faster R-CNN [1].\n",
        "\n",
        "[1] Ren, S., He, K., Girshick, R., Sun, J .: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015 ) 91–99\n",
        "\n",
        "https://arxiv.org/pdf/1506.01497.pdf\n",
        "\n",
        "Please use the following. It is an implementation using Keras.\n",
        "\n",
        "https://github.com/duckrabbits/ObjectDetection/tree/master\n",
        "\n"
      ],
      "metadata": {
        "id": "yPtle1dCVDIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import numpy as np # linear algebra\n",
        "import os # accessing directory structure\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "metadata": {
        "id": "CKtbfGloYbxy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bv7AKWxkZWHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 1] Learning and Estimation\n",
        "Please refer to the README to run the above implementation.\n",
        "\n",
        "[Problem 2] Code reading\n",
        "Please read the code of the above implementation.\n",
        "\n",
        "First, list the parts of the Faster R-CNN [1] that you think are important. Then find the code that corresponds to it.\n",
        "\n",
        "(example)\n",
        "\n",
        "Where is the code that realizes RPN?\n",
        "Where is the code that implements RoI pooling?\n",
        "The framework provides general classes such as convolutional layers, but does not provide specific methods such as RoI pooling. It is possible to create an original layer, and for Keras, the following page has a collection of information.\n",
        "\n",
        "Creating your own Keras layer-Keras documentation: https://keras.io/getting_started/"
      ],
      "metadata": {
        "id": "zTLfHLHqZxlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "《Reference》\n",
        "\n",
        "Keras provides a VGG16 class that can be used easily. By providing the argumentinclude_top = False , the full join layer for the output is removed. Withweights = 'imagenet', you can also get a trained model using ImageNet for Transfer learning. If you set weights = 'None', the initialization will be random.\n",
        "\n",
        "Applications-Keras Documentation\n",
        "https://keras.io/guides/"
      ],
      "metadata": {
        "id": "9V6T7UGda1wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLOv3\n",
        "Learn and estimate Simpsons datasets (https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset) using methods other than Faster R-CNN. Use the Keras implementation of YOLOv3 [2].\n",
        "\n",
        "qqwweee / keras-yolo3: https://github.com/qqwweee/keras-yolo3/blob/master/yolo3/model.py\n",
        " A Keras implementation of YOLOv3 (Tensorflow backend)\n",
        "\n",
        "[2] Jeseph Redmon, Ali Farhadi. YOLOv3: An Incremental Improvement\n",
        "\n",
        "https://pjreddie.com/media/files/papers/YOLOv3.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "cii5dDKSa_q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 3] Estimation by learned weights\n",
        "How to perform the estimation using the learned weights is described in the Quick Start section of README.md.\n",
        "\n",
        "First of all, please follow this procedure to perform detection on some images or videos.\n",
        "\n",
        "Submit the output results as part of the assignment.\n",
        "\n",
        "[Problem 4] Create a file for learning\n",
        "Learn new data (Simpsons dataset). Read the Training section of the README.md and create the necessary files to train the Simpsons dataset.\n",
        "\n",
        "The format of the annotation file is different from the implementation of Problem 1, so it needs to be converted.\n",
        "\n",
        "[Problem 5] Confirmation that learning can be done\n",
        "Use the file you created in Problem 4 to learn. If it takes a long time to learn in the execution environment, you can just confirm that you can learn.\n",
        "\n",
        "[Problem 6] (Advance assignment) Code reading\n",
        "Please read the code of the above implementation.\n",
        "\n",
        "First, list the parts of the YOLOv3 [2] paper that you think are important. Then find the code that corresponds to it."
      ],
      "metadata": {
        "id": "0MaROhN9b9EC"
      }
    }
  ]
}